1
00:00:00,000 --> 00:00:04,799
dive into the world of python web

2
00:00:02,280 --> 00:00:06,960
scraping with this scrapey course for

3
00:00:04,799 --> 00:00:09,720
beginners learn to create scrapey

4
00:00:06,960 --> 00:00:12,360
spiders crawl websites clean and save

5
00:00:09,720 --> 00:00:14,460
data avoid getting blocked and deploy

6
00:00:12,360 --> 00:00:17,039
your scraper to the cloud all while

7
00:00:14,460 --> 00:00:19,560
using python scrapey Joe Kearney

8
00:00:17,039 --> 00:00:21,900
developed this course he is an expert in

9
00:00:19,560 --> 00:00:24,480
web scraping and he's the co-founder of

10
00:00:21,900 --> 00:00:26,220
scrape Ops hey everyone welcome to the

11
00:00:24,480 --> 00:00:28,680
free code campus crepey beginners course

12
00:00:26,220 --> 00:00:30,480
my name is Joe I'm a co-founder at

13
00:00:28,680 --> 00:00:32,160
scrape UPS a platform that gives

14
00:00:30,480 --> 00:00:34,079
developers the tooling they need to

15
00:00:32,160 --> 00:00:36,480
monitor schedule and run their web

16
00:00:34,079 --> 00:00:38,760
scrapers also the co-founder of the

17
00:00:36,480 --> 00:00:40,920
Python scrapey Playbook a complete

18
00:00:38,760 --> 00:00:42,300
collection of guides and tutorials that

19
00:00:40,920 --> 00:00:45,059
teaches you everything you need to know

20
00:00:42,300 --> 00:00:47,040
to become a scrapey developer in this

21
00:00:45,059 --> 00:00:49,260
free code Camp scrapey course we're

22
00:00:47,040 --> 00:00:51,600
going to take you from complete scrapey

23
00:00:49,260 --> 00:00:53,940
beginner to being able to build deploy

24
00:00:51,600 --> 00:00:56,219
and scale your own scrapey spiders so

25
00:00:53,940 --> 00:00:58,079
you can scrape the data you need you can

26
00:00:56,219 --> 00:00:59,760
find the code and written guides for

27
00:00:58,079 --> 00:01:02,280
this course over at the python scrapey

28
00:00:59,760 --> 00:01:04,019
Playbook book free code Camp course page

29
00:01:02,280 --> 00:01:04,860
so you can easily follow along with this

30
00:01:04,019 --> 00:01:07,080
video

31
00:01:04,860 --> 00:01:08,939
the link will be in the description

32
00:01:07,080 --> 00:01:11,280
we've broken this course down into 13

33
00:01:08,939 --> 00:01:13,080
Parts which cover all the scrapey basics

34
00:01:11,280 --> 00:01:15,060
you need to go from never having used

35
00:01:13,080 --> 00:01:17,040
scraping before to being a competent

36
00:01:15,060 --> 00:01:19,020
Scrappy developer if you want to dive

37
00:01:17,040 --> 00:01:20,759
deeper into any of these topics then

38
00:01:19,020 --> 00:01:23,700
check out the python scrapey Playbook

39
00:01:20,759 --> 00:01:26,340
itself or the scrape Ops YouTube channel

40
00:01:23,700 --> 00:01:28,560
which is linked in the description

41
00:01:26,340 --> 00:01:30,360
here you will find guides that dive

42
00:01:28,560 --> 00:01:32,700
deeper into the topics we've discussed

43
00:01:30,360 --> 00:01:34,979
in this course and cover some more

44
00:01:32,700 --> 00:01:37,619
advanced grippy topics too such as

45
00:01:34,979 --> 00:01:40,560
scraping Dynamic websites and scaling

46
00:01:37,619 --> 00:01:43,799
your web scrapers using redis

47
00:01:40,560 --> 00:01:46,200
so what is scrapey so I think the best

48
00:01:43,799 --> 00:01:50,159
little summary for that is directly on

49
00:01:46,200 --> 00:01:51,720
the scrapey.org website so scrapey is an

50
00:01:50,159 --> 00:01:53,759
open source and collaborative framework

51
00:01:51,720 --> 00:01:57,180
for extracting the data you need from

52
00:01:53,759 --> 00:01:58,140
websites in a fast simple yet extensible

53
00:01:57,180 --> 00:02:01,020
way

54
00:01:58,140 --> 00:02:04,079
so it's an open source framework that

55
00:02:01,020 --> 00:02:07,619
anyone can use with python and it makes

56
00:02:04,079 --> 00:02:09,840
scraping websites much much easier

57
00:02:07,619 --> 00:02:12,840
so it helps you do things like retrieve

58
00:02:09,840 --> 00:02:15,480
a Pages HTML parse and then process the

59
00:02:12,840 --> 00:02:18,060
data and then store that data in the

60
00:02:15,480 --> 00:02:20,520
file formats you want and in the

61
00:02:18,060 --> 00:02:23,160
location that you want

62
00:02:20,520 --> 00:02:25,260
so that's what scrapey is so now the

63
00:02:23,160 --> 00:02:26,879
next question you probably have is well

64
00:02:25,260 --> 00:02:28,319
why should we choose scraping what does

65
00:02:26,879 --> 00:02:29,340
scraping have to offer over anything

66
00:02:28,319 --> 00:02:32,160
else

67
00:02:29,340 --> 00:02:33,540
so some of you might have already done a

68
00:02:32,160 --> 00:02:35,400
bit of scraping with python and you

69
00:02:33,540 --> 00:02:39,180
might have used things like just

70
00:02:35,400 --> 00:02:41,280
straight python requests to request the

71
00:02:39,180 --> 00:02:43,080
page and then get the response and then

72
00:02:41,280 --> 00:02:46,080
you might have parsed that response

73
00:02:43,080 --> 00:02:50,760
using something like beautiful soup

74
00:02:46,080 --> 00:02:53,040
which helps you parse HTML so this is

75
00:02:50,760 --> 00:02:54,780
perfect if you're doing just very simple

76
00:02:53,040 --> 00:02:57,599
scraping and you just want to scrape a

77
00:02:54,780 --> 00:03:00,480
couple of pages of a website or you just

78
00:02:57,599 --> 00:03:01,620
have something like a one-off site that

79
00:03:00,480 --> 00:03:03,660
you want to scrape

80
00:03:01,620 --> 00:03:05,879
by all means use something like python

81
00:03:03,660 --> 00:03:07,739
requests and beautiful soup if you're

82
00:03:05,879 --> 00:03:10,860
looking to do anything

83
00:03:07,739 --> 00:03:12,239
from small medium to large scale it's

84
00:03:10,860 --> 00:03:15,300
much better to use something like

85
00:03:12,239 --> 00:03:17,819
scrapey because it comes with a load of

86
00:03:15,300 --> 00:03:20,340
features built in that you don't have to

87
00:03:17,819 --> 00:03:23,640
worry about so it helps you do things

88
00:03:20,340 --> 00:03:26,819
such as data extraction from the HTML

89
00:03:23,640 --> 00:03:28,980
using CSS selectors you can do

90
00:03:26,819 --> 00:03:30,780
automatic data formatting so it'll

91
00:03:28,980 --> 00:03:35,220
format the data you scraped into things

92
00:03:30,780 --> 00:03:38,099
like CSV Json XML and many other formats

93
00:03:35,220 --> 00:03:40,980
you can save the stuff directly into

94
00:03:38,099 --> 00:03:42,599
things like S3 buckets onto your local

95
00:03:40,980 --> 00:03:44,519
machine

96
00:03:42,599 --> 00:03:46,500
and use middlewares to save the data

97
00:03:44,519 --> 00:03:49,739
into databases so so much of that is

98
00:03:46,500 --> 00:03:50,879
already taken care for you so what else

99
00:03:49,739 --> 00:03:52,739
does that have that you don't have to

100
00:03:50,879 --> 00:03:55,260
worry about automatic retrace for

101
00:03:52,739 --> 00:03:57,840
example if you look for a page and the

102
00:03:55,260 --> 00:04:00,120
page comes back with an error it'll Auto

103
00:03:57,840 --> 00:04:02,159
retry it for you and you don't even have

104
00:04:00,120 --> 00:04:05,459
to worry about all the logic that goes

105
00:04:02,159 --> 00:04:06,260
into things like Auto retrace it looks

106
00:04:05,459 --> 00:04:09,959
after

107
00:04:06,260 --> 00:04:11,400
concurrency so you can scrape from one

108
00:04:09,959 --> 00:04:14,519
page all the way up to thousands of

109
00:04:11,400 --> 00:04:15,959
pages at the exact same time using the

110
00:04:14,519 --> 00:04:19,620
same piece of code

111
00:04:15,959 --> 00:04:21,900
so with all this stuff

112
00:04:19,620 --> 00:04:24,120
so much is taken off your plate that you

113
00:04:21,900 --> 00:04:26,699
can just focus on doing what you want to

114
00:04:24,120 --> 00:04:29,340
do which is okay this is the data I want

115
00:04:26,699 --> 00:04:31,500
from the website and this is the format

116
00:04:29,340 --> 00:04:33,840
that I want to save it into

117
00:04:31,500 --> 00:04:36,000
and the other great thing the fact that

118
00:04:33,840 --> 00:04:38,759
scrapey is an open sourced framework

119
00:04:36,000 --> 00:04:40,560
means that there's many thousands of

120
00:04:38,759 --> 00:04:43,500
developers all over the world who've

121
00:04:40,560 --> 00:04:45,900
made great plugins and extensions for it

122
00:04:43,500 --> 00:04:47,940
almost every single kind of question we

123
00:04:45,900 --> 00:04:50,759
might have is probably already answered

124
00:04:47,940 --> 00:04:52,740
so it's very easy to find the questions

125
00:04:50,759 --> 00:04:54,900
and answers online and things like stack

126
00:04:52,740 --> 00:04:57,900
Overflow for the use cases you'll be

127
00:04:54,900 --> 00:05:00,180
going through and it makes it very easy

128
00:04:57,900 --> 00:05:01,500
if you don't find the question just ask

129
00:05:00,180 --> 00:05:05,100
another one

130
00:05:01,500 --> 00:05:06,840
so I would really recommend scrapey if

131
00:05:05,100 --> 00:05:09,419
you're looking to do anything that is

132
00:05:06,840 --> 00:05:10,919
more than Justice very very very simple

133
00:05:09,419 --> 00:05:13,440
use case so if you're looking to scrape

134
00:05:10,919 --> 00:05:15,960
any kind of website scrapey would be the

135
00:05:13,440 --> 00:05:17,820
place to start off your scripting

136
00:05:15,960 --> 00:05:20,340
Journey

137
00:05:17,820 --> 00:05:21,780
okay so this course will be delivered

138
00:05:20,340 --> 00:05:23,699
through video of course which you're

139
00:05:21,780 --> 00:05:26,039
watching right now but along with the

140
00:05:23,699 --> 00:05:27,840
video we have an accompanying article

141
00:05:26,039 --> 00:05:31,320
that goes with each section

142
00:05:27,840 --> 00:05:34,259
so for example part two we have a full

143
00:05:31,320 --> 00:05:37,259
article here with all the commands you

144
00:05:34,259 --> 00:05:39,000
need to run code Snippets and more

145
00:05:37,259 --> 00:05:41,759
in-depth explanations

146
00:05:39,000 --> 00:05:43,139
so this makes things much easier if

147
00:05:41,759 --> 00:05:45,180
you're not someone who likes to watch

148
00:05:43,139 --> 00:05:47,340
video and you prefer to read things and

149
00:05:45,180 --> 00:05:51,180
take things step by step that way we

150
00:05:47,340 --> 00:05:54,120
will also have all the codes that we use

151
00:05:51,180 --> 00:05:56,340
if you want to just jump into a part

152
00:05:54,120 --> 00:05:58,620
further down the line you can go and you

153
00:05:56,340 --> 00:06:01,500
can download whatever part you need we

154
00:05:58,620 --> 00:06:03,120
will have an accompanying git repo where

155
00:06:01,500 --> 00:06:05,220
you can just download the code at that

156
00:06:03,120 --> 00:06:06,960
point in time and follow on so hopefully

157
00:06:05,220 --> 00:06:08,580
that should make your learning that bit

158
00:06:06,960 --> 00:06:11,039
easier

159
00:06:08,580 --> 00:06:13,380
okay so what we're going to cover on

160
00:06:11,039 --> 00:06:16,500
this course so we're in part one now

161
00:06:13,380 --> 00:06:18,360
part two will be setting up your virtual

162
00:06:16,500 --> 00:06:19,500
environment and setting up scrapey on

163
00:06:18,360 --> 00:06:21,960
your computer

164
00:06:19,500 --> 00:06:23,400
part three we look at how to create a

165
00:06:21,960 --> 00:06:26,039
scrappy project

166
00:06:23,400 --> 00:06:27,240
part four creating your first scrapey

167
00:06:26,039 --> 00:06:29,639
spider

168
00:06:27,240 --> 00:06:33,600
and navigating through different pages

169
00:06:29,639 --> 00:06:37,020
getting the HTML from the page and then

170
00:06:33,600 --> 00:06:39,539
extracting what we need from the HTML

171
00:06:37,020 --> 00:06:42,479
we look at crawling through multiple

172
00:06:39,539 --> 00:06:44,460
Pages then how to clean the data that

173
00:06:42,479 --> 00:06:45,780
you've just scraped using item pipelines

174
00:06:44,460 --> 00:06:48,479
and Scrappy

175
00:06:45,780 --> 00:06:51,060
then in part 7 we'll be looking at

176
00:06:48,479 --> 00:06:52,800
saving the data to files and databases

177
00:06:51,060 --> 00:06:53,819
and all the different ways we can do

178
00:06:52,800 --> 00:06:57,180
that

179
00:06:53,819 --> 00:06:59,759
part 8 looking at everything to do with

180
00:06:57,180 --> 00:07:03,539
headers and user agents and how we can

181
00:06:59,759 --> 00:07:06,240
use user agents and headers to bypass

182
00:07:03,539 --> 00:07:08,819
certain restrictions that websites might

183
00:07:06,240 --> 00:07:09,960
be putting on us when we try and collect

184
00:07:08,819 --> 00:07:12,660
the data

185
00:07:09,960 --> 00:07:15,720
part nine we'll be looking at rotating

186
00:07:12,660 --> 00:07:18,479
proxies and proxy apis and how they can

187
00:07:15,720 --> 00:07:21,419
help us bypass some of those issues

188
00:07:18,479 --> 00:07:23,580
about getting blocked

189
00:07:21,419 --> 00:07:25,560
part 10 we'll be looking at deploying

190
00:07:23,580 --> 00:07:28,919
and scheduling our spiders using

191
00:07:25,560 --> 00:07:31,620
scrapeyd so when we want to run a spider

192
00:07:28,919 --> 00:07:33,479
on a regular basis we want to have

193
00:07:31,620 --> 00:07:35,280
something set up that we don't have to

194
00:07:33,479 --> 00:07:37,919
worry about and kick off manually but

195
00:07:35,280 --> 00:07:40,259
that will be programmatically run on a

196
00:07:37,919 --> 00:07:42,300
daily basis hourly basis every 10

197
00:07:40,259 --> 00:07:44,039
minutes or whatever so we look at

198
00:07:42,300 --> 00:07:46,139
everything to do with that in Parts 10

199
00:07:44,039 --> 00:07:48,180
11 and 12 and we're looking at the

200
00:07:46,139 --> 00:07:50,340
different options that are out there in

201
00:07:48,180 --> 00:07:52,139
terms of the free options open source

202
00:07:50,340 --> 00:07:55,800
options paid options

203
00:07:52,139 --> 00:07:59,280
the pros and cons of those options that

204
00:07:55,800 --> 00:08:03,120
we have and then that brings us to our

205
00:07:59,280 --> 00:08:05,099
recap at the end which is part 13. and

206
00:08:03,120 --> 00:08:08,580
part 13 we'll just go through everything

207
00:08:05,099 --> 00:08:11,340
we've learned and recap and talk about

208
00:08:08,580 --> 00:08:14,759
what there is left to do if you want to

209
00:08:11,340 --> 00:08:16,919
take your scripting to the next level

210
00:08:14,759 --> 00:08:20,460
so I think that's everything I wanted to

211
00:08:16,919 --> 00:08:22,819
talk about and we will see you in part

212
00:08:20,460 --> 00:08:22,819
two

213
00:08:25,319 --> 00:08:28,860
so in part two of this course we're

214
00:08:26,940 --> 00:08:30,060
going to be looking at how to install

215
00:08:28,860 --> 00:08:31,440
python

216
00:08:30,060 --> 00:08:35,700
setting up your price in Virtual

217
00:08:31,440 --> 00:08:37,860
environment on Mac Windows analytics and

218
00:08:35,700 --> 00:08:41,159
then finally how to install scraping

219
00:08:37,860 --> 00:08:42,719
okay so let's get started with that

220
00:08:41,159 --> 00:08:44,880
so first things first how to install

221
00:08:42,719 --> 00:08:46,440
python so it's fairly easy the first

222
00:08:44,880 --> 00:08:48,080
thing we want to do is we want to just

223
00:08:46,440 --> 00:08:49,980
go to

224
00:08:48,080 --> 00:08:52,200
python.org

225
00:08:49,980 --> 00:08:54,540
go to the downloads section

226
00:08:52,200 --> 00:08:58,620
and then you can click the download

227
00:08:54,540 --> 00:08:59,519
python 3.11 or whatever version it will

228
00:08:58,620 --> 00:09:01,560
be

229
00:08:59,519 --> 00:09:03,180
when you're looking at it so obviously

230
00:09:01,560 --> 00:09:05,160
I'm doing it I'm on the Mac so it

231
00:09:03,180 --> 00:09:07,019
automatically detects that and it

232
00:09:05,160 --> 00:09:09,839
automatically proposes that I download

233
00:09:07,019 --> 00:09:12,240
the version for Mac OS you guys if

234
00:09:09,839 --> 00:09:14,880
you're on Windows you'd be wanting to

235
00:09:12,240 --> 00:09:17,160
download the latest version for Windows

236
00:09:14,880 --> 00:09:18,600
so go ahead do that if you don't already

237
00:09:17,160 --> 00:09:20,279
have python

238
00:09:18,600 --> 00:09:23,760
we can quickly check if you do have

239
00:09:20,279 --> 00:09:24,899
python by going to your terminal or

240
00:09:23,760 --> 00:09:26,700
Powershell

241
00:09:24,899 --> 00:09:29,040
so open that up

242
00:09:26,700 --> 00:09:32,760
and then I'm just going to open one up

243
00:09:29,040 --> 00:09:34,740
quickly here in Visual Studio code

244
00:09:32,760 --> 00:09:37,620
and what you just want to do then is

245
00:09:34,740 --> 00:09:39,540
just type in Python

246
00:09:37,620 --> 00:09:42,720
and

247
00:09:39,540 --> 00:09:46,860
dash dash version

248
00:09:42,720 --> 00:09:49,980
and as you can see here python version

249
00:09:46,860 --> 00:09:51,720
3.9 is installed for me so I don't need

250
00:09:49,980 --> 00:09:53,459
to go under loaders because I know it's

251
00:09:51,720 --> 00:09:56,640
already installed

252
00:09:53,459 --> 00:09:58,140
so go ahead and check if you have python

253
00:09:56,640 --> 00:10:00,000
installed if you do have it installed

254
00:09:58,140 --> 00:10:02,540
you can move on to the next section and

255
00:10:00,000 --> 00:10:05,940
if you don't just go ahead download

256
00:10:02,540 --> 00:10:09,839
Python and install it

257
00:10:05,940 --> 00:10:11,880
okay so the next thing we want to do is

258
00:10:09,839 --> 00:10:14,279
install pip if it's not already

259
00:10:11,880 --> 00:10:16,339
installed so pip is just a package

260
00:10:14,279 --> 00:10:19,200
manager for python so we can download

261
00:10:16,339 --> 00:10:20,519
third-party packages for our python

262
00:10:19,200 --> 00:10:22,680
project

263
00:10:20,519 --> 00:10:25,320
so what we do again we just check is it

264
00:10:22,680 --> 00:10:27,300
installed so it's just pip

265
00:10:25,320 --> 00:10:31,440
version again

266
00:10:27,300 --> 00:10:33,420
and as I can see I have Pip version 22

267
00:10:31,440 --> 00:10:36,180
installed here

268
00:10:33,420 --> 00:10:40,500
so if you don't have it installed

269
00:10:36,180 --> 00:10:42,720
we link to it in the documentation in

270
00:10:40,500 --> 00:10:45,600
the article that we have for this and in

271
00:10:42,720 --> 00:10:48,600
the video as well

272
00:10:45,600 --> 00:10:50,660
so to install pip

273
00:10:48,600 --> 00:10:54,480
you go to

274
00:10:50,660 --> 00:10:56,160
pip.pypa.io and go to installation and

275
00:10:54,480 --> 00:10:59,760
they have supported methods for

276
00:10:56,160 --> 00:11:02,100
installing on Linux Mac OS and windows

277
00:10:59,760 --> 00:11:03,000
and to give you the commands you need to

278
00:11:02,100 --> 00:11:06,240
run

279
00:11:03,000 --> 00:11:08,880
once you have a python installed to

280
00:11:06,240 --> 00:11:11,040
install pip so it's very

281
00:11:08,880 --> 00:11:13,980
self-explanatory and very easy to do all

282
00:11:11,040 --> 00:11:15,959
you need to do is copy this line and

283
00:11:13,980 --> 00:11:19,620
paste it into your terminal and hit

284
00:11:15,959 --> 00:11:21,839
enter so if I do it here it will

285
00:11:19,620 --> 00:11:25,560
just

286
00:11:21,839 --> 00:11:27,420
tell me that I already have it installed

287
00:11:25,560 --> 00:11:29,279
so as you can see it says requirement

288
00:11:27,420 --> 00:11:32,579
already satisfied

289
00:11:29,279 --> 00:11:36,660
pip in and then it gives the path

290
00:11:32,579 --> 00:11:39,240
so we have Pip installed and the next

291
00:11:36,660 --> 00:11:40,500
part is we want to install a virtual

292
00:11:39,240 --> 00:11:44,579
environment

293
00:11:40,500 --> 00:11:47,220
so VNV which comes with Python 3 the

294
00:11:44,579 --> 00:11:49,980
latest version of python will already be

295
00:11:47,220 --> 00:11:51,720
installed if you have python 3.3 and

296
00:11:49,980 --> 00:11:54,060
above

297
00:11:51,720 --> 00:11:57,360
if you have a lower version of python

298
00:11:54,060 --> 00:12:00,899
you might need to install the VNS

299
00:11:57,360 --> 00:12:02,399
manually and if you're on Windows you

300
00:12:00,899 --> 00:12:05,459
may need to

301
00:12:02,399 --> 00:12:09,480
install it manually as well so to do

302
00:12:05,459 --> 00:12:11,579
that you just pip install virtual ends

303
00:12:09,480 --> 00:12:14,100
if you're on Windows

304
00:12:11,579 --> 00:12:16,680
and I'll do that right now

305
00:12:14,100 --> 00:12:19,320
pip install virtual ends

306
00:12:16,680 --> 00:12:21,600
and that will go ahead and install

307
00:12:19,320 --> 00:12:25,380
virtual end for you

308
00:12:21,600 --> 00:12:26,220
if you're on Mac you don't need to do

309
00:12:25,380 --> 00:12:28,440
this

310
00:12:26,220 --> 00:12:30,720
or if you're on Ubuntu you more than

311
00:12:28,440 --> 00:12:34,260
likely won't need to do this either

312
00:12:30,720 --> 00:12:37,800
so we have python installed we have Pip

313
00:12:34,260 --> 00:12:39,899
installed we have virtual lens rvm

314
00:12:37,800 --> 00:12:41,519
installed so the next thing we can go

315
00:12:39,899 --> 00:12:43,139
ahead and do is actually create our

316
00:12:41,519 --> 00:12:46,620
virtual environment

317
00:12:43,139 --> 00:12:49,260
so a virtual environment is just think

318
00:12:46,620 --> 00:12:51,540
of it as a folder that sits on top of

319
00:12:49,260 --> 00:12:54,779
python where you can add all these

320
00:12:51,540 --> 00:12:57,000
third-party libraries and modules and

321
00:12:54,779 --> 00:12:59,100
they'll only be specific to the project

322
00:12:57,000 --> 00:13:01,139
you're currently running because what

323
00:12:59,100 --> 00:13:03,720
can happen is if you've got multiple

324
00:13:01,139 --> 00:13:06,839
python projects you can also often have

325
00:13:03,720 --> 00:13:09,600
multiple of the same packages but

326
00:13:06,839 --> 00:13:11,700
different versions to run your code and

327
00:13:09,600 --> 00:13:14,339
you don't want if you for example

328
00:13:11,700 --> 00:13:16,200
upgrade some third-party package that it

329
00:13:14,339 --> 00:13:17,700
breaks one of your other projects

330
00:13:16,200 --> 00:13:20,040
because one of your other projects

331
00:13:17,700 --> 00:13:22,620
needed an older version of that

332
00:13:20,040 --> 00:13:24,360
third-party package so by using virtual

333
00:13:22,620 --> 00:13:25,740
environments it just means that each

334
00:13:24,360 --> 00:13:27,540
project you have

335
00:13:25,740 --> 00:13:30,120
the third party libraries you installed

336
00:13:27,540 --> 00:13:33,720
are specific to that project

337
00:13:30,120 --> 00:13:35,820
so let's go ahead now and we'll just do

338
00:13:33,720 --> 00:13:38,779
python

339
00:13:35,820 --> 00:13:38,779
minus n

340
00:13:39,440 --> 00:13:45,600
and we're going to call the folder that

341
00:13:43,440 --> 00:13:48,180
we want VM also

342
00:13:45,600 --> 00:13:50,519
I just want to make sure I'm in the

343
00:13:48,180 --> 00:13:52,680
correct

344
00:13:50,519 --> 00:13:55,279
um folder so I've just made a part two

345
00:13:52,680 --> 00:13:58,560
folder with nothing in it as you can see

346
00:13:55,279 --> 00:14:01,800
and I'm just going to

347
00:13:58,560 --> 00:14:05,700
to thought no minus m

348
00:14:01,800 --> 00:14:07,620
v n f the end

349
00:14:05,700 --> 00:14:10,620
so that's gone ahead and let's create

350
00:14:07,620 --> 00:14:12,839
this VN folder with

351
00:14:10,620 --> 00:14:15,779
these

352
00:14:12,839 --> 00:14:18,000
items in it here and it's installed

353
00:14:15,779 --> 00:14:19,920
correctly

354
00:14:18,000 --> 00:14:22,860
if you're on Windows you're just going

355
00:14:19,920 --> 00:14:24,180
to be using the virtual end command

356
00:14:22,860 --> 00:14:26,399
instead

357
00:14:24,180 --> 00:14:28,740
okay so now that we have our virtual

358
00:14:26,399 --> 00:14:31,200
environment installed

359
00:14:28,740 --> 00:14:33,720
and you can see it here we want to just

360
00:14:31,200 --> 00:14:35,220
activate it so that any third-party

361
00:14:33,720 --> 00:14:38,700
package we installed after this will

362
00:14:35,220 --> 00:14:42,120
also be installed into this VN folder so

363
00:14:38,700 --> 00:14:46,920
to do that we just type in source

364
00:14:42,120 --> 00:14:49,740
and then VM bin activate

365
00:14:46,920 --> 00:14:53,399
so then you can see it's activated

366
00:14:49,740 --> 00:14:54,420
because we have the folder name VNV in

367
00:14:53,399 --> 00:14:55,560
Brackets

368
00:14:54,420 --> 00:14:58,260
here

369
00:14:55,560 --> 00:15:00,360
so that means anything we installed from

370
00:14:58,260 --> 00:15:03,300
now on using the package installer pip

371
00:15:00,360 --> 00:15:06,480
will be installed into this folder and

372
00:15:03,300 --> 00:15:08,399
be specific only to this project

373
00:15:06,480 --> 00:15:10,139
so we can go ahead now and install

374
00:15:08,399 --> 00:15:13,620
scrapey

375
00:15:10,139 --> 00:15:15,480
so we just do pip

376
00:15:13,620 --> 00:15:16,920
install

377
00:15:15,480 --> 00:15:19,680
scrapey

378
00:15:16,920 --> 00:15:23,459
and you can also get this command from

379
00:15:19,680 --> 00:15:25,980
the scrapey website itself as you can

380
00:15:23,459 --> 00:15:30,120
see pip install scrapey will install the

381
00:15:25,980 --> 00:15:30,120
latest version of scrapey 2.7.1

382
00:15:30,720 --> 00:15:37,260
so I'm just going ahead and hit enter

383
00:15:34,260 --> 00:15:41,220
and as you can see it's downloading

384
00:15:37,260 --> 00:15:43,980
everything it needs for scrape B to run

385
00:15:41,220 --> 00:15:47,100
so depending on your connection and your

386
00:15:43,980 --> 00:15:48,720
computer it can take a minute or two

387
00:15:47,100 --> 00:15:51,899
okay

388
00:15:48,720 --> 00:15:52,920
so that's installed correctly as far as

389
00:15:51,899 --> 00:15:54,660
I know

390
00:15:52,920 --> 00:15:55,800
check it's installed correctly we can

391
00:15:54,660 --> 00:15:57,420
just run

392
00:15:55,800 --> 00:15:58,980
scrapey

393
00:15:57,420 --> 00:16:02,880
and

394
00:15:58,980 --> 00:16:05,339
it should give us a list of commands so

395
00:16:02,880 --> 00:16:07,680
if you see this output here where it's

396
00:16:05,339 --> 00:16:10,019
lists the available commands you know

397
00:16:07,680 --> 00:16:12,300
that scrapey is installed correctly as

398
00:16:10,019 --> 00:16:14,100
you can see from this line here scrapey

399
00:16:12,300 --> 00:16:16,500
has detected that there's no scrapey

400
00:16:14,100 --> 00:16:18,060
project created yet so it just says no

401
00:16:16,500 --> 00:16:20,279
active project

402
00:16:18,060 --> 00:16:22,860
so that's going to be the next step in

403
00:16:20,279 --> 00:16:23,699
part three is setting up our scrippy

404
00:16:22,860 --> 00:16:29,060
project

405
00:16:23,699 --> 00:16:29,060
so let's get going into part three

406
00:16:32,040 --> 00:16:36,120
so in part three we're going to be

407
00:16:33,660 --> 00:16:38,339
looking at how to create a scrappy

408
00:16:36,120 --> 00:16:40,500
project using scrapey

409
00:16:38,339 --> 00:16:43,680
then we're going to have an overview

410
00:16:40,500 --> 00:16:46,560
look at the project files that are

411
00:16:43,680 --> 00:16:48,120
generated when you create a new project

412
00:16:46,560 --> 00:16:52,160
and then after that we're going to go

413
00:16:48,120 --> 00:16:55,560
into detail on all the different parts

414
00:16:52,160 --> 00:16:59,040
of a scrapey project so that entails

415
00:16:55,560 --> 00:17:02,160
scrapey spiders items item pipelines

416
00:16:59,040 --> 00:17:04,679
it's created middlewares and settings

417
00:17:02,160 --> 00:17:07,319
so part three is really going to be a

418
00:17:04,679 --> 00:17:10,319
kind of a theory heavy part of this

419
00:17:07,319 --> 00:17:12,480
course so if you already know a bit of

420
00:17:10,319 --> 00:17:14,240
python this is probably going to be a

421
00:17:12,480 --> 00:17:17,760
lot more interesting than if you don't

422
00:17:14,240 --> 00:17:20,400
so you can feel free to dip around and

423
00:17:17,760 --> 00:17:22,980
and have a look at what parts of this

424
00:17:20,400 --> 00:17:24,480
would be most interesting to you we also

425
00:17:22,980 --> 00:17:25,799
have an article that goes along with

426
00:17:24,480 --> 00:17:27,720
this that might be a bit easier to

427
00:17:25,799 --> 00:17:30,780
digest

428
00:17:27,720 --> 00:17:34,320
so let's get going and create our

429
00:17:30,780 --> 00:17:37,559
project so to do that I've got a folder

430
00:17:34,320 --> 00:17:39,660
here part three and inside that folder

431
00:17:37,559 --> 00:17:41,760
I've got just the full project we're

432
00:17:39,660 --> 00:17:44,100
going to go through in a second and I've

433
00:17:41,760 --> 00:17:45,600
got my virtual environment that I've

434
00:17:44,100 --> 00:17:48,539
already activated

435
00:17:45,600 --> 00:17:50,940
so you guys if you've followed on from

436
00:17:48,539 --> 00:17:53,220
part two you should have already

437
00:17:50,940 --> 00:17:54,419
activated your virtual environment and

438
00:17:53,220 --> 00:17:56,460
you should already have scrapey

439
00:17:54,419 --> 00:17:58,799
installed

440
00:17:56,460 --> 00:18:01,500
so if you don't have that done just hop

441
00:17:58,799 --> 00:18:03,900
back to part two and make sure scrape is

442
00:18:01,500 --> 00:18:08,160
installed and your virtual environment

443
00:18:03,900 --> 00:18:09,840
is activated okay so now we can go ahead

444
00:18:08,160 --> 00:18:14,880
and

445
00:18:09,840 --> 00:18:16,919
use the scrapey start project command to

446
00:18:14,880 --> 00:18:19,919
create our new project

447
00:18:16,919 --> 00:18:22,440
so it's just simply scrapey space start

448
00:18:19,919 --> 00:18:24,179
project space and then the name of your

449
00:18:22,440 --> 00:18:26,340
project we're going to call this one

450
00:18:24,179 --> 00:18:29,580
book scraper because we're going to be

451
00:18:26,340 --> 00:18:33,240
scraping a site with books in it

452
00:18:29,580 --> 00:18:35,760
so if I hit enter

453
00:18:33,240 --> 00:18:39,360
it's gone ahead and created a new folder

454
00:18:35,760 --> 00:18:41,460
here book scraper you can see and if I

455
00:18:39,360 --> 00:18:43,580
do an LS you can see book scraper is

456
00:18:41,460 --> 00:18:46,799
there as well if

457
00:18:43,580 --> 00:18:51,000
and if we go into

458
00:18:46,799 --> 00:18:54,539
book scraper itself we can see we have

459
00:18:51,000 --> 00:18:56,720
book scraper and scrapey.cfg so I'm just

460
00:18:54,539 --> 00:19:01,500
going to open up

461
00:18:56,720 --> 00:19:04,080
the folder here and we can see inside of

462
00:19:01,500 --> 00:19:06,900
that we have several different files and

463
00:19:04,080 --> 00:19:08,220
folders so first off we have our spiders

464
00:19:06,900 --> 00:19:11,220
folder

465
00:19:08,220 --> 00:19:13,860
that at the moment has no spiders in it

466
00:19:11,220 --> 00:19:15,720
but we'll be doing that in part four

467
00:19:13,860 --> 00:19:18,660
we'll be generating spiders that go in

468
00:19:15,720 --> 00:19:21,299
there then we have items middlewares

469
00:19:18,660 --> 00:19:25,080
pipelines and settings

470
00:19:21,299 --> 00:19:27,780
so your basic scrapery project will

471
00:19:25,080 --> 00:19:30,780
contain these parts now you don't have

472
00:19:27,780 --> 00:19:32,460
to use items you don't have to create

473
00:19:30,780 --> 00:19:33,720
middlewares you don't have to touch

474
00:19:32,460 --> 00:19:36,419
Pipelines

475
00:19:33,720 --> 00:19:38,880
but you will always have a spider

476
00:19:36,419 --> 00:19:40,980
so you can think of items middlewares

477
00:19:38,880 --> 00:19:43,140
and pipelines are optional but we will

478
00:19:40,980 --> 00:19:45,299
be using them because

479
00:19:43,140 --> 00:19:48,539
if you're scraping anything more than

480
00:19:45,299 --> 00:19:52,799
just one page it becomes a lot easier

481
00:19:48,539 --> 00:19:55,679
just to use the pipelines items in

482
00:19:52,799 --> 00:19:59,820
middlewares and instead of trying to

483
00:19:55,679 --> 00:20:02,400
have everything custom made in a spider

484
00:19:59,820 --> 00:20:05,220
Okay so the next thing we're going to

485
00:20:02,400 --> 00:20:08,220
quickly look at is a fully

486
00:20:05,220 --> 00:20:10,679
fleshed out spider

487
00:20:08,220 --> 00:20:12,960
so just to give you an idea of what

488
00:20:10,679 --> 00:20:15,360
would go into these things like what is

489
00:20:12,960 --> 00:20:17,460
in items what does middlewares mean what

490
00:20:15,360 --> 00:20:20,039
we put in pipelines that's going to go

491
00:20:17,460 --> 00:20:21,059
through some code and give you an

492
00:20:20,039 --> 00:20:23,640
example

493
00:20:21,059 --> 00:20:25,200
so don't be too scared if you don't

494
00:20:23,640 --> 00:20:26,580
understand any of this stuff right now

495
00:20:25,200 --> 00:20:29,880
we're going to be going through all of

496
00:20:26,580 --> 00:20:33,419
it in Parts four five six seven eight

497
00:20:29,880 --> 00:20:36,539
so we start off with our spider

498
00:20:33,419 --> 00:20:39,840
so in here in our spiders folder there's

499
00:20:36,539 --> 00:20:43,100
just a simple spider called book Spider

500
00:20:39,840 --> 00:20:47,400
it's just a simple class it's got a name

501
00:20:43,100 --> 00:20:52,220
it's got some functions there

502
00:20:47,400 --> 00:20:56,820
and inside it has things like items

503
00:20:52,220 --> 00:20:59,880
which link into our items.py file here

504
00:20:56,820 --> 00:21:04,020
as we can see we're importing it

505
00:20:59,880 --> 00:21:05,400
and this is just a basic spider so I'll

506
00:21:04,020 --> 00:21:08,100
just give you a quick overview of what

507
00:21:05,400 --> 00:21:11,580
it does once you run the spider it goes

508
00:21:08,100 --> 00:21:15,419
to start requests and it puts this URL

509
00:21:11,580 --> 00:21:18,059
into this URL variable and then it

510
00:21:15,419 --> 00:21:22,020
returns scraping that request function

511
00:21:18,059 --> 00:21:24,419
with the URL and once the request comes

512
00:21:22,020 --> 00:21:29,100
back from the page with the HTML in it

513
00:21:24,419 --> 00:21:30,299
it goes to the next function which is to

514
00:21:29,100 --> 00:21:32,700
find in

515
00:21:30,299 --> 00:21:36,120
the parse function

516
00:21:32,700 --> 00:21:39,960
so this parse function then

517
00:21:36,120 --> 00:21:43,440
lets us use the response that contains

518
00:21:39,960 --> 00:21:47,159
the HTML and we can then manipulate this

519
00:21:43,440 --> 00:21:49,260
HTML and extract the things that we want

520
00:21:47,159 --> 00:21:53,220
such as the title category description

521
00:21:49,260 --> 00:21:56,880
price once we've got those those pieces

522
00:21:53,220 --> 00:22:00,780
of data are put into our book item and

523
00:21:56,880 --> 00:22:04,020
that is then returned to us in the

524
00:22:00,780 --> 00:22:08,760
console are if you've got other things

525
00:22:04,020 --> 00:22:11,179
set up such as feeds into a file

526
00:22:08,760 --> 00:22:13,679
so that might all be completely

527
00:22:11,179 --> 00:22:15,299
overwhelming for you but don't worry

528
00:22:13,679 --> 00:22:18,360
we're going to be going through all of

529
00:22:15,299 --> 00:22:20,340
this in extreme detail and showing you

530
00:22:18,360 --> 00:22:22,320
exactly how to do everything that is

531
00:22:20,340 --> 00:22:24,419
already here this is just to give you an

532
00:22:22,320 --> 00:22:26,340
idea of what's what

533
00:22:24,419 --> 00:22:28,020
Okay so

534
00:22:26,340 --> 00:22:31,020
you might have seen this book item that

535
00:22:28,020 --> 00:22:35,159
I mentioned so book items

536
00:22:31,020 --> 00:22:37,700
then links into our items.py file and in

537
00:22:35,159 --> 00:22:42,000
that file we just describe

538
00:22:37,700 --> 00:22:44,520
how we want the item to be set up so we

539
00:22:42,000 --> 00:22:47,760
want our book should contain title

540
00:22:44,520 --> 00:22:50,640
category description and price

541
00:22:47,760 --> 00:22:54,120
so then using this we can then use this

542
00:22:50,640 --> 00:22:56,280
book item both in our spiders when we

543
00:22:54,120 --> 00:22:59,940
fill the book item with the different

544
00:22:56,280 --> 00:23:01,860
pieces of data and return it and also in

545
00:22:59,940 --> 00:23:05,100
our Pipelines

546
00:23:01,860 --> 00:23:06,780
so in our Pipelines

547
00:23:05,100 --> 00:23:09,600
we have a simple

548
00:23:06,780 --> 00:23:11,880
test one set up here which goes through

549
00:23:09,600 --> 00:23:15,960
mimicking how you would

550
00:23:11,880 --> 00:23:17,700
then get the data that is returned

551
00:23:15,960 --> 00:23:20,700
um in the book Spider

552
00:23:17,700 --> 00:23:24,299
this book item with all the details and

553
00:23:20,700 --> 00:23:28,799
it goes through how it would save the

554
00:23:24,299 --> 00:23:31,080
item in a database so think of it we

555
00:23:28,799 --> 00:23:35,159
extract the data The Next Step would be

556
00:23:31,080 --> 00:23:38,640
to push the data into the item and then

557
00:23:35,159 --> 00:23:40,260
to put the item into a database

558
00:23:38,640 --> 00:23:43,100
so Pipelines

559
00:23:40,260 --> 00:23:46,860
are what happens once you've extracted

560
00:23:43,100 --> 00:23:49,320
and you're yielding returning the data

561
00:23:46,860 --> 00:23:51,840
from your spider

562
00:23:49,320 --> 00:23:54,360
so here we have for example process item

563
00:23:51,840 --> 00:23:55,559
it's fairly self-explanatory it takes

564
00:23:54,360 --> 00:23:58,500
the item

565
00:23:55,559 --> 00:24:01,620
with the title the category description

566
00:23:58,500 --> 00:24:04,380
and it inserts it into our books

567
00:24:01,620 --> 00:24:07,440
database table

568
00:24:04,380 --> 00:24:11,280
so that's what

569
00:24:07,440 --> 00:24:13,500
gets put into items and item Pipelines

570
00:24:11,280 --> 00:24:15,600
again this could be all very confusing

571
00:24:13,500 --> 00:24:18,059
for you if you know a bit of python

572
00:24:15,600 --> 00:24:20,820
hopefully it shouldn't be too confusing

573
00:24:18,059 --> 00:24:24,900
but we'll be going into it in a lot more

574
00:24:20,820 --> 00:24:27,659
detail later on okay so

575
00:24:24,900 --> 00:24:28,880
then we have our middlewares

576
00:24:27,659 --> 00:24:32,280
so

577
00:24:28,880 --> 00:24:34,860
middlewares are where you can

578
00:24:32,280 --> 00:24:38,700
get into the nitty-gritty of

579
00:24:34,860 --> 00:24:41,820
how you want the spider to operate

580
00:24:38,700 --> 00:24:43,799
so it gives you control over lots of

581
00:24:41,820 --> 00:24:46,260
different things such as

582
00:24:43,799 --> 00:24:49,320
timing out requests how long you want

583
00:24:46,260 --> 00:24:51,900
the request to go on for what headers

584
00:24:49,320 --> 00:24:55,080
you want to send when you make a request

585
00:24:51,900 --> 00:24:57,299
what user agents should be used when you

586
00:24:55,080 --> 00:24:59,520
make a request if you want to do things

587
00:24:57,299 --> 00:25:01,260
like multiple retries you can mess

588
00:24:59,520 --> 00:25:05,400
around with that in the middlewares

589
00:25:01,260 --> 00:25:09,120
section and as you can see it comes with

590
00:25:05,400 --> 00:25:12,120
several kind of defaults that are there

591
00:25:09,120 --> 00:25:14,520
that you can either update to what you

592
00:25:12,120 --> 00:25:16,500
want or you can create your own ones

593
00:25:14,520 --> 00:25:19,340
that go in here too

594
00:25:16,500 --> 00:25:22,320
so you also have managing cookies

595
00:25:19,340 --> 00:25:25,919
caches there's everything like that

596
00:25:22,320 --> 00:25:28,200
would be dealt with in your middlewares

597
00:25:25,919 --> 00:25:30,299
now there's two types of middlewares

598
00:25:28,200 --> 00:25:31,799
there is downloader middlewares and

599
00:25:30,299 --> 00:25:34,080
spider middlewares

600
00:25:31,799 --> 00:25:35,640
most of what we'd be doing would

601
00:25:34,080 --> 00:25:37,020
probably go into the downloader

602
00:25:35,640 --> 00:25:39,659
middlewares

603
00:25:37,020 --> 00:25:42,500
but spider middlewares can also do

604
00:25:39,659 --> 00:25:47,000
things such as adding or removing

605
00:25:42,500 --> 00:25:50,039
requests or items handling different

606
00:25:47,000 --> 00:25:52,140
exceptions that crop up if there's an

607
00:25:50,039 --> 00:25:53,700
error with your spider handling things

608
00:25:52,140 --> 00:25:56,419
like that

609
00:25:53,700 --> 00:25:59,880
so all these middlewares go in the

610
00:25:56,419 --> 00:26:01,380
middlewares.py file and then last of all

611
00:25:59,880 --> 00:26:03,659
we have our settings

612
00:26:01,380 --> 00:26:05,220
so settings is fairly self-explanatory

613
00:26:03,659 --> 00:26:07,320
it's where you

614
00:26:05,220 --> 00:26:10,880
put all your settings so you've got

615
00:26:07,320 --> 00:26:14,039
basic things like do we obey our

616
00:26:10,880 --> 00:26:16,320
robots.txt file when initial request is

617
00:26:14,039 --> 00:26:19,799
made to a website do we check that first

618
00:26:16,320 --> 00:26:22,679
and if it says don't scrape this site do

619
00:26:19,799 --> 00:26:25,260
we obey that yes or no it said here the

620
00:26:22,679 --> 00:26:27,299
number of concurrent requests we make so

621
00:26:25,260 --> 00:26:30,480
if we're scraping a website do we send

622
00:26:27,299 --> 00:26:32,820
one request at a time or do we send 10

623
00:26:30,480 --> 00:26:34,020
or 100 requests at a time that's also

624
00:26:32,820 --> 00:26:37,860
set here

625
00:26:34,020 --> 00:26:41,640
so everything to do with how your spider

626
00:26:37,860 --> 00:26:45,419
and crawling operates will be either

627
00:26:41,640 --> 00:26:48,600
enabled or disabled in this settings.py

628
00:26:45,419 --> 00:26:50,760
file now we also have going back to what

629
00:26:48,600 --> 00:26:53,039
we were talking about our middlewares we

630
00:26:50,760 --> 00:26:55,620
have our spider middlewares

631
00:26:53,039 --> 00:26:58,380
as you can see here and our downloader

632
00:26:55,620 --> 00:27:00,120
middlewares as you can see here so this

633
00:26:58,380 --> 00:27:03,659
is where you can if you create a new

634
00:27:00,120 --> 00:27:06,299
middleware so this one directly links to

635
00:27:03,659 --> 00:27:07,380
the book scraper spider middleware that

636
00:27:06,299 --> 00:27:08,760
is

637
00:27:07,380 --> 00:27:11,580
right here

638
00:27:08,760 --> 00:27:13,559
so you need to make sure if you create a

639
00:27:11,580 --> 00:27:15,840
new middleware that you then enable it

640
00:27:13,559 --> 00:27:19,020
in settings also

641
00:27:15,840 --> 00:27:21,720
and also Verizon pipelines that's also

642
00:27:19,020 --> 00:27:24,960
where you need to enable if you create a

643
00:27:21,720 --> 00:27:26,760
new item pipeline that it is enabled in

644
00:27:24,960 --> 00:27:30,240
here also

645
00:27:26,760 --> 00:27:33,779
okay so I think we've gone through the

646
00:27:30,240 --> 00:27:35,700
basics of a full scrapey project and

647
00:27:33,779 --> 00:27:39,360
what's contained in there we've gone

648
00:27:35,700 --> 00:27:42,000
through what's usually in a spider gone

649
00:27:39,360 --> 00:27:45,179
through items and item pipelines how

650
00:27:42,000 --> 00:27:49,020
they can process the data once we've

651
00:27:45,179 --> 00:27:52,799
scraped the data from a page and then

652
00:27:49,020 --> 00:27:54,900
we've looked at middlewares and how in

653
00:27:52,799 --> 00:27:56,700
settings we can turn everything on or

654
00:27:54,900 --> 00:27:58,260
off

655
00:27:56,700 --> 00:28:01,200
so I think that's everything we wanted

656
00:27:58,260 --> 00:28:03,539
to cover in this part now again don't be

657
00:28:01,200 --> 00:28:07,140
too overwhelmed by this

658
00:28:03,539 --> 00:28:10,620
it does get a lot easier trust me so

659
00:28:07,140 --> 00:28:14,100
stick with it and in part four we'll be

660
00:28:10,620 --> 00:28:17,960
creating our first spider and extracting

661
00:28:14,100 --> 00:28:17,960
some data from a web page

662
00:28:20,820 --> 00:28:25,200
in part four of our scrapey beginners

663
00:28:22,980 --> 00:28:26,700
course we're going to look at how to

664
00:28:25,200 --> 00:28:30,000
create a scrapey spider

665
00:28:26,700 --> 00:28:31,320
using the Scrapy shell to find the CSS

666
00:28:30,000 --> 00:28:33,840
selectors we need

667
00:28:31,320 --> 00:28:36,840
using those CSS selectors in our spider

668
00:28:33,840 --> 00:28:39,720
to extract the data we want from the

669
00:28:36,840 --> 00:28:42,299
page and then finally we're going to get

670
00:28:39,720 --> 00:28:47,000
our spider to go through multiple pages

671
00:28:42,299 --> 00:28:47,000
and extract data from multiple pages

672
00:28:47,340 --> 00:28:52,860
so let's get going so I've got my

673
00:28:50,880 --> 00:28:55,380
terminal open here I've already

674
00:28:52,860 --> 00:28:58,080
activated my virtual environment I'm

675
00:28:55,380 --> 00:28:59,520
continuing on from part three so if

676
00:28:58,080 --> 00:29:01,919
you're just joining us here make sure

677
00:28:59,520 --> 00:29:03,779
you already have everything set up as we

678
00:29:01,919 --> 00:29:06,600
have done in part three

679
00:29:03,779 --> 00:29:08,220
I want to go all the way down into my

680
00:29:06,600 --> 00:29:11,220
spider's folder so at the moment it's

681
00:29:08,220 --> 00:29:16,080
empty it's just got an init.py file in

682
00:29:11,220 --> 00:29:18,480
it so we want to go down the level and

683
00:29:16,080 --> 00:29:21,600
down into the spiders so now I'm in my

684
00:29:18,480 --> 00:29:25,320
spiders folder and I can see there's

685
00:29:21,600 --> 00:29:27,600
just a new py there so in this spider's

686
00:29:25,320 --> 00:29:31,380
folder I'm going to run

687
00:29:27,600 --> 00:29:33,779
this command scrapey gen spider

688
00:29:31,380 --> 00:29:36,480
the name of my spider which I'm going to

689
00:29:33,779 --> 00:29:39,720
call book spider and then

690
00:29:36,480 --> 00:29:42,120
the URL of the website that we're going

691
00:29:39,720 --> 00:29:43,200
to be scraping and in this case it's

692
00:29:42,120 --> 00:29:48,360
going to be

693
00:29:43,200 --> 00:29:50,399
the books.2 scrape.com site which is a

694
00:29:48,360 --> 00:29:53,340
site that is there for people to

695
00:29:50,399 --> 00:29:54,720
practice their scraping up

696
00:29:53,340 --> 00:29:56,880
so

697
00:29:54,720 --> 00:29:59,520
if you go ahead and go down to your

698
00:29:56,880 --> 00:30:00,899
spiders folder and type this command

699
00:29:59,520 --> 00:30:03,419
into your terminal

700
00:30:00,899 --> 00:30:06,360
and hit enter

701
00:30:03,419 --> 00:30:08,039
scrapey will then create this spider as

702
00:30:06,360 --> 00:30:10,860
you can see here created spider book

703
00:30:08,039 --> 00:30:14,820
Spider using template basic in module

704
00:30:10,860 --> 00:30:16,399
and then it gives this so if we check

705
00:30:14,820 --> 00:30:19,799
that out now we can see that book

706
00:30:16,399 --> 00:30:24,299
spider.py is there and if I open this up

707
00:30:19,799 --> 00:30:27,120
in my vs code we can see it created

708
00:30:24,299 --> 00:30:30,899
book Spider here so

709
00:30:27,120 --> 00:30:33,240
this is just a very very basic spider

710
00:30:30,899 --> 00:30:36,240
we'll be adding a lot more to this

711
00:30:33,240 --> 00:30:39,480
but we'll just go through a few bits of

712
00:30:36,240 --> 00:30:42,779
what were generated here so obviously

713
00:30:39,480 --> 00:30:45,539
the name of our spider is book Spider so

714
00:30:42,779 --> 00:30:48,059
when we do Scrapy crawl to actually kick

715
00:30:45,539 --> 00:30:51,600
the spider off using scrapey we'll be

716
00:30:48,059 --> 00:30:54,059
doing scrapey crawl book Spider

717
00:30:51,600 --> 00:30:57,179
the allow domains list

718
00:30:54,059 --> 00:31:00,240
is books.2script.com

719
00:30:57,179 --> 00:31:02,100
this is important because later on when

720
00:31:00,240 --> 00:31:03,419
we're going to be doing crawling our

721
00:31:02,100 --> 00:31:05,640
spider is going to be going through

722
00:31:03,419 --> 00:31:08,880
multiple different links

723
00:31:05,640 --> 00:31:10,860
and having this allowed domains here

724
00:31:08,880 --> 00:31:13,919
listing only the domain we want to

725
00:31:10,860 --> 00:31:16,380
scrape prevents our spider from going

726
00:31:13,919 --> 00:31:18,299
off and scraping hundreds of different

727
00:31:16,380 --> 00:31:20,880
websites across the internet because as

728
00:31:18,299 --> 00:31:22,980
you can imagine URLs link from one page

729
00:31:20,880 --> 00:31:25,020
to another and sometimes a website might

730
00:31:22,980 --> 00:31:27,779
link to an outside website

731
00:31:25,020 --> 00:31:29,820
and in this case you would not want your

732
00:31:27,779 --> 00:31:31,260
spider to start crawling and scraping

733
00:31:29,820 --> 00:31:33,240
the entire internet

734
00:31:31,260 --> 00:31:36,600
so that's why we have allowed domains

735
00:31:33,240 --> 00:31:40,620
here next we have start URLs so this is

736
00:31:36,600 --> 00:31:43,020
usually just the first URL that the

737
00:31:40,620 --> 00:31:45,120
spider starts scraping but you can

738
00:31:43,020 --> 00:31:48,600
actually have multiple URLs here as well

739
00:31:45,120 --> 00:31:50,700
for it to go through one after the other

740
00:31:48,600 --> 00:31:52,860
then we have our parse function our

741
00:31:50,700 --> 00:31:55,500
parse function is

742
00:31:52,860 --> 00:31:58,320
the function that gets called once the

743
00:31:55,500 --> 00:32:00,960
response comes back so we'll be filling

744
00:31:58,320 --> 00:32:03,240
this parse function with all the

745
00:32:00,960 --> 00:32:06,360
different pieces we want to extract the

746
00:32:03,240 --> 00:32:10,260
data from the page itself

747
00:32:06,360 --> 00:32:12,600
Okay so we've gone through the basics of

748
00:32:10,260 --> 00:32:13,980
this generated spider the next thing

749
00:32:12,600 --> 00:32:15,539
we're going to do is we're going to use

750
00:32:13,980 --> 00:32:19,020
the scrapey shell

751
00:32:15,539 --> 00:32:23,700
to find the CSS selectors

752
00:32:19,020 --> 00:32:26,760
we want to get the data from the page

753
00:32:23,700 --> 00:32:29,220
so what I mean by CSS selectors for

754
00:32:26,760 --> 00:32:31,580
those of you who aren't familiar so

755
00:32:29,220 --> 00:32:33,480
first of all if you just open your

756
00:32:31,580 --> 00:32:36,419
developer tools

757
00:32:33,480 --> 00:32:39,059
you can do this by right clicking in

758
00:32:36,419 --> 00:32:40,919
Chrome or Safari or Firefox and it's

759
00:32:39,059 --> 00:32:42,779
usually inspect or sometimes it's called

760
00:32:40,919 --> 00:32:45,659
developer tools

761
00:32:42,779 --> 00:32:47,580
so you do that and this comes up here if

762
00:32:45,659 --> 00:32:52,559
you go to the elements tab you'll see

763
00:32:47,580 --> 00:32:54,360
then all the makeup of the page in HTML

764
00:32:52,559 --> 00:32:57,360
and CSS

765
00:32:54,360 --> 00:33:00,779
so here for example we've got a H3 tag

766
00:32:57,360 --> 00:33:05,880
and an a tag for links and this is the

767
00:33:00,779 --> 00:33:09,419
link to the page of this book here

768
00:33:05,880 --> 00:33:12,600
so we'll be looking now at how we can

769
00:33:09,419 --> 00:33:15,360
actually pick out these tags so that

770
00:33:12,600 --> 00:33:17,940
scrapey knows which pieces of data we

771
00:33:15,360 --> 00:33:20,880
want to extract from the page itself

772
00:33:17,940 --> 00:33:23,880
okay so let's just go back to our

773
00:33:20,880 --> 00:33:26,100
terminal and just to make using the

774
00:33:23,880 --> 00:33:27,899
Scrapy shell a little bit easier we're

775
00:33:26,100 --> 00:33:32,940
going to do

776
00:33:27,899 --> 00:33:37,559
just a pip install and then I python

777
00:33:32,940 --> 00:33:40,799
which just is a different shell would

778
00:33:37,559 --> 00:33:43,440
help if I spelled it correctly this is

779
00:33:40,799 --> 00:33:47,279
just a different shell which is a bit

780
00:33:43,440 --> 00:33:50,399
easier to read so I just did pip install

781
00:33:47,279 --> 00:33:52,500
IPython and then to activate this we

782
00:33:50,399 --> 00:33:55,140
want to go to settings

783
00:33:52,500 --> 00:33:57,659
and

784
00:33:55,140 --> 00:33:59,240
we I know we want to go to sorry not

785
00:33:57,659 --> 00:34:00,860
settings this

786
00:33:59,240 --> 00:34:04,260
scrapey.cfg

787
00:34:00,860 --> 00:34:10,619
and we're going to add

788
00:34:04,260 --> 00:34:14,339
the shell as a separate line here so

789
00:34:10,619 --> 00:34:17,820
now that that's done I can close Dash

790
00:34:14,339 --> 00:34:23,159
and we can run scrapey shell

791
00:34:17,820 --> 00:34:24,839
Scrappy shell then gives us this so what

792
00:34:23,159 --> 00:34:27,599
we want to do now we have scrape your

793
00:34:24,839 --> 00:34:29,760
shell open is as you can see we've got a

794
00:34:27,599 --> 00:34:32,460
list of the commands that it gives us

795
00:34:29,760 --> 00:34:35,339
that are available so we can do

796
00:34:32,460 --> 00:34:37,320
useful shortcuts fetch which is the

797
00:34:35,339 --> 00:34:41,280
command that we're going to be using

798
00:34:37,320 --> 00:34:42,780
so this fetches a URL and updates

799
00:34:41,280 --> 00:34:44,760
the

800
00:34:42,780 --> 00:34:49,200
local objects

801
00:34:44,760 --> 00:34:53,159
so we'll just run this fetch command now

802
00:34:49,200 --> 00:34:55,800
and I'll show you exactly what I mean

803
00:34:53,159 --> 00:34:57,540
so we want to fetch this books to

804
00:34:55,800 --> 00:34:59,940
scrape.com

805
00:34:57,540 --> 00:35:03,720
so I'm just going to paste in

806
00:34:59,940 --> 00:35:06,300
this URL here and

807
00:35:03,720 --> 00:35:09,980
it's going to go off it's going to fetch

808
00:35:06,300 --> 00:35:14,040
this and it's going to put the resulting

809
00:35:09,980 --> 00:35:18,660
HTML everything in here that we see

810
00:35:14,040 --> 00:35:21,300
into a variable inside in the scrapey

811
00:35:18,660 --> 00:35:23,880
Shell so we can access it and run

812
00:35:21,300 --> 00:35:26,880
different commands on it

813
00:35:23,880 --> 00:35:29,579
so enables us to kind of practice the

814
00:35:26,880 --> 00:35:32,280
code we want to then put into our spider

815
00:35:29,579 --> 00:35:34,200
so what we want to do is it put

816
00:35:32,280 --> 00:35:35,700
everything in from the page into this

817
00:35:34,200 --> 00:35:37,700
response

818
00:35:35,700 --> 00:35:41,640
variables so now we can just do

819
00:35:37,700 --> 00:35:44,099
response.css and

820
00:35:41,640 --> 00:35:47,160
let's say we're going to look for

821
00:35:44,099 --> 00:35:49,859
something specific on the page so what

822
00:35:47,160 --> 00:35:51,720
we can do is move our Mouse over these

823
00:35:49,859 --> 00:35:54,599
different

824
00:35:51,720 --> 00:35:56,160
tags on the page we can see article

825
00:35:54,599 --> 00:35:57,480
contains

826
00:35:56,160 --> 00:36:00,960
one

827
00:35:57,480 --> 00:36:04,920
book on the page so we can just say okay

828
00:36:00,960 --> 00:36:04,920
give me article

829
00:36:05,839 --> 00:36:12,180
and we'll just have the class name in as

830
00:36:09,780 --> 00:36:14,660
well so any class name

831
00:36:12,180 --> 00:36:19,260
needs to have just dot in front of it

832
00:36:14,660 --> 00:36:21,359
when we're referring to it like this

833
00:36:19,260 --> 00:36:24,240
so that has given us

834
00:36:21,359 --> 00:36:25,380
all the different books that are on the

835
00:36:24,240 --> 00:36:27,180
page

836
00:36:25,380 --> 00:36:29,579
now let's say we want to just get the

837
00:36:27,180 --> 00:36:33,420
first book we can just do this

838
00:36:29,579 --> 00:36:35,760
with a guess and it's giving us just the

839
00:36:33,420 --> 00:36:39,540
HTML that is

840
00:36:35,760 --> 00:36:42,180
for that first book if we want to then

841
00:36:39,540 --> 00:36:44,099
put all the books into a different

842
00:36:42,180 --> 00:36:46,020
variable so we can run some other

843
00:36:44,099 --> 00:36:48,720
commands on them within the scrapey

844
00:36:46,020 --> 00:36:51,680
shell we can do something like

845
00:36:48,720 --> 00:36:55,980
books is equal to

846
00:36:51,680 --> 00:36:59,820
response.cssarticle dot product pod

847
00:36:55,980 --> 00:37:02,640
if we do that it's after putting all the

848
00:36:59,820 --> 00:37:06,300
different books into this books variable

849
00:37:02,640 --> 00:37:09,900
so then if we run Len

850
00:37:06,300 --> 00:37:11,099
on books then in Python gets us the

851
00:37:09,900 --> 00:37:13,680
length

852
00:37:11,099 --> 00:37:15,420
so it gives us that there's 20 books if

853
00:37:13,680 --> 00:37:18,660
we go back to our page

854
00:37:15,420 --> 00:37:20,640
we can indeed see there is 20 books so

855
00:37:18,660 --> 00:37:24,060
there's four in each Road there's one

856
00:37:20,640 --> 00:37:27,180
two three four five rows showing one of

857
00:37:24,060 --> 00:37:28,859
20 so that's correct

858
00:37:27,180 --> 00:37:31,380
Okay so

859
00:37:28,859 --> 00:37:33,839
for the purposes of this part four we're

860
00:37:31,380 --> 00:37:38,280
going to extract the name of the book

861
00:37:33,839 --> 00:37:40,079
the price of the book and the URL so we

862
00:37:38,280 --> 00:37:41,760
can actually go in and get further

863
00:37:40,079 --> 00:37:45,119
details later

864
00:37:41,760 --> 00:37:48,780
so the name the price and the URL

865
00:37:45,119 --> 00:37:50,280
so now that we've got our books what we

866
00:37:48,780 --> 00:37:55,160
can do is

867
00:37:50,280 --> 00:37:55,160
we can put the first book

868
00:37:55,260 --> 00:38:00,380
of the list of books

869
00:37:58,260 --> 00:38:04,280
so we'll make a new variable called book

870
00:38:00,380 --> 00:38:06,839
and we'll say that's equal to

871
00:38:04,280 --> 00:38:10,920
equal to books

872
00:38:06,839 --> 00:38:11,940
and the first item in the list of books

873
00:38:10,920 --> 00:38:15,180
that we have

874
00:38:11,940 --> 00:38:17,880
so now if I do

875
00:38:15,180 --> 00:38:19,680
book.css

876
00:38:17,880 --> 00:38:23,160
and then

877
00:38:19,680 --> 00:38:27,119
I go back and okay we want to get the

878
00:38:23,160 --> 00:38:30,359
the title of the book here so I can see

879
00:38:27,119 --> 00:38:33,540
from this that we've got a H3 tag and

880
00:38:30,359 --> 00:38:37,920
we've got an A tag and I want the text

881
00:38:33,540 --> 00:38:40,980
that is within this a tag here so I'm

882
00:38:37,920 --> 00:38:44,700
going to do H3 and a

883
00:38:40,980 --> 00:38:47,460
and that should get me the text that I'm

884
00:38:44,700 --> 00:38:49,380
looking for the title of the book

885
00:38:47,460 --> 00:38:54,180
so go back here

886
00:38:49,380 --> 00:38:57,900
and I do H3 a and then we just need a

887
00:38:54,180 --> 00:38:59,820
little bit extra which is just this text

888
00:38:57,900 --> 00:39:02,099
and I do get

889
00:38:59,820 --> 00:39:05,339
that gets me exactly what I was looking

890
00:39:02,099 --> 00:39:08,579
for which is a light in the dot dot dot

891
00:39:05,339 --> 00:39:11,040
this corresponds exactly to this so I've

892
00:39:08,579 --> 00:39:15,480
got the title of my book

893
00:39:11,040 --> 00:39:18,660
next I want to get the price of my book

894
00:39:15,480 --> 00:39:20,160
so I'm going to just remove these two

895
00:39:18,660 --> 00:39:23,220
pieces here

896
00:39:20,160 --> 00:39:27,359
and I'm going to inspect the price

897
00:39:23,220 --> 00:39:30,660
okay so if we look at dot product price

898
00:39:27,359 --> 00:39:34,380
and then dot price underscore color

899
00:39:30,660 --> 00:39:36,540
should give us the price so let's do

900
00:39:34,380 --> 00:39:38,880
that now so

901
00:39:36,540 --> 00:39:42,619
that's

902
00:39:38,880 --> 00:39:42,619
Dash product price

903
00:39:43,160 --> 00:39:50,040
color let's try and run that

904
00:39:47,099 --> 00:39:52,920
okay it didn't

905
00:39:50,040 --> 00:39:55,859
exactly what I I know because I S I did

906
00:39:52,920 --> 00:39:58,619
an extra double dot there you go so that

907
00:39:55,859 --> 00:40:02,400
got us exactly the price we're looking

908
00:39:58,619 --> 00:40:04,380
for and finally we want to get the URL

909
00:40:02,400 --> 00:40:06,180
so the URL

910
00:40:04,380 --> 00:40:08,820
is

911
00:40:06,180 --> 00:40:12,780
interesting because it's also part of

912
00:40:08,820 --> 00:40:14,880
this h3a tag but we want instead of the

913
00:40:12,780 --> 00:40:18,119
text within it we want this href

914
00:40:14,880 --> 00:40:21,660
attribute here which contains the part

915
00:40:18,119 --> 00:40:22,680
of the link to more information on the

916
00:40:21,660 --> 00:40:25,740
actual

917
00:40:22,680 --> 00:40:28,200
book itself so if we open this up in a

918
00:40:25,740 --> 00:40:32,220
new tab we'll see the full page that

919
00:40:28,200 --> 00:40:34,980
we're looking for and here you can see

920
00:40:32,220 --> 00:40:36,780
full project description and lots more

921
00:40:34,980 --> 00:40:38,700
details there

922
00:40:36,780 --> 00:40:41,599
Okay so

923
00:40:38,700 --> 00:40:44,940
we still want to do

924
00:40:41,599 --> 00:40:49,500
H3 and a

925
00:40:44,940 --> 00:40:52,200
but instead of text we're going to say

926
00:40:49,500 --> 00:40:54,720
a

927
00:40:52,200 --> 00:40:58,740
a trib

928
00:40:54,720 --> 00:41:00,000
e hatred that gives us our href

929
00:40:58,740 --> 00:41:04,079
attributes

930
00:41:00,000 --> 00:41:07,380
that was contained in this a tag that we

931
00:41:04,079 --> 00:41:08,940
were looking at a second ago so using

932
00:41:07,380 --> 00:41:12,900
the Scrapy shell we've managed to see

933
00:41:08,940 --> 00:41:16,800
how we can use the CSS selectors to

934
00:41:12,900 --> 00:41:18,960
extract the title the price and the URL

935
00:41:16,800 --> 00:41:21,240
for one book

936
00:41:18,960 --> 00:41:23,280
so now that we know that we can add

937
00:41:21,240 --> 00:41:26,280
these into our parse function

938
00:41:23,280 --> 00:41:28,380
and we can also Loop through all that

939
00:41:26,280 --> 00:41:30,420
list of books and get all the details

940
00:41:28,380 --> 00:41:32,460
for the 20 books that are on the page

941
00:41:30,420 --> 00:41:35,880
Okay so

942
00:41:32,460 --> 00:41:38,520
let's start adding things to our parse

943
00:41:35,880 --> 00:41:41,820
function so first I'm just going to add

944
00:41:38,520 --> 00:41:43,260
in what we initially had to get all the

945
00:41:41,820 --> 00:41:46,079
books

946
00:41:43,260 --> 00:41:50,700
that were there and that is

947
00:41:46,079 --> 00:41:54,000
books equals to response dot CSS article

948
00:41:50,700 --> 00:41:55,619
and then product underscore pod so we

949
00:41:54,000 --> 00:41:58,220
had that

950
00:41:55,619 --> 00:41:58,220
up

951
00:41:58,740 --> 00:42:02,820
here

952
00:42:00,300 --> 00:42:04,440
so I'm just taking this line here that

953
00:42:02,820 --> 00:42:07,500
we used in our Scrapy shell and I'm

954
00:42:04,440 --> 00:42:10,079
putting it in to our parse

955
00:42:07,500 --> 00:42:12,300
function okay the next thing we want to

956
00:42:10,079 --> 00:42:14,880
do is we're just going to Loop through

957
00:42:12,300 --> 00:42:18,420
it so we just want

958
00:42:14,880 --> 00:42:23,579
four book in books

959
00:42:18,420 --> 00:42:27,359
and then we are going to type yield

960
00:42:23,579 --> 00:42:30,119
so yield is like return

961
00:42:27,359 --> 00:42:32,460
and then what we want is scrapey to

962
00:42:30,119 --> 00:42:34,380
return to us is going to be the name the

963
00:42:32,460 --> 00:42:36,780
price and URL

964
00:42:34,380 --> 00:42:40,200
so we'll start with the name and then

965
00:42:36,780 --> 00:42:42,839
we're going to go up to where we

966
00:42:40,200 --> 00:42:44,040
got our text and we're going to use this

967
00:42:42,839 --> 00:42:45,960
exact

968
00:42:44,040 --> 00:42:50,240
piece here

969
00:42:45,960 --> 00:42:53,640
and then we're going to get our price

970
00:42:50,240 --> 00:42:56,060
and we're going to go to where we got

971
00:42:53,640 --> 00:42:56,060
our price

972
00:42:59,339 --> 00:43:05,119
and then

973
00:43:01,020 --> 00:43:05,119
last of all our URL

974
00:43:05,339 --> 00:43:12,119
and for that we have our

975
00:43:09,380 --> 00:43:13,380
href attributes

976
00:43:12,119 --> 00:43:15,839
okay

977
00:43:13,380 --> 00:43:18,900
now that we have that

978
00:43:15,839 --> 00:43:21,720
we should be able to go ahead and run

979
00:43:18,900 --> 00:43:24,720
our spider and see what happens so first

980
00:43:21,720 --> 00:43:25,920
let's exit our scrapey Shell by typing

981
00:43:24,720 --> 00:43:30,660
exit

982
00:43:25,920 --> 00:43:32,280
and then we might need to go up a level

983
00:43:30,660 --> 00:43:34,920
to our

984
00:43:32,280 --> 00:43:36,180
book scraper folder and we should be

985
00:43:34,920 --> 00:43:38,160
able to run

986
00:43:36,180 --> 00:43:40,740
scrapey crawl

987
00:43:38,160 --> 00:43:42,300
book Spider which is the name of our

988
00:43:40,740 --> 00:43:43,800
spider

989
00:43:42,300 --> 00:43:47,700
so

990
00:43:43,800 --> 00:43:49,980
if that goes according to plan

991
00:43:47,700 --> 00:43:53,040
we should see item script count of 20

992
00:43:49,980 --> 00:43:56,099
there are the 20 books on the page and

993
00:43:53,040 --> 00:43:59,339
you can see what was returned here the

994
00:43:56,099 --> 00:44:04,020
name there is a book name a price there

995
00:43:59,339 --> 00:44:06,359
is a price and URL there's the URL and

996
00:44:04,020 --> 00:44:09,420
if we just scroll up we can see that the

997
00:44:06,359 --> 00:44:12,000
20 books that were on the page all the

998
00:44:09,420 --> 00:44:14,640
data was script and output to our

999
00:44:12,000 --> 00:44:16,020
terminal so that worked exactly how we

1000
00:44:14,640 --> 00:44:18,599
wanted it to work

1001
00:44:16,020 --> 00:44:22,380
now as you've seen we have multiple

1002
00:44:18,599 --> 00:44:24,599
Pages there's not just this one page of

1003
00:44:22,380 --> 00:44:26,640
20 books there is actually a lot more

1004
00:44:24,599 --> 00:44:28,500
than that so we're going to look at how

1005
00:44:26,640 --> 00:44:29,220
we can go to the next page if there is

1006
00:44:28,500 --> 00:44:31,560
one

1007
00:44:29,220 --> 00:44:34,079
and then scrape all the books on the

1008
00:44:31,560 --> 00:44:36,900
next page and then keep looping through

1009
00:44:34,079 --> 00:44:40,920
all the pages of books until there are

1010
00:44:36,900 --> 00:44:43,020
no more pages of books to scrape

1011
00:44:40,920 --> 00:44:47,339
so

1012
00:44:43,020 --> 00:44:49,800
as you can see here we have a next page

1013
00:44:47,339 --> 00:44:51,960
at the bottom of every

1014
00:44:49,800 --> 00:44:54,980
page of books so if we click the next

1015
00:44:51,960 --> 00:44:58,020
page button it goes to catalog

1016
00:44:54,980 --> 00:45:02,160
page2.html and then we have a new page

1017
00:44:58,020 --> 00:45:04,140
of 20 different books and as you can see

1018
00:45:02,160 --> 00:45:06,599
it's going through all the different

1019
00:45:04,140 --> 00:45:10,800
pages page three and there's a previous

1020
00:45:06,599 --> 00:45:12,780
there as well to go back a page so

1021
00:45:10,800 --> 00:45:17,700
we're going to want

1022
00:45:12,780 --> 00:45:19,099
scrapey to bring us to page Dash three

1023
00:45:17,700 --> 00:45:22,319
our

1024
00:45:19,099 --> 00:45:23,880
page-4.html if there is another one to

1025
00:45:22,319 --> 00:45:27,599
scrape

1026
00:45:23,880 --> 00:45:31,579
so we're going to go back and we're

1027
00:45:27,599 --> 00:45:31,579
going to do scrape your shell again

1028
00:45:32,240 --> 00:45:39,000
to open our shell we're going to again

1029
00:45:35,640 --> 00:45:41,099
roll in our fetch command

1030
00:45:39,000 --> 00:45:46,500
to fetch

1031
00:45:41,099 --> 00:45:51,119
our website URL and then we are going to

1032
00:45:46,500 --> 00:45:52,740
try and get the link so to do that we're

1033
00:45:51,119 --> 00:45:55,859
going to inspect

1034
00:45:52,740 --> 00:46:00,480
the next button and as we can see here

1035
00:45:55,859 --> 00:46:03,060
it's in An Li tag and then it's got a

1036
00:46:00,480 --> 00:46:05,640
class name of next and then within that

1037
00:46:03,060 --> 00:46:09,300
we want the

1038
00:46:05,640 --> 00:46:12,839
link which is contained in this href

1039
00:46:09,300 --> 00:46:14,700
attribute and that's contained in an a

1040
00:46:12,839 --> 00:46:18,000
tag for links

1041
00:46:14,700 --> 00:46:21,920
so let's see if we can get that now

1042
00:46:18,000 --> 00:46:21,920
using our scripture

1043
00:46:21,960 --> 00:46:31,260
so we do response.css and then

1044
00:46:26,599 --> 00:46:34,200
allies dot next so Ally for the Ally tag

1045
00:46:31,260 --> 00:46:39,000
dot next for the class name and then a

1046
00:46:34,200 --> 00:46:40,740
and then we want the href attribute

1047
00:46:39,000 --> 00:46:45,319
so

1048
00:46:40,740 --> 00:46:45,319
so that c can me to Dash

1049
00:46:45,420 --> 00:46:52,460
and that gives us exactly our catalog

1050
00:46:48,599 --> 00:46:56,760
forward slash page 2.html which

1051
00:46:52,460 --> 00:46:58,740
corresponds to well this was not the

1052
00:46:56,760 --> 00:47:00,839
exact one we're looking at we're looking

1053
00:46:58,740 --> 00:47:04,700
at page one

1054
00:47:00,839 --> 00:47:04,700
so I can just remove that

1055
00:47:05,640 --> 00:47:13,980
and go down here and this one should

1056
00:47:09,720 --> 00:47:17,640
have catalog for slash page 2.html

1057
00:47:13,980 --> 00:47:21,300
and that corresponds to this

1058
00:47:17,640 --> 00:47:24,900
so now that we know how we can get the

1059
00:47:21,300 --> 00:47:27,060
next page we can just

1060
00:47:24,900 --> 00:47:29,099
put in under our Loop

1061
00:47:27,060 --> 00:47:31,800
we're just going to paste what we had

1062
00:47:29,099 --> 00:47:33,180
here to get our

1063
00:47:31,800 --> 00:47:36,180
link

1064
00:47:33,180 --> 00:47:38,700
and we're just going to do next

1065
00:47:36,180 --> 00:47:41,099
page is equal to

1066
00:47:38,700 --> 00:47:44,280
and this is going to contain our next

1067
00:47:41,099 --> 00:47:46,200
page link so

1068
00:47:44,280 --> 00:47:48,119
the next thing we need to check for is

1069
00:47:46,200 --> 00:47:49,980
if we get to the last page

1070
00:47:48,119 --> 00:47:53,040
there's going to be no more next page

1071
00:47:49,980 --> 00:47:56,040
link so that's how we can know

1072
00:47:53,040 --> 00:47:59,220
when we've reached the end

1073
00:47:56,040 --> 00:48:02,760
so we can check that by

1074
00:47:59,220 --> 00:48:04,440
going to page 50.

1075
00:48:02,760 --> 00:48:07,319
so if I

1076
00:48:04,440 --> 00:48:10,020
type in page

1077
00:48:07,319 --> 00:48:13,560
50 and

1078
00:48:10,020 --> 00:48:16,020
go to the bottom I should see that there

1079
00:48:13,560 --> 00:48:17,760
is no more next button there's a

1080
00:48:16,020 --> 00:48:20,940
previous button but there's no next

1081
00:48:17,760 --> 00:48:22,560
button so I've reached the end so that's

1082
00:48:20,940 --> 00:48:24,240
what our test is going to be we're going

1083
00:48:22,560 --> 00:48:27,540
to put in an if statement and we're

1084
00:48:24,240 --> 00:48:28,440
going to say if the next page URL is not

1085
00:48:27,540 --> 00:48:31,619
none

1086
00:48:28,440 --> 00:48:34,500
then we know there's another page so we

1087
00:48:31,619 --> 00:48:36,540
can continue going until there is no

1088
00:48:34,500 --> 00:48:39,480
more pages left to scrape

1089
00:48:36,540 --> 00:48:40,920
so let's add that in now okay we're just

1090
00:48:39,480 --> 00:48:45,000
going to do if

1091
00:48:40,920 --> 00:48:46,680
next page is not none

1092
00:48:45,000 --> 00:48:49,200
and then

1093
00:48:46,680 --> 00:48:52,380
next page

1094
00:48:49,200 --> 00:48:56,040
URL is equal to

1095
00:48:52,380 --> 00:48:58,319
and here we're going to create the full

1096
00:48:56,040 --> 00:49:00,240
URL because next page doesn't contain

1097
00:48:58,319 --> 00:49:02,460
the full URL it's only

1098
00:49:00,240 --> 00:49:05,839
a relative URL

1099
00:49:02,460 --> 00:49:09,960
so we need to get

1100
00:49:05,839 --> 00:49:12,480
this part of the URL plus the catalog

1101
00:49:09,960 --> 00:49:14,900
forward slash whatever the next part of

1102
00:49:12,480 --> 00:49:14,900
the pages

1103
00:49:15,119 --> 00:49:19,339
so

1104
00:49:16,200 --> 00:49:19,339
let's add that in

1105
00:49:19,440 --> 00:49:27,000
save that

1106
00:49:21,859 --> 00:49:28,680
and then the important part is we need

1107
00:49:27,000 --> 00:49:30,800
to do

1108
00:49:28,680 --> 00:49:34,880
yield

1109
00:49:30,800 --> 00:49:34,880
response dot follow

1110
00:49:37,200 --> 00:49:43,040
next page URL

1111
00:49:39,540 --> 00:49:43,040
and then call back

1112
00:49:43,099 --> 00:49:48,119
is equal to self Dot

1113
00:49:46,560 --> 00:49:49,380
parse

1114
00:49:48,119 --> 00:49:51,420
okay

1115
00:49:49,380 --> 00:49:53,760
so what this does is it obviously

1116
00:49:51,420 --> 00:49:57,300
creates our next page URL

1117
00:49:53,760 --> 00:49:59,660
and then we tell scrapey to

1118
00:49:57,300 --> 00:50:04,260
go to this next page URL using

1119
00:49:59,660 --> 00:50:06,540
response.follow and the Callback is the

1120
00:50:04,260 --> 00:50:10,800
function that's going to get executed

1121
00:50:06,540 --> 00:50:12,420
once the response comes back from the

1122
00:50:10,800 --> 00:50:14,520
URL that we've gone to

1123
00:50:12,420 --> 00:50:17,160
so once we get the response from that

1124
00:50:14,520 --> 00:50:21,599
URL it's going to kick off self.parse

1125
00:50:17,160 --> 00:50:23,460
and self.parse is this function again so

1126
00:50:21,599 --> 00:50:24,540
it's going to keep going through and

1127
00:50:23,460 --> 00:50:26,940
keep going through and keep going

1128
00:50:24,540 --> 00:50:30,060
through and calling itself until there

1129
00:50:26,940 --> 00:50:32,640
is no more pages and then in that case

1130
00:50:30,060 --> 00:50:36,540
it's going to stop

1131
00:50:32,640 --> 00:50:39,420
let's try and run that now so let's exit

1132
00:50:36,540 --> 00:50:42,500
out of our scrapey shell and let's just

1133
00:50:39,420 --> 00:50:42,500
do a scrapey crawl again

1134
00:50:42,900 --> 00:50:47,660
scrape your crawl and book Spider

1135
00:50:48,359 --> 00:50:51,559
and see what we get

1136
00:50:52,440 --> 00:50:57,300
okay

1137
00:50:53,700 --> 00:50:59,940
so with an item script count of 40

1138
00:50:57,300 --> 00:51:02,819
so it scraped four pages but obviously

1139
00:50:59,940 --> 00:51:04,740
four pages is not 50 pages

1140
00:51:02,819 --> 00:51:06,780
so there's a bit of a bug here which

1141
00:51:04,740 --> 00:51:10,380
we're going to have to get to the bottom

1142
00:51:06,780 --> 00:51:12,660
of let's start looking at the next page

1143
00:51:10,380 --> 00:51:15,420
URL because that's obviously where it's

1144
00:51:12,660 --> 00:51:17,700
going wrong if it's only finding four

1145
00:51:15,420 --> 00:51:21,180
pages there must be an issue with the

1146
00:51:17,700 --> 00:51:24,480
URL here so if we go back and we again

1147
00:51:21,180 --> 00:51:27,599
inspect the element so here we can see

1148
00:51:24,480 --> 00:51:27,599
it's page-50.html

1149
00:51:27,780 --> 00:51:32,339
and

1150
00:51:29,880 --> 00:51:36,059
I think we had

1151
00:51:32,339 --> 00:51:39,359
and here it's 49 but then if we were on

1152
00:51:36,059 --> 00:51:42,839
the initial page

1153
00:51:39,359 --> 00:51:44,760
and we check here it's got catalog

1154
00:51:42,839 --> 00:51:47,460
forward slash page two

1155
00:51:44,760 --> 00:51:50,780
so sometimes it's got just page Dash 2

1156
00:51:47,460 --> 00:51:54,839
and sometimes it's got the catalog in t

1157
00:51:50,780 --> 00:51:56,400
href so that's obviously why it only

1158
00:51:54,839 --> 00:51:58,680
scraped

1159
00:51:56,400 --> 00:52:01,200
four pages so the fifth page only has

1160
00:51:58,680 --> 00:52:05,339
page five so we're gonna have to just

1161
00:52:01,200 --> 00:52:06,480
modify our next page and if statement

1162
00:52:05,339 --> 00:52:10,260
here

1163
00:52:06,480 --> 00:52:13,740
just to check that we have catalog in

1164
00:52:10,260 --> 00:52:18,059
the href if we do then we do a slightly

1165
00:52:13,740 --> 00:52:21,059
different next page URL than if we don't

1166
00:52:18,059 --> 00:52:22,559
so let's just add that in now so if the

1167
00:52:21,059 --> 00:52:25,500
catalog

1168
00:52:22,559 --> 00:52:29,640
forward slash is in next page

1169
00:52:25,500 --> 00:52:32,760
then the next page URL is going to just

1170
00:52:29,640 --> 00:52:37,680
be what we currently have

1171
00:52:32,760 --> 00:52:40,200
and but if it's not we want an else

1172
00:52:37,680 --> 00:52:42,300
and we're going to say

1173
00:52:40,200 --> 00:52:45,960
add in

1174
00:52:42,300 --> 00:52:50,660
the next page URL

1175
00:52:45,960 --> 00:52:50,660
but we're going to add in catalog here

1176
00:52:51,660 --> 00:52:57,780
so this should ensure that the next page

1177
00:52:55,319 --> 00:53:00,480
URL is correct so if it contains catalog

1178
00:52:57,780 --> 00:53:02,819
we don't need to have catalog

1179
00:53:00,480 --> 00:53:04,980
in this part if it doesn't contain

1180
00:53:02,819 --> 00:53:07,380
catalog we do have to have it and then

1181
00:53:04,980 --> 00:53:10,079
that should make the correct URL

1182
00:53:07,380 --> 00:53:13,500
so hopefully that's fixed that bug so

1183
00:53:10,079 --> 00:53:15,680
let's try run the scripty crawl one more

1184
00:53:13,500 --> 00:53:15,680
time

1185
00:53:18,300 --> 00:53:21,900
okay so this seems to be going through a

1186
00:53:20,640 --> 00:53:24,420
lot more pages

1187
00:53:21,900 --> 00:53:27,059
which is a good sign

1188
00:53:24,420 --> 00:53:29,040
and let's give it another

1189
00:53:27,059 --> 00:53:33,480
minute or two to finish up

1190
00:53:29,040 --> 00:53:37,020
and we can see the total item counts at

1191
00:53:33,480 --> 00:53:40,079
the end and the total page of script so

1192
00:53:37,020 --> 00:53:41,760
this is kind of the process that you

1193
00:53:40,079 --> 00:53:44,160
have to do when you are creating a

1194
00:53:41,760 --> 00:53:46,380
spider to scrape data there'll be small

1195
00:53:44,160 --> 00:53:48,660
bugs like this that pop up and you need

1196
00:53:46,380 --> 00:53:52,020
to do a small bit of detective work to

1197
00:53:48,660 --> 00:53:54,720
find out why your spider is failing at

1198
00:53:52,020 --> 00:53:57,059
certain parts or not able to

1199
00:53:54,720 --> 00:53:58,020
extract certain pieces of data from the

1200
00:53:57,059 --> 00:54:02,160
page

1201
00:53:58,020 --> 00:54:05,400
so we can see here we have

1202
00:54:02,160 --> 00:54:08,460
response received count to 51

1203
00:54:05,400 --> 00:54:09,960
I sub scraped count of a thousand

1204
00:54:08,460 --> 00:54:11,819
so

1205
00:54:09,960 --> 00:54:13,619
if we go back

1206
00:54:11,819 --> 00:54:15,839
there is a thousand results so it

1207
00:54:13,619 --> 00:54:18,540
scraped all the books that we were

1208
00:54:15,839 --> 00:54:20,760
looking for that's pretty much

1209
00:54:18,540 --> 00:54:23,760
everything we wanted to cover in this

1210
00:54:20,760 --> 00:54:26,040
part four in part five we're going to go

1211
00:54:23,760 --> 00:54:28,319
through and each book we're going to

1212
00:54:26,040 --> 00:54:34,500
click into the book and then we're going

1213
00:54:28,319 --> 00:54:36,480
to extract more product data from the

1214
00:54:34,500 --> 00:54:38,880
product page itself

1215
00:54:36,480 --> 00:54:42,059
so right now we're just kind of doing

1216
00:54:38,880 --> 00:54:44,640
the easy thing of just going through one

1217
00:54:42,059 --> 00:54:47,520
page one page one page just the name and

1218
00:54:44,640 --> 00:54:52,559
extracting just the name the URL and the

1219
00:54:47,520 --> 00:54:55,020
price but in real life scenarios most of

1220
00:54:52,559 --> 00:54:57,359
the time we want to get a lot more data

1221
00:54:55,020 --> 00:54:59,160
and that involves doing things like

1222
00:54:57,359 --> 00:55:00,780
going in clicking into the actual

1223
00:54:59,160 --> 00:55:04,380
product we're looking at and actually

1224
00:55:00,780 --> 00:55:06,059
getting more in-depth data

1225
00:55:04,380 --> 00:55:06,900
so we'll be looking at how we can do

1226
00:55:06,059 --> 00:55:09,980
that

1227
00:55:06,900 --> 00:55:09,980
in part five

1228
00:55:12,480 --> 00:55:18,180
so in part five we're going to look at

1229
00:55:15,119 --> 00:55:19,140
how to crawl Pages using the scrapey

1230
00:55:18,180 --> 00:55:21,839
spider

1231
00:55:19,140 --> 00:55:25,020
using CSS selectors and expats to

1232
00:55:21,839 --> 00:55:27,839
extract more complicated pieces of data

1233
00:55:25,020 --> 00:55:30,660
from Pages such as from tables

1234
00:55:27,839 --> 00:55:32,880
from breadcrumbs things like that and

1235
00:55:30,660 --> 00:55:35,700
then we're going to move on to saving

1236
00:55:32,880 --> 00:55:39,119
the data into certain file formats such

1237
00:55:35,700 --> 00:55:40,680
as CSV or Json format so let's get

1238
00:55:39,119 --> 00:55:43,500
started

1239
00:55:40,680 --> 00:55:46,680
we're continuing on from part four so if

1240
00:55:43,500 --> 00:55:48,599
you need to get the code for that we'll

1241
00:55:46,680 --> 00:55:51,180
have that available for to for you to

1242
00:55:48,599 --> 00:55:52,500
download and follow on at this point or

1243
00:55:51,180 --> 00:55:54,780
if you've got your part 4 already

1244
00:55:52,500 --> 00:55:55,500
completed you can just continue on from

1245
00:55:54,780 --> 00:55:58,619
there

1246
00:55:55,500 --> 00:56:00,960
so in part four we just ran our spider

1247
00:55:58,619 --> 00:56:03,599
and it went through and got us the

1248
00:56:00,960 --> 00:56:07,500
details of the Thousand books that are

1249
00:56:03,599 --> 00:56:08,819
on the books to scrape.com site

1250
00:56:07,500 --> 00:56:10,740
so the next thing we're going to be

1251
00:56:08,819 --> 00:56:12,720
doing is we're going to be instead of

1252
00:56:10,740 --> 00:56:15,180
just scraping the

1253
00:56:12,720 --> 00:56:16,740
URL the price and the name of the book

1254
00:56:15,180 --> 00:56:19,500
we're actually going to be going into

1255
00:56:16,740 --> 00:56:22,079
the book page itself and we're going to

1256
00:56:19,500 --> 00:56:24,980
be taking things such as the rating

1257
00:56:22,079 --> 00:56:29,359
the product description the product type

1258
00:56:24,980 --> 00:56:32,760
the price excluding that including that

1259
00:56:29,359 --> 00:56:34,079
the category that it's in such as poetry

1260
00:56:32,760 --> 00:56:36,480
in this case

1261
00:56:34,079 --> 00:56:38,040
so we're going to be looping through all

1262
00:56:36,480 --> 00:56:41,339
the different books looping through

1263
00:56:38,040 --> 00:56:44,040
every single page and getting all the

1264
00:56:41,339 --> 00:56:45,780
specific data for each book that is on

1265
00:56:44,040 --> 00:56:47,220
this site

1266
00:56:45,780 --> 00:56:52,740
okay

1267
00:56:47,220 --> 00:56:54,839
so let's go back to our Spyder code

1268
00:56:52,740 --> 00:56:58,859
and the first thing we're going to look

1269
00:56:54,839 --> 00:57:02,819
at doing is we want to start going into

1270
00:56:58,859 --> 00:57:05,400
each page individually and so to do that

1271
00:57:02,819 --> 00:57:08,040
we're going to do something that's very

1272
00:57:05,400 --> 00:57:12,599
similar to going to each next page URL

1273
00:57:08,040 --> 00:57:16,020
so we'll copy this code from the bottom

1274
00:57:12,599 --> 00:57:19,020
and we're going to do this for every

1275
00:57:16,020 --> 00:57:22,140
book in the list of books so instead of

1276
00:57:19,020 --> 00:57:24,780
just yielding the data we're going to be

1277
00:57:22,140 --> 00:57:27,660
going into the URL so we're just going

1278
00:57:24,780 --> 00:57:29,099
to paste that in over our yield

1279
00:57:27,660 --> 00:57:31,380
right here

1280
00:57:29,099 --> 00:57:34,079
we're going to remove the

1281
00:57:31,380 --> 00:57:38,579
next page non-section

1282
00:57:34,079 --> 00:57:40,220
we're going to move our next page URL

1283
00:57:38,579 --> 00:57:43,440
Pier

1284
00:57:40,220 --> 00:57:48,420
and we're going to have to just modify

1285
00:57:43,440 --> 00:57:53,099
where we get the URL for the each

1286
00:57:48,420 --> 00:57:55,319
individual book so if we just go back

1287
00:57:53,099 --> 00:57:59,339
into inspect

1288
00:57:55,319 --> 00:58:03,000
the elements again and check out okay so

1289
00:57:59,339 --> 00:58:06,079
that was H3 and a and we want the href

1290
00:58:03,000 --> 00:58:12,300
for that H3 a tag

1291
00:58:06,079 --> 00:58:13,380
so we just want h3a attribute href that

1292
00:58:12,300 --> 00:58:16,380
should get us

1293
00:58:13,380 --> 00:58:19,500
the next page

1294
00:58:16,380 --> 00:58:22,460
and then it should create the correct

1295
00:58:19,500 --> 00:58:22,460
URL for us

1296
00:58:22,800 --> 00:58:28,079
instead let's call of next page because

1297
00:58:25,440 --> 00:58:30,000
it's not the next page it's the relative

1298
00:58:28,079 --> 00:58:32,540
URL of the book lets us call it relative

1299
00:58:30,000 --> 00:58:32,540
URL

1300
00:58:34,700 --> 00:58:40,200
and let's just

1301
00:58:38,160 --> 00:58:42,000
put this

1302
00:58:40,200 --> 00:58:44,579
here

1303
00:58:42,000 --> 00:58:48,420
and

1304
00:58:44,579 --> 00:58:52,319
the next thing we want to do is

1305
00:58:48,420 --> 00:58:56,400
we want the Callback function instead of

1306
00:58:52,319 --> 00:58:58,819
being parse we're going to do parse book

1307
00:58:56,400 --> 00:58:58,819
page

1308
00:58:59,880 --> 00:59:04,920
so we're going to make a new function to

1309
00:59:02,099 --> 00:59:06,240
parse the book page individually one by

1310
00:59:04,920 --> 00:59:08,940
one

1311
00:59:06,240 --> 00:59:11,099
so let's go down here

1312
00:59:08,940 --> 00:59:14,240
let's do Def

1313
00:59:11,099 --> 00:59:14,240
passbook page

1314
00:59:15,780 --> 00:59:20,520
self and response

1315
00:59:18,540 --> 00:59:22,079
and

1316
00:59:20,520 --> 00:59:27,059
at the moment let's just put pass in

1317
00:59:22,079 --> 00:59:30,020
there so fat is going to

1318
00:59:27,059 --> 00:59:30,020
Loop through

1319
00:59:30,540 --> 00:59:35,700
instead of next page URL I'm going to

1320
00:59:34,079 --> 00:59:38,780
call this one

1321
00:59:35,700 --> 00:59:38,780
book URL

1322
00:59:39,180 --> 00:59:45,299
and

1323
00:59:42,059 --> 00:59:47,220
Okay so

1324
00:59:45,299 --> 00:59:49,200
the only other thing to change that is

1325
00:59:47,220 --> 00:59:50,640
incorrect is that obviously

1326
00:59:49,200 --> 00:59:54,059
we need to Loop through the list of

1327
00:59:50,640 --> 00:59:56,880
books so this book needs to be used and

1328
00:59:54,059 --> 00:59:58,980
we're going to get the book.css and

1329
00:59:56,880 --> 01:00:02,160
that's where we're going to get our

1330
00:59:58,980 --> 01:00:04,920
link from with the

1331
01:00:02,160 --> 01:00:07,859
this link here

1332
01:00:04,920 --> 01:00:09,240
so that goes to relative URL and

1333
01:00:07,859 --> 01:00:12,420
variable

1334
01:00:09,240 --> 01:00:16,260
then we make the correct book URL and

1335
01:00:12,420 --> 01:00:17,460
then using this book URL we yield

1336
01:00:16,260 --> 01:00:20,579
um

1337
01:00:17,460 --> 01:00:24,540
so we basically go into this

1338
01:00:20,579 --> 01:00:26,940
um book URL and then the response HTML

1339
01:00:24,540 --> 01:00:29,280
that comes back from this URL will get

1340
01:00:26,940 --> 01:00:31,980
parsed by the parse underscore book

1341
01:00:29,280 --> 01:00:35,700
underscore page function which is the

1342
01:00:31,980 --> 01:00:39,020
one we made down here okay so I think we

1343
01:00:35,700 --> 01:00:39,020
can save that for now

1344
01:00:39,079 --> 01:00:45,599
and next thing we want to do is start

1345
01:00:42,119 --> 01:00:47,040
flushing out our parse book page so what

1346
01:00:45,599 --> 01:00:50,579
we're going to do first is we're going

1347
01:00:47,040 --> 01:00:53,040
to open up our scrapey shell again like

1348
01:00:50,579 --> 01:00:56,640
we did in part four and we're going to

1349
01:00:53,040 --> 01:00:59,339
look at the different CSS selectors and

1350
01:00:56,640 --> 01:01:02,160
expat selectors for the different items

1351
01:00:59,339 --> 01:01:03,839
that we want to scrape on the book page

1352
01:01:02,160 --> 01:01:07,140
itself so let's click into one of the

1353
01:01:03,839 --> 01:01:09,599
books and we're going to see what if we

1354
01:01:07,140 --> 01:01:11,940
want to extract from this page

1355
01:01:09,599 --> 01:01:16,140
let's go back open up

1356
01:01:11,940 --> 01:01:19,579
our scrapey shell in our terminal again

1357
01:01:16,140 --> 01:01:19,579
so just Scrapy shell

1358
01:01:19,640 --> 01:01:25,740
and when that opens we're just going to

1359
01:01:22,200 --> 01:01:29,099
use our fetch function again to fetch

1360
01:01:25,740 --> 01:01:31,200
the full URL from one of the book pages

1361
01:01:29,099 --> 01:01:32,880
which in this case is just the very

1362
01:01:31,200 --> 01:01:35,099
first book I've picked

1363
01:01:32,880 --> 01:01:36,839
in the list

1364
01:01:35,099 --> 01:01:40,799
so

1365
01:01:36,839 --> 01:01:43,500
I can just put in the URL in here hit

1366
01:01:40,799 --> 01:01:46,859
enter that's going to go off get the

1367
01:01:43,500 --> 01:01:48,660
HTML of that page and stick it in the

1368
01:01:46,859 --> 01:01:52,440
response variable

1369
01:01:48,660 --> 01:01:54,420
so just like we did in part 4 we can

1370
01:01:52,440 --> 01:01:56,760
see what works and what doesn't work

1371
01:01:54,420 --> 01:01:58,740
with our CSS selectors

1372
01:01:56,760 --> 01:02:01,740
so let's just do

1373
01:01:58,740 --> 01:02:03,900
response dot CSS

1374
01:02:01,740 --> 01:02:05,460
and then let's inspect the page again

1375
01:02:03,900 --> 01:02:08,880
and just

1376
01:02:05,460 --> 01:02:12,059
okay so we have

1377
01:02:08,880 --> 01:02:14,640
product description there is an ID

1378
01:02:12,059 --> 01:02:17,760
and

1379
01:02:14,640 --> 01:02:20,640
there is a P tag underneath that

1380
01:02:17,760 --> 01:02:22,859
so product pay underscore page there so

1381
01:02:20,640 --> 01:02:27,119
that gives us the whole page so let's

1382
01:02:22,859 --> 01:02:29,760
just try and see what happens if we do

1383
01:02:27,119 --> 01:02:32,760
product on the score page

1384
01:02:29,760 --> 01:02:34,020
that seems to give us back the whole

1385
01:02:32,760 --> 01:02:38,339
page

1386
01:02:34,020 --> 01:02:39,180
now let's look at getting the title of

1387
01:02:38,339 --> 01:02:41,640
the book

1388
01:02:39,180 --> 01:02:44,339
for example so in this case

1389
01:02:41,640 --> 01:02:48,119
on this page

1390
01:02:44,339 --> 01:02:49,799
it's in the product underscore Main and

1391
01:02:48,119 --> 01:02:55,260
it's H1

1392
01:02:49,799 --> 01:02:58,319
so let's go ahead and just do

1393
01:02:55,260 --> 01:03:00,599
so Dosh

1394
01:02:58,319 --> 01:03:02,940
product underscore Main

1395
01:03:00,599 --> 01:03:05,099
H1

1396
01:03:02,940 --> 01:03:07,319
text

1397
01:03:05,099 --> 01:03:10,380
and there we have a Light in the Attic

1398
01:03:07,319 --> 01:03:12,839
which matches to our Title Here

1399
01:03:10,380 --> 01:03:14,339
so that's very simple just as we've done

1400
01:03:12,839 --> 01:03:15,720
before

1401
01:03:14,339 --> 01:03:18,359
so now let's

1402
01:03:15,720 --> 01:03:21,059
do something a little more complex let's

1403
01:03:18,359 --> 01:03:23,040
get the category up here so we have

1404
01:03:21,059 --> 01:03:25,559
poetry in this case

1405
01:03:23,040 --> 01:03:27,540
so for things that are a little bit more

1406
01:03:25,559 --> 01:03:30,960
complicated like this sometimes it can

1407
01:03:27,540 --> 01:03:33,720
be easier just to use expats instead of

1408
01:03:30,960 --> 01:03:35,940
CSS selectors so expats are very similar

1409
01:03:33,720 --> 01:03:40,440
but instead of using class names

1410
01:03:35,940 --> 01:03:42,660
directly the format of how we write

1411
01:03:40,440 --> 01:03:44,220
the expats is just a little different to

1412
01:03:42,660 --> 01:03:47,480
how we would use

1413
01:03:44,220 --> 01:03:49,799
CSS selectors so I've got one

1414
01:03:47,480 --> 01:03:50,700
pre-written out which I'll just paste in

1415
01:03:49,799 --> 01:03:52,380
here

1416
01:03:50,700 --> 01:03:55,079
so

1417
01:03:52,380 --> 01:03:57,780
paste in my XPath and that gives me

1418
01:03:55,079 --> 01:04:00,059
poetry so I'll just explain to you how

1419
01:03:57,780 --> 01:04:04,559
this gosh poetry

1420
01:04:00,059 --> 01:04:05,940
so it went to the UL HTML tag

1421
01:04:04,559 --> 01:04:08,940
it's

1422
01:04:05,940 --> 01:04:12,119
put in the class breadcrumb

1423
01:04:08,940 --> 01:04:14,839
so if we go back to the top here

1424
01:04:12,119 --> 01:04:18,420
we should see it's in a UL

1425
01:04:14,839 --> 01:04:21,780
HTML tag and the class is breadcrumb and

1426
01:04:18,420 --> 01:04:25,799
then we have several Li tags and then we

1427
01:04:21,780 --> 01:04:27,420
have an a tag within the LI tag and we

1428
01:04:25,799 --> 01:04:31,020
have the href

1429
01:04:27,420 --> 01:04:32,700
so here we can see that's where tli tag

1430
01:04:31,020 --> 01:04:34,619
comes into it

1431
01:04:32,700 --> 01:04:36,780
and then

1432
01:04:34,619 --> 01:04:41,220
the active class

1433
01:04:36,780 --> 01:04:42,420
is on the grayed out section here

1434
01:04:41,220 --> 01:04:45,780
so

1435
01:04:42,420 --> 01:04:48,900
it's then going from it's going to here

1436
01:04:45,780 --> 01:04:52,859
and then it's saying preceding sibling

1437
01:04:48,900 --> 01:04:55,319
so get me the preceding Li tag before

1438
01:04:52,859 --> 01:04:57,780
the one that has the active equals class

1439
01:04:55,319 --> 01:04:59,819
in it so it's going to here and then

1440
01:04:57,780 --> 01:05:03,299
it's going back one to the preceding

1441
01:04:59,819 --> 01:05:04,260
sibling and it's getting the text within

1442
01:05:03,299 --> 01:05:07,200
here

1443
01:05:04,260 --> 01:05:10,680
and it's doing that with

1444
01:05:07,200 --> 01:05:13,020
the text at the end here so as you can

1445
01:05:10,680 --> 01:05:14,460
see proceeding sibling Li

1446
01:05:13,020 --> 01:05:18,720
one

1447
01:05:14,460 --> 01:05:21,180
and then a and text at the end

1448
01:05:18,720 --> 01:05:23,880
so expats are quite similar

1449
01:05:21,180 --> 01:05:24,960
to CSS selectors in not every case will

1450
01:05:23,880 --> 01:05:29,760
you have

1451
01:05:24,960 --> 01:05:31,740
a class name or an ID tag on a HTML tag

1452
01:05:29,760 --> 01:05:33,299
so in the case of the product

1453
01:05:31,740 --> 01:05:35,400
description which I showed you a second

1454
01:05:33,299 --> 01:05:40,859
ago we'll just look at it again there is

1455
01:05:35,400 --> 01:05:43,859
no class name or no CSS ID on on this P

1456
01:05:40,859 --> 01:05:46,859
for paragraph tag here so in that case

1457
01:05:43,859 --> 01:05:50,280
as well we can say right go to the

1458
01:05:46,859 --> 01:05:52,680
product description using the expats and

1459
01:05:50,280 --> 01:05:54,420
then get me the following sibling that's

1460
01:05:52,680 --> 01:05:58,859
a P tag

1461
01:05:54,420 --> 01:06:01,799
so I can just show you that one so it's

1462
01:05:58,859 --> 01:06:05,280
go to the product description ID

1463
01:06:01,799 --> 01:06:08,700
get me the following sibling with the P

1464
01:06:05,280 --> 01:06:12,839
tag and then within that get me the text

1465
01:06:08,700 --> 01:06:14,940
so that's how expats work for getting

1466
01:06:12,839 --> 01:06:18,540
some of these Corner cases where you

1467
01:06:14,940 --> 01:06:22,680
might not have a simple class name or a

1468
01:06:18,540 --> 01:06:25,619
simple ID on the HTML tag

1469
01:06:22,680 --> 01:06:29,400
okay so we know how we can get the

1470
01:06:25,619 --> 01:06:30,420
product description and the category tag

1471
01:06:29,400 --> 01:06:32,700
up here

1472
01:06:30,420 --> 01:06:33,839
we know how we can get the price and the

1473
01:06:32,700 --> 01:06:37,079
title

1474
01:06:33,839 --> 01:06:38,819
next let's look at extracting data from

1475
01:06:37,079 --> 01:06:40,020
tables

1476
01:06:38,819 --> 01:06:43,319
so

1477
01:06:40,020 --> 01:06:46,680
if we inspect element again we can see

1478
01:06:43,319 --> 01:06:49,440
that this is all contained within a

1479
01:06:46,680 --> 01:06:54,059
table and this table has several rows

1480
01:06:49,440 --> 01:06:59,579
which have TR as the HTML tag and then

1481
01:06:54,059 --> 01:07:02,640
within the table row we have th and TD

1482
01:06:59,579 --> 01:07:04,319
and that goes the whole way down so each

1483
01:07:02,640 --> 01:07:07,500
row has

1484
01:07:04,319 --> 01:07:12,539
one more th and TD so what we can do is

1485
01:07:07,500 --> 01:07:14,880
we can specify okay get me all the rows

1486
01:07:12,539 --> 01:07:17,400
in this table and then we can say okay

1487
01:07:14,880 --> 01:07:19,619
we know that the product type is always

1488
01:07:17,400 --> 01:07:24,900
going to be the second row

1489
01:07:19,619 --> 01:07:28,140
so let's always look for the text that

1490
01:07:24,900 --> 01:07:29,520
is within the TD of the second row if we

1491
01:07:28,140 --> 01:07:30,660
wanted for example to see the product

1492
01:07:29,520 --> 01:07:32,819
type

1493
01:07:30,660 --> 01:07:35,880
so first we want to get all the table

1494
01:07:32,819 --> 01:07:38,819
rows so we're going to look at table and

1495
01:07:35,880 --> 01:07:43,140
then all the TRS that are in that table

1496
01:07:38,819 --> 01:07:46,260
if so then let's assign that to

1497
01:07:43,140 --> 01:07:47,940
a table underscore rows in our Scrapy

1498
01:07:46,260 --> 01:07:51,180
shell so if you just use table

1499
01:07:47,940 --> 01:07:56,119
underscore rows equals to response dot

1500
01:07:51,180 --> 01:07:58,500
CSS and then have table space TR within

1501
01:07:56,119 --> 01:08:00,720
the brackets

1502
01:07:58,500 --> 01:08:03,240
that will make sure that all those table

1503
01:08:00,720 --> 01:08:06,480
rows are within table rows we can

1504
01:08:03,240 --> 01:08:09,480
quickly check the length of that

1505
01:08:06,480 --> 01:08:13,020
that gives us seven rows and there's one

1506
01:08:09,480 --> 01:08:15,599
two three four five six seven rows so

1507
01:08:13,020 --> 01:08:19,080
now we can do something as simple as

1508
01:08:15,599 --> 01:08:20,759
table underscore rows

1509
01:08:19,080 --> 01:08:24,660
let's look at what we said the second

1510
01:08:20,759 --> 01:08:27,179
ago the second one the dot CSS again and

1511
01:08:24,660 --> 01:08:30,420
we want the TD

1512
01:08:27,179 --> 01:08:33,359
and we wanted the text within that

1513
01:08:30,420 --> 01:08:34,560
and I guess again we use get and we get

1514
01:08:33,359 --> 01:08:35,839
books

1515
01:08:34,560 --> 01:08:40,500
so again

1516
01:08:35,839 --> 01:08:42,960
the numbering starts at zero and then

1517
01:08:40,500 --> 01:08:45,540
the second one is

1518
01:08:42,960 --> 01:08:47,940
second line is number one and then we

1519
01:08:45,540 --> 01:08:51,359
look at the T D here

1520
01:08:47,940 --> 01:08:55,859
the TD and that gets us books

1521
01:08:51,359 --> 01:08:58,380
so that's how this line corresponds to

1522
01:08:55,859 --> 01:09:01,259
here in this table

1523
01:08:58,380 --> 01:09:04,140
so knowing that we can then go ahead and

1524
01:09:01,259 --> 01:09:08,520
get things like

1525
01:09:04,140 --> 01:09:11,640
the price excluding tax we can just put

1526
01:09:08,520 --> 01:09:12,660
in something very similar the next row

1527
01:09:11,640 --> 01:09:14,460
down

1528
01:09:12,660 --> 01:09:17,100
and that should give us the price

1529
01:09:14,460 --> 01:09:20,400
excluding tax

1530
01:09:17,100 --> 01:09:22,380
so we now can get all the data we need

1531
01:09:20,400 --> 01:09:25,859
from this table

1532
01:09:22,380 --> 01:09:29,040
and the only last thing to look at is

1533
01:09:25,859 --> 01:09:32,040
looking at how we can guess

1534
01:09:29,040 --> 01:09:34,859
the Stars

1535
01:09:32,040 --> 01:09:38,219
so if we just look at

1536
01:09:34,859 --> 01:09:42,719
inspect element we can see

1537
01:09:38,219 --> 01:09:44,759
we have several Stars here it has icon

1538
01:09:42,719 --> 01:09:47,880
star icon star

1539
01:09:44,759 --> 01:09:51,120
and star rating of three

1540
01:09:47,880 --> 01:09:53,460
so that's where we can see the number of

1541
01:09:51,120 --> 01:09:56,520
stars it's within this class

1542
01:09:53,460 --> 01:09:59,280
that they've written three so we need to

1543
01:09:56,520 --> 01:10:01,620
do something slightly different for this

1544
01:09:59,280 --> 01:10:03,660
so first off we're going to get

1545
01:10:01,620 --> 01:10:05,900
the star region

1546
01:10:03,660 --> 01:10:09,000
so we're going to do

1547
01:10:05,900 --> 01:10:12,239
response.css then p

1548
01:10:09,000 --> 01:10:15,179
star rating for the class and then we're

1549
01:10:12,239 --> 01:10:17,160
going to ask for the attribute of class

1550
01:10:15,179 --> 01:10:19,920
so this is the attribute the attribute

1551
01:10:17,160 --> 01:10:23,699
name is class and then it should give us

1552
01:10:19,920 --> 01:10:27,300
our three that we're looking for

1553
01:10:23,699 --> 01:10:28,980
so let's just do that now

1554
01:10:27,300 --> 01:10:31,080
so

1555
01:10:28,980 --> 01:10:35,880
response

1556
01:10:31,080 --> 01:10:38,940
dot CSS P star rating attribute class

1557
01:10:35,880 --> 01:10:42,060
and that gives us star rating of three

1558
01:10:38,940 --> 01:10:44,340
so using our scrippy shell

1559
01:10:42,060 --> 01:10:47,100
we've looked at how we can get all the

1560
01:10:44,340 --> 01:10:50,300
different pieces of data from the page

1561
01:10:47,100 --> 01:10:55,500
so let's start filling that into our

1562
01:10:50,300 --> 01:10:58,500
parse book page function here

1563
01:10:55,500 --> 01:11:01,739
so we can actually get all the book data

1564
01:10:58,500 --> 01:11:03,900
to be scraped correctly okay so let's

1565
01:11:01,739 --> 01:11:06,739
just exit

1566
01:11:03,900 --> 01:11:14,760
out of our Scrappy show

1567
01:11:06,739 --> 01:11:17,600
and let's first just get the table rows

1568
01:11:14,760 --> 01:11:17,600
so

1569
01:11:17,820 --> 01:11:22,140
table rolls are going to be equals to

1570
01:11:19,800 --> 01:11:24,179
response

1571
01:11:22,140 --> 01:11:27,360
dot CSS

1572
01:11:24,179 --> 01:11:30,239
and then we're gonna have

1573
01:11:27,360 --> 01:11:34,140
what we had up here

1574
01:11:30,239 --> 01:11:38,340
where you can see table rows is table TR

1575
01:11:34,140 --> 01:11:41,219
so table TR is a table rows and we can

1576
01:11:38,340 --> 01:11:43,739
work with that now to fill in the rest

1577
01:11:41,219 --> 01:11:47,600
of the details so we want to remove pass

1578
01:11:43,739 --> 01:11:47,600
and then we're just going to do yield

1579
01:11:47,640 --> 01:11:55,500
and we're going to have our details

1580
01:11:50,880 --> 01:11:57,420
inside here so let's start off with the

1581
01:11:55,500 --> 01:12:01,020
URL

1582
01:11:57,420 --> 01:12:03,179
that's easy because we can just use

1583
01:12:01,020 --> 01:12:06,179
the

1584
01:12:03,179 --> 01:12:06,179
response

1585
01:12:06,540 --> 01:12:10,380
sponsor URL

1586
01:12:08,400 --> 01:12:12,600
so the URL of the page is contained

1587
01:12:10,380 --> 01:12:17,600
within this response

1588
01:12:12,600 --> 01:12:17,600
object then let's get the title

1589
01:12:17,880 --> 01:12:24,540
so we had Dash up here

1590
01:12:22,380 --> 01:12:25,860
here

1591
01:12:24,540 --> 01:12:28,380
so

1592
01:12:25,860 --> 01:12:31,080
can just copy this directly

1593
01:12:28,380 --> 01:12:33,540
don't forget to add

1594
01:12:31,080 --> 01:12:38,159
commas at the end

1595
01:12:33,540 --> 01:12:41,640
and let's get the product type the price

1596
01:12:38,159 --> 01:12:45,960
excluding tax including tax the tax

1597
01:12:41,640 --> 01:12:47,219
so these are all to do with the table

1598
01:12:45,960 --> 01:12:50,340
so

1599
01:12:47,219 --> 01:12:53,940
we're going to be doing table row

1600
01:12:50,340 --> 01:12:57,420
one for the second row and then TD text

1601
01:12:53,940 --> 01:12:59,520
get for the product type and so on for

1602
01:12:57,420 --> 01:13:02,159
the price excluding tax and including

1603
01:12:59,520 --> 01:13:04,739
tax as you can see

1604
01:13:02,159 --> 01:13:06,780
price excluding tax price including tax

1605
01:13:04,739 --> 01:13:08,699
they're all one after the other here so

1606
01:13:06,780 --> 01:13:10,739
all we're doing is incrementing the

1607
01:13:08,699 --> 01:13:13,860
number here

1608
01:13:10,739 --> 01:13:16,860
and the tax itself

1609
01:13:13,860 --> 01:13:19,860
and we might as well add in the

1610
01:13:16,860 --> 01:13:21,840
availability and the number of reviews

1611
01:13:19,860 --> 01:13:25,679
as well

1612
01:13:21,840 --> 01:13:28,260
since that's just a continuation of the

1613
01:13:25,679 --> 01:13:30,239
same thing

1614
01:13:28,260 --> 01:13:32,040
so

1615
01:13:30,239 --> 01:13:35,640
add those two in

1616
01:13:32,040 --> 01:13:39,300
we'll add in the Stars by

1617
01:13:35,640 --> 01:13:41,520
doing this so let's copy

1618
01:13:39,300 --> 01:13:43,560
we have them here

1619
01:13:41,520 --> 01:13:46,320
and we'll just call that

1620
01:13:43,560 --> 01:13:50,280
Stars

1621
01:13:46,320 --> 01:13:53,159
and we can just paste that directly

1622
01:13:50,280 --> 01:13:57,060
and let's also get the category and the

1623
01:13:53,159 --> 01:14:00,000
description like we had a second ago

1624
01:13:57,060 --> 01:14:03,360
so category

1625
01:14:00,000 --> 01:14:05,040
or with that using the expats so that

1626
01:14:03,360 --> 01:14:07,739
was this guy here

1627
01:14:05,040 --> 01:14:10,140
let's just

1628
01:14:07,739 --> 01:14:12,179
paste Dash let me have to just get it

1629
01:14:10,140 --> 01:14:14,900
all in the same line

1630
01:14:12,179 --> 01:14:19,219
perfect and

1631
01:14:14,900 --> 01:14:19,219
let's get the description

1632
01:14:19,500 --> 01:14:26,340
okay and we just want

1633
01:14:23,280 --> 01:14:28,560
this XPath that we were using earlier in

1634
01:14:26,340 --> 01:14:32,760
our Scrapy shell as well

1635
01:14:28,560 --> 01:14:35,820
again just making sure that we

1636
01:14:32,760 --> 01:14:40,199
have it all in the same line and

1637
01:14:35,820 --> 01:14:44,280
that we add in our commas finally the

1638
01:14:40,199 --> 01:14:48,060
only thing missing is the price

1639
01:14:44,280 --> 01:14:50,580
and we can do that by getting the

1640
01:14:48,060 --> 01:14:52,380
response

1641
01:14:50,580 --> 01:14:56,400
and then

1642
01:14:52,380 --> 01:14:58,020
dot CSS P dot price color and then the

1643
01:14:56,400 --> 01:15:02,300
text from that

1644
01:14:58,020 --> 01:15:02,300
so the price is up here

1645
01:15:02,820 --> 01:15:08,280
and that's the class of price color

1646
01:15:05,239 --> 01:15:11,219
within the P tag

1647
01:15:08,280 --> 01:15:14,040
so if we save all that that should be

1648
01:15:11,219 --> 01:15:18,000
everything we need to parse the

1649
01:15:14,040 --> 01:15:19,620
individual book pages and the thing

1650
01:15:18,000 --> 01:15:22,260
that's just missing here

1651
01:15:19,620 --> 01:15:24,300
is the next page which we just need to

1652
01:15:22,260 --> 01:15:25,739
add back in we deleted that earlier by

1653
01:15:24,300 --> 01:15:30,900
mistake

1654
01:15:25,739 --> 01:15:34,739
so that's how we get the next page URL

1655
01:15:30,900 --> 01:15:36,420
okay so everything else looks correct so

1656
01:15:34,739 --> 01:15:39,179
just a quick recap

1657
01:15:36,420 --> 01:15:41,219
the spider is going to kick off go to

1658
01:15:39,179 --> 01:15:43,800
this start URL

1659
01:15:41,219 --> 01:15:45,480
the response that comes back the first

1660
01:15:43,800 --> 01:15:48,360
time around

1661
01:15:45,480 --> 01:15:51,060
we'll go into this parse function

1662
01:15:48,360 --> 01:15:52,800
this parse function then we get all the

1663
01:15:51,060 --> 01:15:55,920
books on the main page

1664
01:15:52,800 --> 01:15:57,840
so that is starting with all these

1665
01:15:55,920 --> 01:16:00,659
different books here

1666
01:15:57,840 --> 01:16:04,020
then the next thing that happens is we

1667
01:16:00,659 --> 01:16:06,719
get the relative URL and we turn that

1668
01:16:04,020 --> 01:16:09,840
into the book URL we do that by getting

1669
01:16:06,719 --> 01:16:13,140
each of these URLs here

1670
01:16:09,840 --> 01:16:14,940
once we get the first books URL we then

1671
01:16:13,140 --> 01:16:16,920
go into that book page

1672
01:16:14,940 --> 01:16:20,159
so what happens is the code basically

1673
01:16:16,920 --> 01:16:22,679
goes in here it's then

1674
01:16:20,159 --> 01:16:27,120
goes to the Callback function which is

1675
01:16:22,679 --> 01:16:31,080
parse book page down here it goes gets

1676
01:16:27,120 --> 01:16:33,360
all these details here that we specified

1677
01:16:31,080 --> 01:16:35,880
and then is

1678
01:16:33,360 --> 01:16:39,480
Loops to the next book on the page

1679
01:16:35,880 --> 01:16:41,820
because this so it comes back out

1680
01:16:39,480 --> 01:16:45,360
here and then Loops back up to the start

1681
01:16:41,820 --> 01:16:48,780
and goes to the second book on the page

1682
01:16:45,360 --> 01:16:51,060
goes Clicks in gets the data

1683
01:16:48,780 --> 01:16:52,620
comes back does the third book in the

1684
01:16:51,060 --> 01:16:54,719
page so Loops through all the books on

1685
01:16:52,620 --> 01:16:57,179
the pages keeps getting all the data for

1686
01:16:54,719 --> 01:16:58,679
each book and then it goes to the next

1687
01:16:57,179 --> 01:17:01,500
page

1688
01:16:58,679 --> 01:17:04,020
and then once all the pages are done it

1689
01:17:01,500 --> 01:17:06,900
finishes so if we've done everything

1690
01:17:04,020 --> 01:17:09,600
correctly we should be able to

1691
01:17:06,900 --> 01:17:10,739
now run our spider and see does that

1692
01:17:09,600 --> 01:17:15,300
work

1693
01:17:10,739 --> 01:17:16,980
so let's try and do a scrapey crawl

1694
01:17:15,300 --> 01:17:19,440
the thing we're going to do that's

1695
01:17:16,980 --> 01:17:22,920
slightly different this time is we're

1696
01:17:19,440 --> 01:17:24,600
going to have the output go to a file so

1697
01:17:22,920 --> 01:17:27,060
instead of having the output come into

1698
01:17:24,600 --> 01:17:30,000
just our terminal we're going to also

1699
01:17:27,060 --> 01:17:34,020
get it to save to a file so we do this

1700
01:17:30,000 --> 01:17:35,940
by doing minus all uh our Dash o and

1701
01:17:34,020 --> 01:17:39,300
then we're just going to call this book

1702
01:17:35,940 --> 01:17:41,219
data and we'll do two

1703
01:17:39,300 --> 01:17:44,940
um CSV

1704
01:17:41,219 --> 01:17:48,000
CSV so CSV file formats can be opened in

1705
01:17:44,940 --> 01:17:50,940
Excel or can be put into you know Google

1706
01:17:48,000 --> 01:17:52,860
Sheets and different applications like

1707
01:17:50,940 --> 01:17:54,659
that so it's CS face just stands for

1708
01:17:52,860 --> 01:17:57,600
comma separated values

1709
01:17:54,659 --> 01:17:58,940
so if we run that hopefully there's no

1710
01:17:57,600 --> 01:18:03,060
issues

1711
01:17:58,940 --> 01:18:04,199
and as you can see there's book data.csv

1712
01:18:03,060 --> 01:18:08,219
here

1713
01:18:04,199 --> 01:18:09,900
you can open that and we can see we have

1714
01:18:08,219 --> 01:18:11,699
nodes of data

1715
01:18:09,900 --> 01:18:12,960
so all the stuff that we were looking

1716
01:18:11,699 --> 01:18:15,600
for

1717
01:18:12,960 --> 01:18:18,719
price description

1718
01:18:15,600 --> 01:18:20,520
everything else seems to be there

1719
01:18:18,719 --> 01:18:22,560
so I'm just going to stop that now

1720
01:18:20,520 --> 01:18:24,239
before it gets the end because that

1721
01:18:22,560 --> 01:18:27,060
seemed like it's working correctly it

1722
01:18:24,239 --> 01:18:29,880
was already on page 15 here let's see

1723
01:18:27,060 --> 01:18:31,860
okay and I'll run that one more time

1724
01:18:29,880 --> 01:18:36,840
except this time

1725
01:18:31,860 --> 01:18:40,560
and do it in instead of book data.csv

1726
01:18:36,840 --> 01:18:42,360
we're going to get her to Output to Json

1727
01:18:40,560 --> 01:18:46,620
so I'm just going to

1728
01:18:42,360 --> 01:18:49,140
delete that one and get it to a push to

1729
01:18:46,620 --> 01:18:51,179
Json format instead

1730
01:18:49,140 --> 01:18:54,179
Json format can just be a bit easier to

1731
01:18:51,179 --> 01:18:58,020
read and if you're doing further coding

1732
01:18:54,179 --> 01:19:00,659
it can be easier to parse as well

1733
01:18:58,020 --> 01:19:05,159
so if we opening up as you can see it

1734
01:19:00,659 --> 01:19:09,960
has all the data nicely formatted the

1735
01:19:05,159 --> 01:19:09,960
title price including tax availability

1736
01:19:10,219 --> 01:19:14,400
the number of reviews all the data is

1737
01:19:13,320 --> 01:19:17,760
all there

1738
01:19:14,400 --> 01:19:19,860
so that's working nicely obviously it's

1739
01:19:17,760 --> 01:19:21,780
going to take a minute or two to scrape

1740
01:19:19,860 --> 01:19:23,280
all a thousand books

1741
01:19:21,780 --> 01:19:25,920
but I think that's everything we wanted

1742
01:19:23,280 --> 01:19:27,840
to go through in part five

1743
01:19:25,920 --> 01:19:32,100
in part six we're just going to be

1744
01:19:27,840 --> 01:19:33,420
looking at how we can use items and item

1745
01:19:32,100 --> 01:19:37,080
Pipelines

1746
01:19:33,420 --> 01:19:39,300
to better structure and clean our data

1747
01:19:37,080 --> 01:19:41,100
before we start saving it into things

1748
01:19:39,300 --> 01:19:42,840
like a database

1749
01:19:41,100 --> 01:19:45,239
so it'll just put a bit more structure

1750
01:19:42,840 --> 01:19:47,640
on our code and it'll enable us through

1751
01:19:45,239 --> 01:19:51,900
things such as for example we could

1752
01:19:47,640 --> 01:19:56,699
change the prices from pounds to dollars

1753
01:19:51,900 --> 01:20:01,739
four gets saved we could you know remove

1754
01:19:56,699 --> 01:20:03,659
any trailing white space lots of

1755
01:20:01,739 --> 01:20:07,260
different examples we'll go through in

1756
01:20:03,659 --> 01:20:11,239
part six of how to clean up the data

1757
01:20:07,260 --> 01:20:11,239
okay see in part six guys

1758
01:20:14,340 --> 01:20:18,260
so in part six of the scrappy beginners

1759
01:20:16,739 --> 01:20:21,420
course we're going to be looking at

1760
01:20:18,260 --> 01:20:24,060
scrapey items and Scrappy Pipelines

1761
01:20:21,420 --> 01:20:26,460
so first off we're going to go through

1762
01:20:24,060 --> 01:20:29,760
what scrapey items are then we're going

1763
01:20:26,460 --> 01:20:33,000
to use scrapey items to structure our

1764
01:20:29,760 --> 01:20:36,060
existing spider a bit better then we're

1765
01:20:33,000 --> 01:20:38,520
going to go into what scrapey pipelines

1766
01:20:36,060 --> 01:20:40,739
are and what they do and then we're

1767
01:20:38,520 --> 01:20:42,360
going to use the scrapey pipelines to

1768
01:20:40,739 --> 01:20:45,540
clean our data

1769
01:20:42,360 --> 01:20:47,940
so let's get started

1770
01:20:45,540 --> 01:20:49,440
if you're continuing on from part five

1771
01:20:47,940 --> 01:20:53,280
you should have everything already set

1772
01:20:49,440 --> 01:20:56,280
up if not you can download the code from

1773
01:20:53,280 --> 01:20:57,420
our repo and continue on from where we

1774
01:20:56,280 --> 01:21:01,440
are now

1775
01:20:57,420 --> 01:21:04,739
so I'm presuming you already have your

1776
01:21:01,440 --> 01:21:07,560
book scraper project set up with your

1777
01:21:04,739 --> 01:21:09,800
spider set up and you've got your

1778
01:21:07,560 --> 01:21:13,380
environment activated

1779
01:21:09,800 --> 01:21:15,480
and you have screen be installed and

1780
01:21:13,380 --> 01:21:16,739
python installed and everything else is

1781
01:21:15,480 --> 01:21:20,880
running

1782
01:21:16,739 --> 01:21:24,540
okay so items so when you generate a

1783
01:21:20,880 --> 01:21:26,760
Scrapy project it generates this items

1784
01:21:24,540 --> 01:21:29,820
dot py file

1785
01:21:26,760 --> 01:21:33,000
and this is where you put your items so

1786
01:21:29,820 --> 01:21:36,780
items just help us Define what we want

1787
01:21:33,000 --> 01:21:39,000
in a block of data that we're scraping

1788
01:21:36,780 --> 01:21:41,580
and that were returning

1789
01:21:39,000 --> 01:21:45,060
so for example here you can see in our

1790
01:21:41,580 --> 01:21:47,520
book Spider we have no specific item

1791
01:21:45,060 --> 01:21:50,640
declared we're not using an item that's

1792
01:21:47,520 --> 01:21:54,179
been created in relationship is that py

1793
01:21:50,640 --> 01:21:56,400
but instead we just have a yield with

1794
01:21:54,179 --> 01:21:58,199
all the different pieces of data we're

1795
01:21:56,400 --> 01:22:00,840
extracting from the page

1796
01:21:58,199 --> 01:22:03,420
so that works fine but just to clean

1797
01:22:00,840 --> 01:22:06,719
things up and to make things a bit less

1798
01:22:03,420 --> 01:22:11,400
ambiguous the best thing to do is to use

1799
01:22:06,719 --> 01:22:12,480
the items.py and declare a specific item

1800
01:22:11,400 --> 01:22:17,219
in there

1801
01:22:12,480 --> 01:22:19,260
so let's go ahead and do that now

1802
01:22:17,219 --> 01:22:22,860
so I'm just going to copy and paste the

1803
01:22:19,260 --> 01:22:25,860
one I've already got in called book item

1804
01:22:22,860 --> 01:22:27,420
the book scraper item is just the

1805
01:22:25,860 --> 01:22:28,320
default one you can leave that there for

1806
01:22:27,420 --> 01:22:31,679
the moment

1807
01:22:28,320 --> 01:22:33,960
so book item just has everything that we

1808
01:22:31,679 --> 01:22:36,960
already have used in

1809
01:22:33,960 --> 01:22:39,120
our book Spider so URL title product

1810
01:22:36,960 --> 01:22:41,280
type all these different things

1811
01:22:39,120 --> 01:22:44,760
but instead were

1812
01:22:41,280 --> 01:22:46,340
declaring them specifically here so you

1813
01:22:44,760 --> 01:22:50,699
might say well what's the point of that

1814
01:22:46,340 --> 01:22:54,000
well one example is that if I

1815
01:22:50,699 --> 01:22:55,020
do a Miss type and reviews goes in like

1816
01:22:54,000 --> 01:22:58,080
this

1817
01:22:55,020 --> 01:23:00,900
this might then not go into my database

1818
01:22:58,080 --> 01:23:02,400
or might not go and be processed further

1819
01:23:00,900 --> 01:23:05,580
down the line

1820
01:23:02,400 --> 01:23:08,159
and I might not even notice it but

1821
01:23:05,580 --> 01:23:10,679
if I'm using an item

1822
01:23:08,159 --> 01:23:13,679
scraper will throw an error and say

1823
01:23:10,679 --> 01:23:16,739
this Norm underscore reviews with two

1824
01:23:13,679 --> 01:23:19,380
r's does not exist and it alert you to

1825
01:23:16,739 --> 01:23:21,540
the fact that there is a typo here so

1826
01:23:19,380 --> 01:23:23,820
that's one very good reason as to why we

1827
01:23:21,540 --> 01:23:27,179
will use our items

1828
01:23:23,820 --> 01:23:29,040
and actually Define the item first so

1829
01:23:27,179 --> 01:23:31,620
now that we've got the item to find

1830
01:23:29,040 --> 01:23:34,800
we've got our item class created let's

1831
01:23:31,620 --> 01:23:37,860
actually start using that

1832
01:23:34,800 --> 01:23:39,000
so first off we want to import that into

1833
01:23:37,860 --> 01:23:40,620
our spider

1834
01:23:39,000 --> 01:23:43,140
so we go up to the top

1835
01:23:40,620 --> 01:23:45,060
and we're importing book item as you can

1836
01:23:43,140 --> 01:23:48,300
see it brings us directly to the book

1837
01:23:45,060 --> 01:23:51,080
item now the next thing we want to do is

1838
01:23:48,300 --> 01:23:52,860
we just want to

1839
01:23:51,080 --> 01:23:54,840
specify

1840
01:23:52,860 --> 01:23:59,040
a book underscore item is equal to book

1841
01:23:54,840 --> 01:24:02,100
item and then we're just going to yield

1842
01:23:59,040 --> 01:24:04,380
book item at the bottom so instead of

1843
01:24:02,100 --> 01:24:07,140
yielding just start texturing there

1844
01:24:04,380 --> 01:24:10,140
we're going to yield book item

1845
01:24:07,140 --> 01:24:11,699
and then we're going to

1846
01:24:10,140 --> 01:24:14,460
remove

1847
01:24:11,699 --> 01:24:19,080
those two brackets and

1848
01:24:14,460 --> 01:24:22,800
we're going to say book item URL

1849
01:24:19,080 --> 01:24:24,659
is equal to response to the URL and so

1850
01:24:22,800 --> 01:24:30,860
on all the way down

1851
01:24:24,659 --> 01:24:30,860
so change all these into using our item

1852
01:24:31,739 --> 01:24:38,179
and then once that's done

1853
01:24:34,860 --> 01:24:38,179
we'll start looking at

1854
01:24:38,580 --> 01:24:44,880
item Pipelines

1855
01:24:41,280 --> 01:24:48,360
so let's look at the data that have been

1856
01:24:44,880 --> 01:24:50,820
saved into our file so this book data

1857
01:24:48,360 --> 01:24:54,360
dot Json was what we did in part five

1858
01:24:50,820 --> 01:24:55,560
that was the output from our spider that

1859
01:24:54,360 --> 01:24:57,179
ran

1860
01:24:55,560 --> 01:25:00,239
so as you can see we had things like the

1861
01:24:57,179 --> 01:25:02,460
URL the title so on so forth

1862
01:25:00,239 --> 01:25:05,460
but if you noticed

1863
01:25:02,460 --> 01:25:09,120
we have the price excluding tax for

1864
01:25:05,460 --> 01:25:12,780
example has this encoded value here so

1865
01:25:09,120 --> 01:25:14,880
it looks like the pound sign did not go

1866
01:25:12,780 --> 01:25:17,940
in correctly

1867
01:25:14,880 --> 01:25:20,219
so you can specify

1868
01:25:17,940 --> 01:25:22,620
a specific

1869
01:25:20,219 --> 01:25:23,940
serializer that you want to use on a

1870
01:25:22,620 --> 01:25:26,000
specific field

1871
01:25:23,940 --> 01:25:29,760
so for example if it was like the price

1872
01:25:26,000 --> 01:25:30,679
I've a serialized price function I can

1873
01:25:29,760 --> 01:25:34,640
write

1874
01:25:30,679 --> 01:25:39,960
and I can then use that serialize price

1875
01:25:34,640 --> 01:25:41,940
to stick a dollar or a parent sign in

1876
01:25:39,960 --> 01:25:46,260
front of the value

1877
01:25:41,940 --> 01:25:50,219
so for example I can stick serialized

1878
01:25:46,260 --> 01:25:52,860
price and I'll put it in front of the

1879
01:25:50,219 --> 01:25:55,140
price excluding tax

1880
01:25:52,860 --> 01:25:58,100
so I could just do something like

1881
01:25:55,140 --> 01:25:58,100
serializer

1882
01:26:00,179 --> 01:26:05,580
serializer is equal to

1883
01:26:02,520 --> 01:26:07,139
and then serialized price so that would

1884
01:26:05,580 --> 01:26:11,580
make

1885
01:26:07,139 --> 01:26:15,659
the value go in here and then have the

1886
01:26:11,580 --> 01:26:19,500
pen size applied to it before it gets

1887
01:26:15,659 --> 01:26:22,320
put into price excluding tax so that's

1888
01:26:19,500 --> 01:26:24,659
also a cool way that you can use items

1889
01:26:22,320 --> 01:26:26,159
with serializers

1890
01:26:24,659 --> 01:26:27,719
so I'm just going to remove that one for

1891
01:26:26,159 --> 01:26:30,659
now because we actually end up

1892
01:26:27,719 --> 01:26:33,480
processing the data from this in item

1893
01:26:30,659 --> 01:26:35,400
pipelines in a second anyway I just

1894
01:26:33,480 --> 01:26:37,920
wanted to show you how you could use

1895
01:26:35,400 --> 01:26:40,440
this if you didn't want to do pipelines

1896
01:26:37,920 --> 01:26:41,940
and you're you're only going to scrape a

1897
01:26:40,440 --> 01:26:44,699
small bit of data and you didn't want to

1898
01:26:41,940 --> 01:26:46,320
do a lot of post-processing there's no

1899
01:26:44,699 --> 01:26:48,480
point using pipelines and you could just

1900
01:26:46,320 --> 01:26:51,120
stick to using just items and have a

1901
01:26:48,480 --> 01:26:52,980
serializer if you needed to but if

1902
01:26:51,120 --> 01:26:55,020
you're going to do anything more complex

1903
01:26:52,980 --> 01:26:56,460
and you want to do a lot more processing

1904
01:26:55,020 --> 01:27:00,000
of your data you're better off using

1905
01:26:56,460 --> 01:27:01,500
pipelines instead of just using serial

1906
01:27:00,000 --> 01:27:03,960
lasers

1907
01:27:01,500 --> 01:27:05,820
the next thing we want to do is look at

1908
01:27:03,960 --> 01:27:09,600
our Pipelines

1909
01:27:05,820 --> 01:27:12,960
so in our pipelines again scrapey

1910
01:27:09,600 --> 01:27:15,719
defines a book scraper pipeline when you

1911
01:27:12,960 --> 01:27:18,239
create the project this is just here to

1912
01:27:15,719 --> 01:27:21,239
give you an idea of what you can get

1913
01:27:18,239 --> 01:27:23,880
started with so using pipelines you can

1914
01:27:21,239 --> 01:27:25,980
clean your data for example you can

1915
01:27:23,880 --> 01:27:29,639
remove the currency signs if you want

1916
01:27:25,980 --> 01:27:33,300
you could convert the price from pounds

1917
01:27:29,639 --> 01:27:34,860
to dollars you can format strings to

1918
01:27:33,300 --> 01:27:36,540
integers if you're going to save it into

1919
01:27:34,860 --> 01:27:39,840
a database that becomes very important

1920
01:27:36,540 --> 01:27:43,080
and you can do things like converting

1921
01:27:39,840 --> 01:27:46,199
your relative URLs to full URLs you can

1922
01:27:43,080 --> 01:27:49,739
validate your data check if the price is

1923
01:27:46,199 --> 01:27:51,840
actually a price or is it sold out and

1924
01:27:49,739 --> 01:27:55,199
then in that case you can you know put

1925
01:27:51,840 --> 01:27:57,239
in a price of zero and you can also use

1926
01:27:55,199 --> 01:27:59,219
the pipelines to store the data so

1927
01:27:57,239 --> 01:28:01,380
instead of having all the data going

1928
01:27:59,219 --> 01:28:03,719
into a file like we've done in part five

1929
01:28:01,380 --> 01:28:06,480
we could have it we could use a pipeline

1930
01:28:03,719 --> 01:28:07,620
to get the data to go directly into a

1931
01:28:06,480 --> 01:28:11,219
database

1932
01:28:07,620 --> 01:28:13,260
which we will be doing in future parts

1933
01:28:11,219 --> 01:28:16,320
of this series

1934
01:28:13,260 --> 01:28:18,719
so let's clean up our data a bit now

1935
01:28:16,320 --> 01:28:21,800
what do we need to clean well straight

1936
01:28:18,719 --> 01:28:24,719
away this is not good for our data this

1937
01:28:21,800 --> 01:28:27,600
encoded value here so we need to sort

1938
01:28:24,719 --> 01:28:30,060
that is another thing we need to sort

1939
01:28:27,600 --> 01:28:33,179
out could be the availability of the

1940
01:28:30,060 --> 01:28:36,060
stock so you might say okay in stock 19

1941
01:28:33,179 --> 01:28:37,739
available is fine but if I needed to run

1942
01:28:36,060 --> 01:28:41,219
a piece of code later

1943
01:28:37,739 --> 01:28:43,620
on this data that's not very useful

1944
01:28:41,219 --> 01:28:46,500
because I just want to know that there's

1945
01:28:43,620 --> 01:28:50,340
19 books I don't want to have this extra

1946
01:28:46,500 --> 01:28:53,400
text here and here and brackets so if I

1947
01:28:50,340 --> 01:28:56,639
just wanted availability to be 19 I

1948
01:28:53,400 --> 01:29:00,420
could use the pipeline to remove the in

1949
01:28:56,639 --> 01:29:03,000
stock and the available parts of the

1950
01:29:00,420 --> 01:29:04,739
string and just convert that 19 into an

1951
01:29:03,000 --> 01:29:08,460
integer

1952
01:29:04,739 --> 01:29:11,460
okay so we'll do that also and I think I

1953
01:29:08,460 --> 01:29:15,500
saw in some places that things like the

1954
01:29:11,460 --> 01:29:19,380
title had a trailing white space

1955
01:29:15,500 --> 01:29:20,880
are the descriptions had trailing white

1956
01:29:19,380 --> 01:29:22,560
space

1957
01:29:20,880 --> 01:29:24,000
so that's also something that we could

1958
01:29:22,560 --> 01:29:27,239
remove

1959
01:29:24,000 --> 01:29:29,000
and another thing would be changing the

1960
01:29:27,239 --> 01:29:31,440
category we could change the category

1961
01:29:29,000 --> 01:29:33,960
instead of it being Thriller with a

1962
01:29:31,440 --> 01:29:35,699
capital we could change that to Thriller

1963
01:29:33,960 --> 01:29:38,280
with lowercase

1964
01:29:35,699 --> 01:29:40,320
so this kind of standardization of data

1965
01:29:38,280 --> 01:29:42,420
before it gets saved into a file or into

1966
01:29:40,320 --> 01:29:43,920
a database is important especially when

1967
01:29:42,420 --> 01:29:45,360
you start scripting at scale and doing

1968
01:29:43,920 --> 01:29:47,159
larger projects

1969
01:29:45,360 --> 01:29:49,560
so we're just going to go through a

1970
01:29:47,159 --> 01:29:52,739
bunch of different processing in our

1971
01:29:49,560 --> 01:29:56,820
process item in our pipeline

1972
01:29:52,739 --> 01:30:00,420
so we will just add everything in here

1973
01:29:56,820 --> 01:30:04,020
and then the item will be returned

1974
01:30:00,420 --> 01:30:05,159
so let's start with just removing the

1975
01:30:04,020 --> 01:30:08,520
white space

1976
01:30:05,159 --> 01:30:11,000
so I'll just paste in the code I've

1977
01:30:08,520 --> 01:30:11,000
already got

1978
01:30:11,280 --> 01:30:16,139
and talk you through it okay we straight

1979
01:30:14,219 --> 01:30:19,440
away get our

1980
01:30:16,139 --> 01:30:21,060
item which gets passed in to our process

1981
01:30:19,440 --> 01:30:23,460
underscore item so we've got the item

1982
01:30:21,060 --> 01:30:24,840
available we pass it into the item

1983
01:30:23,460 --> 01:30:27,480
adapter

1984
01:30:24,840 --> 01:30:29,280
so as you can see up here useful for

1985
01:30:27,480 --> 01:30:31,199
handling different item types with a

1986
01:30:29,280 --> 01:30:34,739
single interface

1987
01:30:31,199 --> 01:30:36,659
with this adapter we can get all the

1988
01:30:34,739 --> 01:30:38,760
field names and then we can Loop through

1989
01:30:36,659 --> 01:30:39,679
using our for loop loop through all the

1990
01:30:38,760 --> 01:30:43,020
field names

1991
01:30:39,679 --> 01:30:47,040
and if it's

1992
01:30:43,020 --> 01:30:50,520
not the description we want to use the

1993
01:30:47,040 --> 01:30:53,520
strip function to strip the white space

1994
01:30:50,520 --> 01:30:55,739
from the strings

1995
01:30:53,520 --> 01:30:57,659
so we're just getting the field name and

1996
01:30:55,739 --> 01:31:00,540
then stripping the value and putting

1997
01:30:57,659 --> 01:31:03,300
that back into what was initially there

1998
01:31:00,540 --> 01:31:06,920
for that value

1999
01:31:03,300 --> 01:31:11,280
okay now let's quickly look at

2000
01:31:06,920 --> 01:31:13,440
converting the product types uppercase

2001
01:31:11,280 --> 01:31:16,440
to lowercase if there is an uppercase

2002
01:31:13,440 --> 01:31:19,440
value for the for example thriller or

2003
01:31:16,440 --> 01:31:21,780
poetry values

2004
01:31:19,440 --> 01:31:24,300
we can specify specific keys that we're

2005
01:31:21,780 --> 01:31:26,040
looking for in this as I mentioned we'd

2006
01:31:24,300 --> 01:31:27,719
look at category you can also do things

2007
01:31:26,040 --> 01:31:29,420
like product type

2008
01:31:27,719 --> 01:31:31,920
and

2009
01:31:29,420 --> 01:31:33,360
we're going to

2010
01:31:31,920 --> 01:31:36,659
just

2011
01:31:33,360 --> 01:31:41,100
do the same thing except we're doing the

2012
01:31:36,659 --> 01:31:44,880
lower function on the value

2013
01:31:41,100 --> 01:31:48,600
now let's look at cleaning the price

2014
01:31:44,880 --> 01:31:51,659
data as I mentioned earlier and as part

2015
01:31:48,600 --> 01:31:53,580
of that make sure that the price data is

2016
01:31:51,659 --> 01:31:55,980
saved as a float

2017
01:31:53,580 --> 01:31:58,380
which can be important all the prices

2018
01:31:55,980 --> 01:31:59,400
aren't always going to be rounded up to

2019
01:31:58,380 --> 01:32:02,760
the nearest

2020
01:31:59,400 --> 01:32:04,320
dollar or pound or Euro for that kind of

2021
01:32:02,760 --> 01:32:05,639
data

2022
01:32:04,320 --> 01:32:08,100
so here

2023
01:32:05,639 --> 01:32:10,500
we Loop through the different price Keys

2024
01:32:08,100 --> 01:32:13,199
which because we're saving several

2025
01:32:10,500 --> 01:32:16,440
different pieces of data we've got price

2026
01:32:13,199 --> 01:32:18,600
price excluding tax price including tax

2027
01:32:16,440 --> 01:32:20,639
and the tax

2028
01:32:18,600 --> 01:32:23,340
and for each one of these we're

2029
01:32:20,639 --> 01:32:25,500
replacing the parent sign with nothing

2030
01:32:23,340 --> 01:32:28,679
and we can also

2031
01:32:25,500 --> 01:32:34,280
do something like replacing the for

2032
01:32:28,679 --> 01:32:34,280
example Unicode with a specific value

2033
01:32:34,500 --> 01:32:40,620
the other one I wanted to do was to

2034
01:32:36,780 --> 01:32:43,800
change the availability to remove the

2035
01:32:40,620 --> 01:32:46,380
extra text that was in there

2036
01:32:43,800 --> 01:32:49,860
so let me quickly add that in

2037
01:32:46,380 --> 01:32:53,520
to do that we're just doing the split

2038
01:32:49,860 --> 01:32:55,199
function on the bracket if it sees that

2039
01:32:53,520 --> 01:32:57,780
there's break there's no bracket there

2040
01:32:55,199 --> 01:33:00,719
then we'll just set the availability to

2041
01:32:57,780 --> 01:33:03,060
zero if there is a bracket there then we

2042
01:33:00,719 --> 01:33:06,420
will split the

2043
01:33:03,060 --> 01:33:08,460
second piece of the array that is

2044
01:33:06,420 --> 01:33:10,800
returned from this function

2045
01:33:08,460 --> 01:33:11,820
and we will say okay the second piece of

2046
01:33:10,800 --> 01:33:14,520
this

2047
01:33:11,820 --> 01:33:18,960
we'll split that again using the split

2048
01:33:14,520 --> 01:33:23,400
function and we then know that the

2049
01:33:18,960 --> 01:33:27,179
first item in this availability array is

2050
01:33:23,400 --> 01:33:29,880
going to be the availability number that

2051
01:33:27,179 --> 01:33:32,400
we had here

2052
01:33:29,880 --> 01:33:33,840
so this is going to be the first ISO in

2053
01:33:32,400 --> 01:33:36,380
that availability array and this is

2054
01:33:33,840 --> 01:33:36,380
going to be the second

2055
01:33:37,500 --> 01:33:43,020
so that should save just the number for

2056
01:33:40,500 --> 01:33:45,960
US of the availability and we'll save

2057
01:33:43,020 --> 01:33:47,580
that back into our item

2058
01:33:45,960 --> 01:33:49,440
let's just look at two other ones

2059
01:33:47,580 --> 01:33:53,760
quickly

2060
01:33:49,440 --> 01:33:56,100
so just converting the review to a

2061
01:33:53,760 --> 01:33:58,199
integer

2062
01:33:56,100 --> 01:34:00,600
so

2063
01:33:58,199 --> 01:34:03,239
we'll just convert Dash

2064
01:34:00,600 --> 01:34:05,520
so the number of reviews make sure that

2065
01:34:03,239 --> 01:34:07,860
it's an INT

2066
01:34:05,520 --> 01:34:09,540
so we're just going to adapter.get and

2067
01:34:07,860 --> 01:34:13,500
then we're using

2068
01:34:09,540 --> 01:34:16,380
our int and putting the string in inside

2069
01:34:13,500 --> 01:34:21,179
the brackets and saving that back into

2070
01:34:16,380 --> 01:34:25,500
the number of views variable and last of

2071
01:34:21,179 --> 01:34:29,159
all we mentioned the star rating and we

2072
01:34:25,500 --> 01:34:31,199
want to turn the star rating into an

2073
01:34:29,159 --> 01:34:32,940
integer also

2074
01:34:31,199 --> 01:34:37,080
so

2075
01:34:32,940 --> 01:34:39,300
to do that we can just get the Stars

2076
01:34:37,080 --> 01:34:42,179
split the string using the split

2077
01:34:39,300 --> 01:34:45,060
function again we've got the array we

2078
01:34:42,179 --> 01:34:47,699
take the second value in the array

2079
01:34:45,060 --> 01:34:51,780
converted to lowercase and then

2080
01:34:47,699 --> 01:34:54,719
depending on what the value is in that

2081
01:34:51,780 --> 01:34:57,480
variable is it zero one two three four

2082
01:34:54,719 --> 01:35:02,100
five then we save

2083
01:34:57,480 --> 01:35:03,600
the Stars value as 0 1 2 3 4 5. so

2084
01:35:02,100 --> 01:35:07,080
pretty easy

2085
01:35:03,600 --> 01:35:10,080
nothing too complicated there

2086
01:35:07,080 --> 01:35:13,020
so that's everything I wanted to cover

2087
01:35:10,080 --> 01:35:14,940
for for pipelines so as you can see

2088
01:35:13,020 --> 01:35:18,420
there's a huge amount of data processing

2089
01:35:14,940 --> 01:35:21,480
that you can do on pipelines and it's a

2090
01:35:18,420 --> 01:35:23,880
good idea to have a look at your data do

2091
01:35:21,480 --> 01:35:26,219
one run of it like we did in part five

2092
01:35:23,880 --> 01:35:29,760
and then have a look at your data and

2093
01:35:26,219 --> 01:35:31,679
actually see what you can fix what needs

2094
01:35:29,760 --> 01:35:33,239
to be fixed up what looks okay what

2095
01:35:31,679 --> 01:35:35,520
doesn't look okay

2096
01:35:33,239 --> 01:35:37,860
sometimes you'll get a missing piece of

2097
01:35:35,520 --> 01:35:40,139
data there'll be blanks but this is a

2098
01:35:37,860 --> 01:35:42,900
process of refinement so the first time

2099
01:35:40,139 --> 01:35:45,360
around you might only you know add in

2100
01:35:42,900 --> 01:35:46,920
two things to your item pipeline you run

2101
01:35:45,360 --> 01:35:49,679
it again and you notice something else

2102
01:35:46,920 --> 01:35:52,560
is wrong and you add in another piece

2103
01:35:49,679 --> 01:35:55,080
into this pipeline

2104
01:35:52,560 --> 01:35:57,540
so the next thing you want to do we talk

2105
01:35:55,080 --> 01:35:59,040
about this in part three is if you've

2106
01:35:57,540 --> 01:36:01,800
got a pipeline

2107
01:35:59,040 --> 01:36:03,300
you want to go into your settings and

2108
01:36:01,800 --> 01:36:04,860
you want to make sure that the pipeline

2109
01:36:03,300 --> 01:36:06,840
is enabled

2110
01:36:04,860 --> 01:36:10,139
so we've got our spider middlewares our

2111
01:36:06,840 --> 01:36:11,520
downloader middlewares extensions and as

2112
01:36:10,139 --> 01:36:14,880
you can see here

2113
01:36:11,520 --> 01:36:18,540
we've got our item Pipelines

2114
01:36:14,880 --> 01:36:22,679
so this book scraper pipeline

2115
01:36:18,540 --> 01:36:24,000
should correspond to the name of our

2116
01:36:22,679 --> 01:36:26,219
class here

2117
01:36:24,000 --> 01:36:29,340
and if I put that in you can see they're

2118
01:36:26,219 --> 01:36:32,100
the same so that should work because

2119
01:36:29,340 --> 01:36:34,260
this is also generated by scrape when

2120
01:36:32,100 --> 01:36:36,900
you generate the project

2121
01:36:34,260 --> 01:36:40,260
so it generally works as long as you

2122
01:36:36,900 --> 01:36:41,400
uncomment this section here

2123
01:36:40,260 --> 01:36:43,860
so

2124
01:36:41,400 --> 01:36:46,679
if everything was done correctly we

2125
01:36:43,860 --> 01:36:49,620
should be able to now run our spider and

2126
01:36:46,679 --> 01:36:53,460
see the results with all

2127
01:36:49,620 --> 01:36:55,260
the data processed just as we want it to

2128
01:36:53,460 --> 01:36:57,659
be processed here

2129
01:36:55,260 --> 01:37:01,560
if there's any errors they'll pop up and

2130
01:36:57,659 --> 01:37:04,199
we can fix them and run it again

2131
01:37:01,560 --> 01:37:06,719
so I'm just going to

2132
01:37:04,199 --> 01:37:08,460
make sure I'm in my project

2133
01:37:06,719 --> 01:37:11,219
and then

2134
01:37:08,460 --> 01:37:14,100
just you want scrape your list to make

2135
01:37:11,219 --> 01:37:17,560
sure everything's working and then

2136
01:37:14,100 --> 01:37:19,199
one Scrapy crawl book Spider

2137
01:37:17,560 --> 01:37:22,980
[Music]

2138
01:37:19,199 --> 01:37:25,980
hopefully there is no issues

2139
01:37:22,980 --> 01:37:28,440
okay straight away I can see there's an

2140
01:37:25,980 --> 01:37:29,639
error being returned so I'm just going

2141
01:37:28,440 --> 01:37:32,820
to

2142
01:37:29,639 --> 01:37:36,540
stop my spider

2143
01:37:32,820 --> 01:37:39,360
so none time none type object has no

2144
01:37:36,540 --> 01:37:42,360
attribute next call

2145
01:37:39,360 --> 01:37:46,620
and we can just

2146
01:37:42,360 --> 01:37:49,500
scroll up and double check this

2147
01:37:46,620 --> 01:37:54,000
so spider must return request item are

2148
01:37:49,500 --> 01:37:57,420
none because ice and meta in guess

2149
01:37:54,000 --> 01:37:59,840
okay so let's sort out this error

2150
01:37:57,420 --> 01:38:02,820
so if we just go back to our book

2151
01:37:59,840 --> 01:38:06,540
spider.py file

2152
01:38:02,820 --> 01:38:07,739
you can see the error is because I'm

2153
01:38:06,540 --> 01:38:11,400
returning

2154
01:38:07,739 --> 01:38:13,860
book item and yielding book item and

2155
01:38:11,400 --> 01:38:15,120
instead it should be book underscore

2156
01:38:13,860 --> 01:38:18,840
item

2157
01:38:15,120 --> 01:38:23,040
so that should fix the issue

2158
01:38:18,840 --> 01:38:25,020
and if I do a Scrapy crawl again

2159
01:38:23,040 --> 01:38:30,020
this time I'll actually get it to go

2160
01:38:25,020 --> 01:38:33,239
into another file we loot call clean

2161
01:38:30,020 --> 01:38:38,100
data dot Json

2162
01:38:33,239 --> 01:38:41,280
so it's the hyphen capital O clean

2163
01:38:38,100 --> 01:38:43,699
data.json and hopefully there's no other

2164
01:38:41,280 --> 01:38:43,699
errors

2165
01:38:43,860 --> 01:38:46,739
there it does look like there's another

2166
01:38:45,420 --> 01:38:49,920
error

2167
01:38:46,739 --> 01:38:52,620
because if I check clean data.json

2168
01:38:49,920 --> 01:38:57,360
there's nothing there okay so I'll just

2169
01:38:52,620 --> 01:38:59,520
close it again and you can see okay

2170
01:38:57,360 --> 01:39:01,860
error processing

2171
01:38:59,520 --> 01:39:04,139
availability

2172
01:39:01,860 --> 01:39:05,659
to give us anything else

2173
01:39:04,139 --> 01:39:09,840
so it says

2174
01:39:05,659 --> 01:39:12,000
pipelines.pyline 21. topple object has

2175
01:39:09,840 --> 01:39:16,400
no attribute strip

2176
01:39:12,000 --> 01:39:19,940
so we can go to our pipelines line 21

2177
01:39:16,400 --> 01:39:21,540
okay so we have our

2178
01:39:19,940 --> 01:39:23,219
value.strip

2179
01:39:21,540 --> 01:39:24,840
and it's saying Tuple object has no

2180
01:39:23,219 --> 01:39:27,600
attribute strip

2181
01:39:24,840 --> 01:39:29,520
so let's just

2182
01:39:27,600 --> 01:39:31,560
print out

2183
01:39:29,520 --> 01:39:34,380
the value

2184
01:39:31,560 --> 01:39:37,860
of value

2185
01:39:34,380 --> 01:39:40,260
let's just add in something above it so

2186
01:39:37,860 --> 01:39:43,139
we can just

2187
01:39:40,260 --> 01:39:45,960
see where it is in the output and try

2188
01:39:43,139 --> 01:39:49,080
runish one more time

2189
01:39:45,960 --> 01:39:51,420
and if we stop it again and scroll up we

2190
01:39:49,080 --> 01:39:54,420
should be able to see

2191
01:39:51,420 --> 01:39:57,480
that we've got our stars and we've got

2192
01:39:54,420 --> 01:40:00,300
in stock available at 19.

2193
01:39:57,480 --> 01:40:03,780
and it is indeed being returned in a

2194
01:40:00,300 --> 01:40:05,040
topple uh with the second

2195
01:40:03,780 --> 01:40:08,280
value

2196
01:40:05,040 --> 01:40:11,219
there's nothing there so obviously we

2197
01:40:08,280 --> 01:40:15,060
need to reference the first value in the

2198
01:40:11,219 --> 01:40:17,100
toggle so we need to just

2199
01:40:15,060 --> 01:40:18,540
do that

2200
01:40:17,100 --> 01:40:21,179
so

2201
01:40:18,540 --> 01:40:22,800
adding the square brackets since zero

2202
01:40:21,179 --> 01:40:25,139
should return

2203
01:40:22,800 --> 01:40:28,739
just the string that we're looking for

2204
01:40:25,139 --> 01:40:30,480
and then dot strip can act on this

2205
01:40:28,739 --> 01:40:33,420
string

2206
01:40:30,480 --> 01:40:38,060
so if we remove our print statements

2207
01:40:33,420 --> 01:40:38,060
save that and build Traverse again

2208
01:40:43,739 --> 01:40:47,040
and

2209
01:40:45,659 --> 01:40:50,280
it looks like there's some errors coming

2210
01:40:47,040 --> 01:40:52,860
in there we can just check our file

2211
01:40:50,280 --> 01:40:56,159
there's nothing in here yet

2212
01:40:52,860 --> 01:40:57,719
so I'll go ahead and stop the spider

2213
01:40:56,159 --> 01:41:00,719
from running

2214
01:40:57,719 --> 01:41:00,719
and

2215
01:41:01,380 --> 01:41:09,060
we can see an error here

2216
01:41:04,100 --> 01:41:11,760
pipelines.py line 21 in process item

2217
01:41:09,060 --> 01:41:13,139
so that's still giving it a bit this

2218
01:41:11,760 --> 01:41:15,600
line

2219
01:41:13,139 --> 01:41:19,460
but this time it's saying

2220
01:41:15,600 --> 01:41:22,139
type error none type object is not

2221
01:41:19,460 --> 01:41:24,179
subscriptable so

2222
01:41:22,139 --> 01:41:26,900
I know what this error is I've had it

2223
01:41:24,179 --> 01:41:29,760
before so this is coming up because

2224
01:41:26,900 --> 01:41:34,080
we're getting all the field names which

2225
01:41:29,760 --> 01:41:35,460
are a from our items.py so it's getting

2226
01:41:34,080 --> 01:41:38,940
all these different field names here

2227
01:41:35,460 --> 01:41:41,400
it's looping through them and one of

2228
01:41:38,940 --> 01:41:46,440
these field names is not

2229
01:41:41,400 --> 01:41:48,659
being found so if we look at our spider

2230
01:41:46,440 --> 01:41:51,119
and compare

2231
01:41:48,659 --> 01:41:52,739
all these guys here

2232
01:41:51,119 --> 01:41:55,619
versus

2233
01:41:52,739 --> 01:41:58,679
what we have here I think I've spotted

2234
01:41:55,619 --> 01:42:02,760
the one already so I think it's this

2235
01:41:58,679 --> 01:42:03,960
UPC field unique product code I think it

2236
01:42:02,760 --> 01:42:07,560
stands for

2237
01:42:03,960 --> 01:42:10,739
and if you look here we don't have book

2238
01:42:07,560 --> 01:42:14,159
item UPC

2239
01:42:10,739 --> 01:42:16,679
so I can just add that in now so I'll

2240
01:42:14,159 --> 01:42:18,840
add that in

2241
01:42:16,679 --> 01:42:21,480
here

2242
01:42:18,840 --> 01:42:24,000
and save that

2243
01:42:21,480 --> 01:42:25,980
so now we should have this

2244
01:42:24,000 --> 01:42:28,199
which should correspond to this and we

2245
01:42:25,980 --> 01:42:29,880
should have no more errors

2246
01:42:28,199 --> 01:42:33,260
so

2247
01:42:29,880 --> 01:42:33,260
let's run that again

2248
01:42:34,619 --> 01:42:42,000
and this time we should see our

2249
01:42:37,560 --> 01:42:45,719
clean data.json file filling up

2250
01:42:42,000 --> 01:42:48,659
so everything looks good there

2251
01:42:45,719 --> 01:42:49,560
open up the clean data chart Json we've

2252
01:42:48,659 --> 01:42:51,540
got

2253
01:42:49,560 --> 01:42:55,320
what looks like

2254
01:42:51,540 --> 01:42:56,520
all the data we wanted

2255
01:42:55,320 --> 01:42:58,619
so

2256
01:42:56,520 --> 01:43:01,860
we can go ahead and just

2257
01:42:58,619 --> 01:43:05,340
stop the spider don't need it to collect

2258
01:43:01,860 --> 01:43:07,139
all 1000 records you can just double

2259
01:43:05,340 --> 01:43:10,860
check that everything did go in

2260
01:43:07,139 --> 01:43:13,860
correctly so you can see just by either

2261
01:43:10,860 --> 01:43:16,619
checking the file or scrolling up

2262
01:43:13,860 --> 01:43:18,659
did everything get processed the way you

2263
01:43:16,619 --> 01:43:20,880
wanted to get processed so

2264
01:43:18,659 --> 01:43:23,159
did the price get get processed

2265
01:43:20,880 --> 01:43:25,380
correctly is it now double the product

2266
01:43:23,159 --> 01:43:27,900
type is the

2267
01:43:25,380 --> 01:43:30,540
first part lower case yes it is the

2268
01:43:27,900 --> 01:43:33,360
number of stars is now an integer

2269
01:43:30,540 --> 01:43:34,520
so it looks like everything that went

2270
01:43:33,360 --> 01:43:38,219
through our

2271
01:43:34,520 --> 01:43:41,040
pipelines.py got processed correctly

2272
01:43:38,219 --> 01:43:43,080
we can scroll up and check the category

2273
01:43:41,040 --> 01:43:45,300
as well and the availability so

2274
01:43:43,080 --> 01:43:48,540
everything worked out

2275
01:43:45,300 --> 01:43:50,400
so that's just how we go through using

2276
01:43:48,540 --> 01:43:52,440
pipelines and items

2277
01:43:50,400 --> 01:43:54,540
I hope that's given you a good idea of

2278
01:43:52,440 --> 01:43:57,900
how you can use items and pipelines

2279
01:43:54,540 --> 01:43:59,159
yourselves to clean the data that you're

2280
01:43:57,900 --> 01:44:02,400
scraping

2281
01:43:59,159 --> 01:44:03,960
and in part seven

2282
01:44:02,400 --> 01:44:07,739
which we'll be looking at next we'll be

2283
01:44:03,960 --> 01:44:11,100
looking at how we can use pipelines to

2284
01:44:07,739 --> 01:44:12,420
save our data into databases and also

2285
01:44:11,100 --> 01:44:15,659
how to use

2286
01:44:12,420 --> 01:44:19,100
feed exporters in a bit more detail so

2287
01:44:15,659 --> 01:44:19,100
see you in part seven guys

2288
01:44:22,260 --> 01:44:26,159
so for part 7 of our scrapey beginners

2289
01:44:24,659 --> 01:44:29,520
course we're going to look at all the

2290
01:44:26,159 --> 01:44:31,199
different ways we can save data so all

2291
01:44:29,520 --> 01:44:34,080
the data we've scraped in the last few

2292
01:44:31,199 --> 01:44:37,679
parts we're just going to see how can we

2293
01:44:34,080 --> 01:44:40,380
save it to different file formats and

2294
01:44:37,679 --> 01:44:43,679
then eventually look at databases

2295
01:44:40,380 --> 01:44:46,199
so first off we're going to look at Via

2296
01:44:43,679 --> 01:44:48,179
the command line how what commands we

2297
01:44:46,199 --> 01:44:50,040
need to run to save it to different file

2298
01:44:48,179 --> 01:44:52,139
formats then we're going to look at how

2299
01:44:50,040 --> 01:44:54,420
we can do that instead via the feed

2300
01:44:52,139 --> 01:44:58,440
settings which we can set in our

2301
01:44:54,420 --> 01:45:00,239
settings are in our main spider file and

2302
01:44:58,440 --> 01:45:03,300
then once we've done that we're going to

2303
01:45:00,239 --> 01:45:07,199
go on and look at saving data directly

2304
01:45:03,300 --> 01:45:09,480
into a database using the Pipelines

2305
01:45:07,199 --> 01:45:11,940
so if you've done part six with us you

2306
01:45:09,480 --> 01:45:14,820
know all about pipelines by now and

2307
01:45:11,940 --> 01:45:19,500
we'll be using those pipelines in part 7

2308
01:45:14,820 --> 01:45:21,659
to save the item and data into the

2309
01:45:19,500 --> 01:45:23,460
database directly

2310
01:45:21,659 --> 01:45:27,239
if you're just joining us now you can

2311
01:45:23,460 --> 01:45:31,020
download the code from our GitHub repo

2312
01:45:27,239 --> 01:45:35,159
we'll have links for that and you can

2313
01:45:31,020 --> 01:45:38,699
follow on from just this part 7. to get

2314
01:45:35,159 --> 01:45:41,580
going I'm just going to go into my

2315
01:45:38,699 --> 01:45:42,840
book scraper folder make sure I'm in the

2316
01:45:41,580 --> 01:45:45,360
right place

2317
01:45:42,840 --> 01:45:46,820
and then run Scrappy crawl and the name

2318
01:45:45,360 --> 01:45:52,260
of our spider

2319
01:45:46,820 --> 01:45:54,600
crawl book Spider Dash capital O

2320
01:45:52,260 --> 01:45:56,900
and then

2321
01:45:54,600 --> 01:46:00,119
book

2322
01:45:56,900 --> 01:46:04,139
data.cs V

2323
01:46:00,119 --> 01:46:08,159
so this is going to Output the data into

2324
01:46:04,139 --> 01:46:10,380
a CSV format which is comma separated

2325
01:46:08,159 --> 01:46:13,679
values so that can be opened in Excel

2326
01:46:10,380 --> 01:46:18,420
and as you can see the data is all there

2327
01:46:13,679 --> 01:46:21,420
correctly okay so we can stop that now

2328
01:46:18,420 --> 01:46:23,540
and if we scroll to the bottom we can

2329
01:46:21,420 --> 01:46:27,060
see we have

2330
01:46:23,540 --> 01:46:28,380
321 rows

2331
01:46:27,060 --> 01:46:31,980
so

2332
01:46:28,380 --> 01:46:35,520
if you want to append data onto a file

2333
01:46:31,980 --> 01:46:37,860
instead of The Hyphen or Dash capital O

2334
01:46:35,520 --> 01:46:39,600
you can do a lowercase o

2335
01:46:37,860 --> 01:46:42,060
and then if we do the same name again

2336
01:46:39,600 --> 01:46:44,340
book data.csv

2337
01:46:42,060 --> 01:46:47,280
and enter

2338
01:46:44,340 --> 01:46:49,619
it should start pending on the data here

2339
01:46:47,280 --> 01:46:50,699
so instead of overwriting the file every

2340
01:46:49,619 --> 01:46:53,340
time

2341
01:46:50,699 --> 01:46:55,260
if we close the file

2342
01:46:53,340 --> 01:46:58,380
open it back up you can see we're

2343
01:46:55,260 --> 01:47:00,659
already up to over 500 records so the

2344
01:46:58,380 --> 01:47:02,580
file doesn't update automatically

2345
01:47:00,659 --> 01:47:05,880
sometimes it can take a couple of

2346
01:47:02,580 --> 01:47:07,020
seconds or you have to close it and

2347
01:47:05,880 --> 01:47:10,500
reopen it

2348
01:47:07,020 --> 01:47:14,460
so as you can see we're up to 700

2349
01:47:10,500 --> 01:47:17,880
records there and if I relish once more

2350
01:47:14,460 --> 01:47:20,219
and do a capital O

2351
01:47:17,880 --> 01:47:22,020
it will overwrite that

2352
01:47:20,219 --> 01:47:24,960
there you go so it's after wiping the

2353
01:47:22,020 --> 01:47:25,920
file and now filling it again

2354
01:47:24,960 --> 01:47:30,300
okay

2355
01:47:25,920 --> 01:47:33,719
so that's the difference with the

2356
01:47:30,300 --> 01:47:37,679
overwriting are appending and you've

2357
01:47:33,719 --> 01:47:39,600
seen just by changing the file format at

2358
01:47:37,679 --> 01:47:43,400
the end of your file name

2359
01:47:39,600 --> 01:47:45,840
is how you can specify types of

2360
01:47:43,400 --> 01:47:47,340
files that you want to write into so

2361
01:47:45,840 --> 01:47:50,900
here we're going to do it again but

2362
01:47:47,340 --> 01:47:54,179
we're going to do it in Json file format

2363
01:47:50,900 --> 01:47:55,739
so we have a new file is created book

2364
01:47:54,179 --> 01:47:58,199
data.json

2365
01:47:55,739 --> 01:48:00,480
as you can see it's in Json format

2366
01:47:58,199 --> 01:48:05,460
and the other one

2367
01:48:00,480 --> 01:48:07,980
is comma separated values format

2368
01:48:05,460 --> 01:48:10,560
now let's move on and look at how we can

2369
01:48:07,980 --> 01:48:12,360
specify where the data will be saved in

2370
01:48:10,560 --> 01:48:16,739
our settings file

2371
01:48:12,360 --> 01:48:21,179
so if we open up our settings file

2372
01:48:16,739 --> 01:48:22,679
what we can do is use the feeds so what

2373
01:48:21,179 --> 01:48:25,679
we do is

2374
01:48:22,679 --> 01:48:29,159
add in a feed section

2375
01:48:25,679 --> 01:48:32,639
and here we're saying save the data into

2376
01:48:29,159 --> 01:48:35,280
a file called data.json

2377
01:48:32,639 --> 01:48:37,560
so that's the scholars

2378
01:48:35,280 --> 01:48:40,020
books data

2379
01:48:37,560 --> 01:48:45,540
dot Json and the format is going to be

2380
01:48:40,020 --> 01:48:47,880
Json so if I delete the two files we've

2381
01:48:45,540 --> 01:48:51,659
just been using there

2382
01:48:47,880 --> 01:48:54,960
and save what I have in my settings

2383
01:48:51,659 --> 01:48:56,820
and rerun it except this time

2384
01:48:54,960 --> 01:49:01,320
remove the

2385
01:48:56,820 --> 01:49:05,100
Dash o and the name of the file and if I

2386
01:49:01,320 --> 01:49:08,040
just go ahead and run my spider

2387
01:49:05,100 --> 01:49:12,360
we've seen it's just created at books

2388
01:49:08,040 --> 01:49:15,960
data.json because I've specified it here

2389
01:49:12,360 --> 01:49:17,760
and it's all in the correct format

2390
01:49:15,960 --> 01:49:21,540
so I'm just going to go ahead and stop

2391
01:49:17,760 --> 01:49:23,940
my spider and the next thing you want to

2392
01:49:21,540 --> 01:49:27,119
do is just show you guys how you can

2393
01:49:23,940 --> 01:49:31,320
specify the feed

2394
01:49:27,119 --> 01:49:35,480
data in your spider so to do that we can

2395
01:49:31,320 --> 01:49:35,480
use the custom settings

2396
01:49:36,840 --> 01:49:43,139
so this just enables you to overwrite

2397
01:49:40,560 --> 01:49:44,699
anything you have in your

2398
01:49:43,139 --> 01:49:47,940
settings file

2399
01:49:44,699 --> 01:49:50,820
and you can just specify it in your

2400
01:49:47,940 --> 01:49:52,320
spider so just need to specify what we

2401
01:49:50,820 --> 01:49:56,820
want to overwrite

2402
01:49:52,320 --> 01:50:00,300
and we're going to overwrite our feed

2403
01:49:56,820 --> 01:50:01,800
and we would then push

2404
01:50:00,300 --> 01:50:04,500
our feed

2405
01:50:01,800 --> 01:50:08,280
settings in here

2406
01:50:04,500 --> 01:50:11,280
so if it sees that the feeds are set

2407
01:50:08,280 --> 01:50:14,400
here it will overwrite what we have in

2408
01:50:11,280 --> 01:50:16,920
our settings.py file

2409
01:50:14,400 --> 01:50:19,679
so this is just an easy way that if you

2410
01:50:16,920 --> 01:50:22,380
guys want to specify certain settings

2411
01:50:19,679 --> 01:50:24,900
you can do them here they don't all have

2412
01:50:22,380 --> 01:50:27,179
to be in your settings.py file

2413
01:50:24,900 --> 01:50:29,340
one important thing to note as well with

2414
01:50:27,179 --> 01:50:33,360
our feeds when we either set it in

2415
01:50:29,340 --> 01:50:37,320
settings or in custom settings is we

2416
01:50:33,360 --> 01:50:38,460
need to set the overwriting so like we

2417
01:50:37,320 --> 01:50:41,760
did earlier

2418
01:50:38,460 --> 01:50:46,500
we just have overrice true our overwrite

2419
01:50:41,760 --> 01:50:49,500
false because it depends on where you're

2420
01:50:46,500 --> 01:50:51,719
storing the data what the default is for

2421
01:50:49,500 --> 01:50:53,820
that setting so it's better just to

2422
01:50:51,719 --> 01:50:56,540
specify that we want to or write the

2423
01:50:53,820 --> 01:50:59,820
file or not we can just have it there

2424
01:50:56,540 --> 01:51:02,400
and run it and it'll overwrite our

2425
01:50:59,820 --> 01:51:05,219
current file

2426
01:51:02,400 --> 01:51:08,880
so now that we have that the next thing

2427
01:51:05,219 --> 01:51:14,760
we want to look at is how to save our

2428
01:51:08,880 --> 01:51:16,800
data into databases using our Pipelines

2429
01:51:14,760 --> 01:51:20,159
so I've gone ahead and I've already

2430
01:51:16,800 --> 01:51:21,600
installed MySQL so MySQL is a very

2431
01:51:20,159 --> 01:51:24,719
popular

2432
01:51:21,600 --> 01:51:27,619
database which you can get yourselves

2433
01:51:24,719 --> 01:51:30,119
just by going to

2434
01:51:27,619 --> 01:51:33,000
mysql.com site and their download

2435
01:51:30,119 --> 01:51:34,800
section and you can just choose your

2436
01:51:33,000 --> 01:51:35,880
operating system so if you've got

2437
01:51:34,800 --> 01:51:38,639
windows

2438
01:51:35,880 --> 01:51:41,340
obviously have it for Windows click

2439
01:51:38,639 --> 01:51:43,860
download and install it then they have

2440
01:51:41,340 --> 01:51:45,900
for many other operating systems the

2441
01:51:43,860 --> 01:51:47,820
available downloads there too

2442
01:51:45,900 --> 01:51:50,760
so once you've downloaded and installed

2443
01:51:47,820 --> 01:51:52,380
that you should be able to then make

2444
01:51:50,760 --> 01:51:56,820
sure it's installed correctly just by

2445
01:51:52,380 --> 01:52:00,020
running MySQL and then dash dash version

2446
01:51:56,820 --> 01:52:04,500
so as you can see here I have version

2447
01:52:00,020 --> 01:52:07,500
8.0.32 for Mac OS 11. and that's the

2448
01:52:04,500 --> 01:52:10,800
latest 8.0.32

2449
01:52:07,500 --> 01:52:12,659
so that's installed for me so the next

2450
01:52:10,800 --> 01:52:15,840
thing I'm going to do is just Connect

2451
01:52:12,659 --> 01:52:18,840
into my MySQL

2452
01:52:15,840 --> 01:52:20,520
so I can just type MySQL and then if

2453
01:52:18,840 --> 01:52:21,900
it's a simple install and you've just

2454
01:52:20,520 --> 01:52:23,940
installed this you should be able to

2455
01:52:21,900 --> 01:52:25,080
just hit enter and it brings you

2456
01:52:23,940 --> 01:52:26,940
straight in

2457
01:52:25,080 --> 01:52:30,119
and you know you're connected in because

2458
01:52:26,940 --> 01:52:31,619
you've got MySQL here and then you can

2459
01:52:30,119 --> 01:52:34,080
just say

2460
01:52:31,619 --> 01:52:36,239
show databases

2461
01:52:34,080 --> 01:52:37,679
and it shows the databases so I've

2462
01:52:36,239 --> 01:52:39,360
already gone ahead and created one

2463
01:52:37,679 --> 01:52:42,900
called books

2464
01:52:39,360 --> 01:52:44,580
that's there but you obviously won't

2465
01:52:42,900 --> 01:52:46,199
have that if you just installed it so

2466
01:52:44,580 --> 01:52:48,179
you want to create a database so you

2467
01:52:46,199 --> 01:52:49,739
just do create

2468
01:52:48,179 --> 01:52:53,580
database

2469
01:52:49,739 --> 01:52:56,340
books and then it'll say

2470
01:52:53,580 --> 01:52:58,440
created I've already got the database

2471
01:52:56,340 --> 01:53:00,060
already there so it says database exists

2472
01:52:58,440 --> 01:53:02,880
for me

2473
01:53:00,060 --> 01:53:05,400
so we need a database to actually Savor

2474
01:53:02,880 --> 01:53:07,440
things into and once it's set up it'll

2475
01:53:05,400 --> 01:53:08,820
be there in your list and you just do

2476
01:53:07,440 --> 01:53:10,739
show

2477
01:53:08,820 --> 01:53:12,960
databases

2478
01:53:10,739 --> 01:53:14,040
to get the list of databases that are

2479
01:53:12,960 --> 01:53:16,619
available

2480
01:53:14,040 --> 01:53:19,199
so we can exit out of that once we have

2481
01:53:16,619 --> 01:53:21,600
our database created

2482
01:53:19,199 --> 01:53:24,060
you might have to connect in to your

2483
01:53:21,600 --> 01:53:25,860
MySQL if you've set up a username and

2484
01:53:24,060 --> 01:53:29,000
password are with a different host if

2485
01:53:25,860 --> 01:53:30,840
you've so you could do host

2486
01:53:29,000 --> 01:53:35,580
localhost

2487
01:53:30,840 --> 01:53:37,080
minus U for user root and minus P if

2488
01:53:35,580 --> 01:53:38,760
you've got a password and then it'll

2489
01:53:37,080 --> 01:53:41,699
prompt you for a password

2490
01:53:38,760 --> 01:53:44,400
so depending on what you have

2491
01:53:41,699 --> 01:53:46,440
so that also works to connect in

2492
01:53:44,400 --> 01:53:49,260
if I'd set up a password it would have

2493
01:53:46,440 --> 01:53:51,960
asked me for a password beforehand so if

2494
01:53:49,260 --> 01:53:54,000
you just type MySQL you can usually get

2495
01:53:51,960 --> 01:53:55,739
in if you ever already haven't got a

2496
01:53:54,000 --> 01:53:58,860
password set up or if you're using a

2497
01:53:55,739 --> 01:54:01,800
different host like digitalocean or some

2498
01:53:58,860 --> 01:54:04,440
other third-party provider you can stick

2499
01:54:01,800 --> 01:54:06,179
the URL to where your database is hosted

2500
01:54:04,440 --> 01:54:09,960
there

2501
01:54:06,179 --> 01:54:13,980
so we've got our database set up and the

2502
01:54:09,960 --> 01:54:15,179
next thing we want to do is we want to

2503
01:54:13,980 --> 01:54:19,320
install

2504
01:54:15,179 --> 01:54:22,020
our MySQL connector so just to make sure

2505
01:54:19,320 --> 01:54:23,400
that python is able to connect to our

2506
01:54:22,020 --> 01:54:24,719
database

2507
01:54:23,400 --> 01:54:27,719
so I'm just going to paste in the

2508
01:54:24,719 --> 01:54:29,400
command for that and you guys can

2509
01:54:27,719 --> 01:54:31,739
have a look at my screen there and type

2510
01:54:29,400 --> 01:54:35,699
it in so it's going to install the MySQL

2511
01:54:31,739 --> 01:54:38,940
and MySQL connector python packages

2512
01:54:35,699 --> 01:54:42,440
with using pip again so

2513
01:54:38,940 --> 01:54:42,440
go ahead and run that

2514
01:54:42,480 --> 01:54:49,739
and now that I have that I should be

2515
01:54:44,820 --> 01:54:52,739
able to start working on the pipeline

2516
01:54:49,739 --> 01:54:55,320
so we can just go directly under our

2517
01:54:52,739 --> 01:54:58,500
existing book scraper pipeline

2518
01:54:55,320 --> 01:55:00,659
and we're going to create

2519
01:54:58,500 --> 01:55:02,400
a new class

2520
01:55:00,659 --> 01:55:05,100
and we're just going to call that save

2521
01:55:02,400 --> 01:55:09,260
to mySQL pipeline

2522
01:55:05,100 --> 01:55:12,360
and then we're going to import our MySQL

2523
01:55:09,260 --> 01:55:16,800
connector to help us connect in

2524
01:55:12,360 --> 01:55:20,760
and we're going to initially just

2525
01:55:16,800 --> 01:55:22,080
when this pipeline is initialized we're

2526
01:55:20,760 --> 01:55:24,719
going to

2527
01:55:22,080 --> 01:55:27,719
set up our connection

2528
01:55:24,719 --> 01:55:30,119
and then set up our cursor

2529
01:55:27,719 --> 01:55:31,260
so I'll show you guys now what that

2530
01:55:30,119 --> 01:55:34,619
entails

2531
01:55:31,260 --> 01:55:37,199
so we have this init function here so

2532
01:55:34,619 --> 01:55:38,659
this is going to start up when

2533
01:55:37,199 --> 01:55:41,100
our spider

2534
01:55:38,659 --> 01:55:43,940
initializes this class

2535
01:55:41,100 --> 01:55:46,980
we have we're using the MySQL

2536
01:55:43,940 --> 01:55:48,900
connector.connect here to set up the

2537
01:55:46,980 --> 01:55:51,900
connection we've got our host our

2538
01:55:48,900 --> 01:55:53,580
username password if you have a password

2539
01:55:51,900 --> 01:55:56,280
you have to add it in here and then the

2540
01:55:53,580 --> 01:55:59,400
database that we just created books

2541
01:55:56,280 --> 01:56:02,280
so we can save all that and then we have

2542
01:55:59,400 --> 01:56:03,719
the cursor which is used to execute the

2543
01:56:02,280 --> 01:56:06,179
commands

2544
01:56:03,719 --> 01:56:10,380
so I have that set up here and that's

2545
01:56:06,179 --> 01:56:14,280
saved into self dot Cur so we can use it

2546
01:56:10,380 --> 01:56:16,440
further in other functions

2547
01:56:14,280 --> 01:56:17,820
so the next thing we want to do is we're

2548
01:56:16,440 --> 01:56:20,639
going to add in

2549
01:56:17,820 --> 01:56:24,480
that we want a new table to be created

2550
01:56:20,639 --> 01:56:27,420
if there is no table to store the data

2551
01:56:24,480 --> 01:56:31,199
so this can just be handy in case you

2552
01:56:27,420 --> 01:56:33,360
are running this over and over again

2553
01:56:31,199 --> 01:56:35,940
or you're testing you might want to go

2554
01:56:33,360 --> 01:56:37,920
in drop the table and if you don't want

2555
01:56:35,940 --> 01:56:39,420
to remember did I just create that is a

2556
01:56:37,920 --> 01:56:41,699
table there or not you would have this

2557
01:56:39,420 --> 01:56:44,340
there so that will just make sure that

2558
01:56:41,699 --> 01:56:47,159
there's a table there so it creates a

2559
01:56:44,340 --> 01:56:49,760
table if it doesn't exist called books

2560
01:56:47,159 --> 01:56:54,719
and that table will have the following

2561
01:56:49,760 --> 01:56:56,580
columns ID URL title UPC product type

2562
01:56:54,719 --> 01:56:58,440
everything that we've already been

2563
01:56:56,580 --> 01:56:59,760
scraping from the page all the different

2564
01:56:58,440 --> 01:57:01,920
data points

2565
01:56:59,760 --> 01:57:04,440
so it will set up all these different

2566
01:57:01,920 --> 01:57:07,800
columns including a primary key called

2567
01:57:04,440 --> 01:57:10,320
ID and then all the data will be able to

2568
01:57:07,800 --> 01:57:13,080
saved into the columns that we want it

2569
01:57:10,320 --> 01:57:16,980
to be saved into

2570
01:57:13,080 --> 01:57:19,380
so we'll have the table set up if it's

2571
01:57:16,980 --> 01:57:21,560
not already set up so that's we don't

2572
01:57:19,380 --> 01:57:24,119
have to go in and manually set it up in

2573
01:57:21,560 --> 01:57:26,099
MySQL ourselves

2574
01:57:24,119 --> 01:57:27,300
and the next thing we'd want to look at

2575
01:57:26,099 --> 01:57:30,000
is

2576
01:57:27,300 --> 01:57:32,639
we're going to again use the process

2577
01:57:30,000 --> 01:57:34,619
item function

2578
01:57:32,639 --> 01:57:37,980
so we've already had that in our other

2579
01:57:34,619 --> 01:57:39,420
pipeline but we're going to add it in

2580
01:57:37,980 --> 01:57:41,699
here

2581
01:57:39,420 --> 01:57:43,500
and this is where we're going to have

2582
01:57:41,699 --> 01:57:49,020
our insert statement so it's going to

2583
01:57:43,500 --> 01:57:51,060
insert our data that we have in our item

2584
01:57:49,020 --> 01:57:53,159
so here it is

2585
01:57:51,060 --> 01:57:55,199
pretty simple so using the cursor that

2586
01:57:53,159 --> 01:57:57,840
we've already defined above I'm going to

2587
01:57:55,199 --> 01:58:01,619
say please execute this command insert

2588
01:57:57,840 --> 01:58:04,260
into books URL title UPC product type

2589
01:58:01,619 --> 01:58:05,760
all the pieces of data that we've

2590
01:58:04,260 --> 01:58:07,139
already scraped

2591
01:58:05,760 --> 01:58:09,540
so

2592
01:58:07,139 --> 01:58:12,300
once that's insert statement just there

2593
01:58:09,540 --> 01:58:14,639
we have to use commit to make sure that

2594
01:58:12,300 --> 01:58:17,099
the insert is actually executed

2595
01:58:14,639 --> 01:58:18,599
correctly and then we just return the

2596
01:58:17,099 --> 01:58:21,119
item

2597
01:58:18,599 --> 01:58:23,580
so that if we add one more layer to our

2598
01:58:21,119 --> 01:58:26,400
pipeline that the item is returned and

2599
01:58:23,580 --> 01:58:28,860
the next piece of our pipeline can also

2600
01:58:26,400 --> 01:58:31,080
continue the only other thing we need to

2601
01:58:28,860 --> 01:58:33,480
add in now is

2602
01:58:31,080 --> 01:58:37,500
we want the connection to our database

2603
01:58:33,480 --> 01:58:40,560
to be closed once the spider is finished

2604
01:58:37,500 --> 01:58:43,560
so to do that we just add in that

2605
01:58:40,560 --> 01:58:45,659
enclosed spider so this is just

2606
01:58:43,560 --> 01:58:48,179
a function that's scrapey looks for if

2607
01:58:45,659 --> 01:58:50,820
close spider is there it executes closed

2608
01:58:48,179 --> 01:58:52,139
spider once the spider is ready to close

2609
01:58:50,820 --> 01:58:55,560
at the end

2610
01:58:52,139 --> 01:58:59,159
so inside enclosed spider we just add in

2611
01:58:55,560 --> 01:59:01,440
cursor close and connection close so

2612
01:58:59,159 --> 01:59:02,639
we're just closing the cursor and the

2613
01:59:01,440 --> 01:59:05,420
connection

2614
01:59:02,639 --> 01:59:09,719
just so that this stuff isn't kept open

2615
01:59:05,420 --> 01:59:12,179
and using memory if we're executing this

2616
01:59:09,719 --> 01:59:14,940
lots and lots of times we don't want all

2617
01:59:12,179 --> 01:59:19,040
this memory to be taken up with cursors

2618
01:59:14,940 --> 01:59:19,040
and connections that are not being used

2619
01:59:19,080 --> 01:59:25,020
so now that we have that we need to go

2620
01:59:22,440 --> 01:59:28,440
to our settings and we need to enable

2621
01:59:25,020 --> 01:59:31,619
our new pipeline

2622
01:59:28,440 --> 01:59:35,520
so we're just going to copy the existing

2623
01:59:31,619 --> 01:59:40,080
line and we're going to say

2624
01:59:35,520 --> 01:59:42,960
execute our pipeline after this

2625
01:59:40,080 --> 01:59:45,000
and existing one so we want the data to

2626
01:59:42,960 --> 01:59:51,060
be clean first and the second step is

2627
01:59:45,000 --> 01:59:54,599
save the data into our mySQL database so

2628
01:59:51,060 --> 01:59:57,000
we're going to just copy the class name

2629
01:59:54,599 --> 01:59:59,460
go to settings

2630
01:59:57,000 --> 02:00:01,800
paste that in here

2631
01:59:59,460 --> 02:00:04,380
and then the only thing we need to do is

2632
02:00:01,800 --> 02:00:05,940
we need to change this number here

2633
02:00:04,380 --> 02:00:07,020
so I don't think I've talked about this

2634
02:00:05,940 --> 02:00:09,239
number yet

2635
02:00:07,020 --> 02:00:13,080
so what this number is is that it's just

2636
02:00:09,239 --> 02:00:16,139
the order in which the items in the item

2637
02:00:13,080 --> 02:00:18,000
pipeline have precedence so the lower

2638
02:00:16,139 --> 02:00:20,340
the number the higher

2639
02:00:18,000 --> 02:00:22,440
the order of importance the first thing

2640
02:00:20,340 --> 02:00:25,619
that's going to be executed so in this

2641
02:00:22,440 --> 02:00:27,659
case number 300 is going to be executed

2642
02:00:25,619 --> 02:00:29,760
first and then number 400 is going to be

2643
02:00:27,659 --> 02:00:32,520
executed after that so this is an easy

2644
02:00:29,760 --> 02:00:35,040
way for us to say please execute this

2645
02:00:32,520 --> 02:00:37,800
pipeline first and this pipeline second

2646
02:00:35,040 --> 02:00:39,119
and if you had multiple pipelines you

2647
02:00:37,800 --> 02:00:41,699
can just use these numbers it doesn't

2648
02:00:39,119 --> 02:00:44,940
have to be three or four hundred it can

2649
02:00:41,699 --> 02:00:47,099
be any number you want I've just picked

2650
02:00:44,940 --> 02:00:51,239
three and four hundred for now

2651
02:00:47,099 --> 02:00:55,320
okay so now that we have that we should

2652
02:00:51,239 --> 02:00:57,840
be able to go ahead and check our

2653
02:00:55,320 --> 02:01:00,060
database to see did the

2654
02:00:57,840 --> 02:01:01,320
items get saved into the database

2655
02:01:00,060 --> 02:01:03,179
correctly

2656
02:01:01,320 --> 02:01:05,820
so let's do that now

2657
02:01:03,179 --> 02:01:10,040
so as before we're just going to do

2658
02:01:05,820 --> 02:01:10,040
Scrappy crawl book Spider

2659
02:01:11,520 --> 02:01:16,040
and

2660
02:01:13,139 --> 02:01:16,040
should kick off

2661
02:01:16,199 --> 02:01:22,159
so we have several books after being

2662
02:01:19,139 --> 02:01:24,780
scraped so that's let's stop our spider

2663
02:01:22,159 --> 02:01:26,520
the next thing we want to do is we want

2664
02:01:24,780 --> 02:01:28,920
to

2665
02:01:26,520 --> 02:01:31,320
log back into our

2666
02:01:28,920 --> 02:01:35,219
MySQL console

2667
02:01:31,320 --> 02:01:37,980
and then we want to

2668
02:01:35,219 --> 02:01:39,599
show database

2669
02:01:37,980 --> 02:01:43,080
this is

2670
02:01:39,599 --> 02:01:44,940
and then we want to use

2671
02:01:43,080 --> 02:01:47,400
books

2672
02:01:44,940 --> 02:01:48,599
this just enables us to select from that

2673
02:01:47,400 --> 02:01:50,340
database

2674
02:01:48,599 --> 02:01:52,380
so once we're using the books database

2675
02:01:50,340 --> 02:01:54,780
we want to just

2676
02:01:52,380 --> 02:01:56,940
show tables

2677
02:01:54,780 --> 02:02:00,540
and we can see that the table was

2678
02:01:56,940 --> 02:02:02,820
created that we asked to create books

2679
02:02:00,540 --> 02:02:05,219
and then we can do

2680
02:02:02,820 --> 02:02:07,920
select all

2681
02:02:05,219 --> 02:02:12,719
from books

2682
02:02:07,920 --> 02:02:16,440
and we can see that there's 138 rows

2683
02:02:12,719 --> 02:02:18,480
there and Dash D data looks like it's

2684
02:02:16,440 --> 02:02:19,619
safe correctly we've got the name of the

2685
02:02:18,480 --> 02:02:21,540
books

2686
02:02:19,619 --> 02:02:23,699
all the other pieces of data that we had

2687
02:02:21,540 --> 02:02:25,320
the description the price

2688
02:02:23,699 --> 02:02:29,040
the tax

2689
02:02:25,320 --> 02:02:32,360
the availability the category it all

2690
02:02:29,040 --> 02:02:32,360
looks like it's saved there correctly

2691
02:02:33,119 --> 02:02:37,920
so we can

2692
02:02:35,880 --> 02:02:40,560
drop the table

2693
02:02:37,920 --> 02:02:43,139
so dropping the table just basically

2694
02:02:40,560 --> 02:02:45,480
removes the table so that it won't exist

2695
02:02:43,139 --> 02:02:48,239
drop table books

2696
02:02:45,480 --> 02:02:49,980
if we want to start again because

2697
02:02:48,239 --> 02:02:51,780
otherwise what it's going to do is it's

2698
02:02:49,980 --> 02:02:52,980
going to keep appending on to the

2699
02:02:51,780 --> 02:02:56,280
database

2700
02:02:52,980 --> 02:02:57,960
so we can see if we show tables that

2701
02:02:56,280 --> 02:02:58,860
there's no more tables in the database

2702
02:02:57,960 --> 02:03:02,040
now

2703
02:02:58,860 --> 02:03:05,580
but our pipeline creates a new table

2704
02:03:02,040 --> 02:03:08,580
anyway so that's fine

2705
02:03:05,580 --> 02:03:10,020
so that's how we create a simple

2706
02:03:08,580 --> 02:03:12,420
database

2707
02:03:10,020 --> 02:03:13,920
get it set up and have a simple pipeline

2708
02:03:12,420 --> 02:03:17,460
script that

2709
02:03:13,920 --> 02:03:19,560
once the data is cleaned up with our

2710
02:03:17,460 --> 02:03:22,800
first pipeline that we did in part six

2711
02:03:19,560 --> 02:03:25,860
it then inserts it into a mySQL database

2712
02:03:22,800 --> 02:03:28,619
in this tutorial that we've just done

2713
02:03:25,860 --> 02:03:29,639
again using our Pipelines so obviously

2714
02:03:28,619 --> 02:03:32,159
if you're

2715
02:03:29,639 --> 02:03:34,440
more familiar with using postgres

2716
02:03:32,159 --> 02:03:37,260
databases or other types of databases

2717
02:03:34,440 --> 02:03:40,800
you can just modify the pipeline

2718
02:03:37,260 --> 02:03:43,080
slightly we will have available articles

2719
02:03:40,800 --> 02:03:47,940
where it'll show you the exact code you

2720
02:03:43,080 --> 02:03:50,219
need to use a pipeline to insert the

2721
02:03:47,940 --> 02:03:52,199
database into a postgres database also

2722
02:03:50,219 --> 02:03:55,199
so you guys can have a look at the

2723
02:03:52,199 --> 02:03:57,780
articles that we'll attach and we'll

2724
02:03:55,199 --> 02:03:59,159
also have the code repos there for you

2725
02:03:57,780 --> 02:04:03,300
guys to

2726
02:03:59,159 --> 02:04:06,599
just download and play around with two

2727
02:04:03,300 --> 02:04:08,460
so I think that's it for part seven in

2728
02:04:06,599 --> 02:04:11,280
part eight we're going to be looking at

2729
02:04:08,460 --> 02:04:16,739
how we can use

2730
02:04:11,280 --> 02:04:20,219
the user agents and headers to get

2731
02:04:16,739 --> 02:04:21,719
around issues with being blocked when

2732
02:04:20,219 --> 02:04:23,099
we're trying to scrape different

2733
02:04:21,719 --> 02:04:25,619
websites

2734
02:04:23,099 --> 02:04:28,139
so we're going to be looking at user

2735
02:04:25,619 --> 02:04:30,000
agents and headers in detail what they

2736
02:04:28,139 --> 02:04:32,219
are how to use them

2737
02:04:30,000 --> 02:04:34,520
so see you in partake guys thanks for

2738
02:04:32,219 --> 02:04:34,520
watching

2739
02:04:36,420 --> 02:04:40,380
welcome to part 8 of the scrapey

2740
02:04:39,000 --> 02:04:42,659
beginners course

2741
02:04:40,380 --> 02:04:44,159
for a free code camp

2742
02:04:42,659 --> 02:04:47,099
so in part 8 we're going to be looking

2743
02:04:44,159 --> 02:04:48,480
at why we get blocked when we're

2744
02:04:47,099 --> 02:04:51,659
scraping the web

2745
02:04:48,480 --> 02:04:55,380
what types of websites might block us

2746
02:04:51,659 --> 02:04:59,219
and then how to use user agents and

2747
02:04:55,380 --> 02:05:02,280
headers to bypass instances where we're

2748
02:04:59,219 --> 02:05:05,520
getting blocked while scraping

2749
02:05:02,280 --> 02:05:09,360
so we'll start off by going straight

2750
02:05:05,520 --> 02:05:12,540
into what headers are

2751
02:05:09,360 --> 02:05:14,099
so if you go to the site we've been

2752
02:05:12,540 --> 02:05:17,099
scraping in the last two parts

2753
02:05:14,099 --> 02:05:17,099
books.2scrape.com

2754
02:05:17,520 --> 02:05:22,500
you open inspect

2755
02:05:20,099 --> 02:05:25,320
an element on the page go to the

2756
02:05:22,500 --> 02:05:28,619
networking Tab and then

2757
02:05:25,320 --> 02:05:31,619
simply refresh the page if you've got

2758
02:05:28,619 --> 02:05:34,320
doc selected or all selected you will

2759
02:05:31,619 --> 02:05:36,840
see what we want to see

2760
02:05:34,320 --> 02:05:41,219
so because this is a just a simple

2761
02:05:36,840 --> 02:05:45,060
website the HTML is sent to us and we

2762
02:05:41,219 --> 02:05:49,440
can see it returned in the preview

2763
02:05:45,060 --> 02:05:51,840
so you can see all the HTML is there

2764
02:05:49,440 --> 02:05:53,940
and that's what we end up scraping when

2765
02:05:51,840 --> 02:05:57,239
we are using scrapey

2766
02:05:53,940 --> 02:06:00,260
but if we look at the headers tab

2767
02:05:57,239 --> 02:06:04,440
we can see everything that is sent when

2768
02:06:00,260 --> 02:06:06,719
we request this page and you've got the

2769
02:06:04,440 --> 02:06:08,400
request URL which is just the URL of the

2770
02:06:06,719 --> 02:06:10,560
site we're trying to scrape you've got

2771
02:06:08,400 --> 02:06:13,739
the method are we trying to get the page

2772
02:06:10,560 --> 02:06:15,540
are we posting data to the page

2773
02:06:13,739 --> 02:06:16,860
and then you've got things like the

2774
02:06:15,540 --> 02:06:19,500
status code

2775
02:06:16,860 --> 02:06:23,400
and so on so on now the important stuff

2776
02:06:19,500 --> 02:06:28,020
for us are the request headers

2777
02:06:23,400 --> 02:06:33,060
so this is everything that we send when

2778
02:06:28,020 --> 02:06:35,639
we make a request to books.2script.com

2779
02:06:33,060 --> 02:06:39,060
and as part of this

2780
02:06:35,639 --> 02:06:40,380
the most important part for us in this

2781
02:06:39,060 --> 02:06:44,580
part eight

2782
02:06:40,380 --> 02:06:46,560
of the tutorial is the user agent

2783
02:06:44,580 --> 02:06:48,480
so the user agent

2784
02:06:46,560 --> 02:06:53,040
gives you all the important information

2785
02:06:48,480 --> 02:06:56,099
about who you are to the server that

2786
02:06:53,040 --> 02:07:00,320
you're requesting the web page from so

2787
02:06:56,099 --> 02:07:03,840
here we can see if I copy this string

2788
02:07:00,320 --> 02:07:07,980
and I've got this site user agent

2789
02:07:03,840 --> 02:07:10,679
string.com and it lets you paste in

2790
02:07:07,980 --> 02:07:12,840
and analyze user agents

2791
02:07:10,679 --> 02:07:15,119
so this is all the stuff that

2792
02:07:12,840 --> 02:07:17,040
is is contained when we make a request

2793
02:07:15,119 --> 02:07:18,960
to a website

2794
02:07:17,040 --> 02:07:20,760
straight away it knows that we're using

2795
02:07:18,960 --> 02:07:21,840
Chrome what version of Chrome we're

2796
02:07:20,760 --> 02:07:24,000
using

2797
02:07:21,840 --> 02:07:25,500
the render engine

2798
02:07:24,000 --> 02:07:28,920
that we're using

2799
02:07:25,500 --> 02:07:32,400
your operating system so I'm using OS X

2800
02:07:28,920 --> 02:07:35,580
I'm using an Intel CPU so all this kind

2801
02:07:32,400 --> 02:07:37,500
of data is sent with every request you

2802
02:07:35,580 --> 02:07:40,619
make automatically

2803
02:07:37,500 --> 02:07:43,679
now this is fine when you're browsing

2804
02:07:40,619 --> 02:07:45,659
the web or you're building your spiders

2805
02:07:43,679 --> 02:07:47,400
and you're doing a bit of testing but if

2806
02:07:45,659 --> 02:07:50,159
you're doing any sort of large-scale

2807
02:07:47,400 --> 02:07:52,260
scraping on any kind of commercial sites

2808
02:07:50,159 --> 02:07:54,719
you're more than likely going to start

2809
02:07:52,260 --> 02:07:56,880
to get blocked

2810
02:07:54,719 --> 02:07:59,099
so a lot of sites think I don't know

2811
02:07:56,880 --> 02:08:02,880
Amazon Walmart

2812
02:07:59,099 --> 02:08:06,119
any kind of big eCommerce sites most of

2813
02:08:02,880 --> 02:08:09,540
them will have some form of antibots

2814
02:08:06,119 --> 02:08:11,639
that stop you from scraping now you

2815
02:08:09,540 --> 02:08:13,380
might say why do they want to stop me

2816
02:08:11,639 --> 02:08:15,900
I'm not doing anything bad I just want

2817
02:08:13,380 --> 02:08:19,639
to collect their data well they'll say

2818
02:08:15,900 --> 02:08:22,500
well this is our data we own the website

2819
02:08:19,639 --> 02:08:25,619
this is only for our customers

2820
02:08:22,500 --> 02:08:27,960
so on so forth now obviously

2821
02:08:25,619 --> 02:08:30,659
you need to look at the terms conditions

2822
02:08:27,960 --> 02:08:34,199
of the site you're scraping and judge

2823
02:08:30,659 --> 02:08:37,560
for yourself if it's legal or illegal

2824
02:08:34,199 --> 02:08:39,659
the rule of thumb to go by is that if

2825
02:08:37,560 --> 02:08:42,179
it's publicly available and you don't

2826
02:08:39,659 --> 02:08:46,739
have to log in and give your details

2827
02:08:42,179 --> 02:08:49,800
then it's more than likely okay to

2828
02:08:46,739 --> 02:08:52,860
scrape the data of the website if you

2829
02:08:49,800 --> 02:08:56,159
have to log in first and by logging in

2830
02:08:52,860 --> 02:08:58,619
you might be agreeing to certain terms

2831
02:08:56,159 --> 02:09:00,780
and conditions then more than likely the

2832
02:08:58,619 --> 02:09:02,280
website will have that in their terms

2833
02:09:00,780 --> 02:09:05,639
and conditions that you are not allowed

2834
02:09:02,280 --> 02:09:09,360
to scrape their data so that's up to you

2835
02:09:05,639 --> 02:09:11,699
guys to decide in a case-by-case basis

2836
02:09:09,360 --> 02:09:12,719
but for a lot of simpler sites like the

2837
02:09:11,699 --> 02:09:15,719
one where

2838
02:09:12,719 --> 02:09:16,860
we're scraping in our tutorial series

2839
02:09:15,719 --> 02:09:19,679
here

2840
02:09:16,860 --> 02:09:22,440
this site has no antibots on it so there

2841
02:09:19,679 --> 02:09:25,440
is nothing which will block us even if

2842
02:09:22,440 --> 02:09:28,679
we have the same user agent

2843
02:09:25,440 --> 02:09:30,179
so therefore it knows if if it gets a

2844
02:09:28,679 --> 02:09:33,780
thousand requests

2845
02:09:30,179 --> 02:09:36,239
from this chrome and my Mac that it

2846
02:09:33,780 --> 02:09:37,440
knows that it's me then it's it's not

2847
02:09:36,239 --> 02:09:38,639
going to do anything about it it's not

2848
02:09:37,440 --> 02:09:41,880
going to block me when I'm trying to

2849
02:09:38,639 --> 02:09:43,320
scrape all these books off this website

2850
02:09:41,880 --> 02:09:46,980
but that's obviously because this

2851
02:09:43,320 --> 02:09:48,480
website is there for people to learn on

2852
02:09:46,980 --> 02:09:51,360
so

2853
02:09:48,480 --> 02:09:53,940
that's kind of why we get blocked so the

2854
02:09:51,360 --> 02:09:57,000
other things that they look for is they

2855
02:09:53,940 --> 02:10:00,719
look at the IP address of the machine

2856
02:09:57,000 --> 02:10:03,780
that you're using so that's also a a

2857
02:10:00,719 --> 02:10:06,659
very simple way for websites to block

2858
02:10:03,780 --> 02:10:08,820
who the requests because they can see

2859
02:10:06,659 --> 02:10:11,639
your IP address every time you make a

2860
02:10:08,820 --> 02:10:12,599
request so they normally look for the IP

2861
02:10:11,639 --> 02:10:14,820
address

2862
02:10:12,599 --> 02:10:17,940
and they might set something in your

2863
02:10:14,820 --> 02:10:21,420
cookies in your session so they might

2864
02:10:17,940 --> 02:10:24,119
set some kind of flag or counter there

2865
02:10:21,420 --> 02:10:25,560
so they know that it's you coming back

2866
02:10:24,119 --> 02:10:28,380
every time

2867
02:10:25,560 --> 02:10:31,500
so it's mainly IP address the cookies or

2868
02:10:28,380 --> 02:10:34,679
the sessions and then the headers in

2869
02:10:31,500 --> 02:10:38,580
general and as part of that the user

2870
02:10:34,679 --> 02:10:41,099
agents so the difference between headers

2871
02:10:38,580 --> 02:10:44,099
and user agents is that headers

2872
02:10:41,099 --> 02:10:48,239
is everything that we have here

2873
02:10:44,099 --> 02:10:51,119
so it it encompasses things like the

2874
02:10:48,239 --> 02:10:54,659
accepted things that are returned does

2875
02:10:51,119 --> 02:10:56,699
it take HTML or does it take images you

2876
02:10:54,659 --> 02:10:59,340
know what do we accept back what does

2877
02:10:56,699 --> 02:11:02,659
the browser accept back as a as a

2878
02:10:59,340 --> 02:11:05,940
response to things like the language

2879
02:11:02,659 --> 02:11:07,199
encoding and then the user agent is just

2880
02:11:05,940 --> 02:11:11,280
one

2881
02:11:07,199 --> 02:11:12,780
subset of the overall request headers

2882
02:11:11,280 --> 02:11:15,440
so

2883
02:11:12,780 --> 02:11:20,040
for some sites that are not too complex

2884
02:11:15,440 --> 02:11:21,780
if we change the user agent each time we

2885
02:11:20,040 --> 02:11:24,420
make a request

2886
02:11:21,780 --> 02:11:25,920
the website will think that it is a

2887
02:11:24,420 --> 02:11:28,860
different browser and a different

2888
02:11:25,920 --> 02:11:31,260
computer looking for the data on the

2889
02:11:28,860 --> 02:11:33,900
site every time so it'll let the request

2890
02:11:31,260 --> 02:11:37,440
go through however for more complicated

2891
02:11:33,900 --> 02:11:39,480
sites they'll look at everything in the

2892
02:11:37,440 --> 02:11:41,699
request headers and they'll want

2893
02:11:39,480 --> 02:11:44,820
everything to be different or at least

2894
02:11:41,699 --> 02:11:48,239
slightly different so

2895
02:11:44,820 --> 02:11:51,360
for example I have Mac OS here so if Mac

2896
02:11:48,239 --> 02:11:54,000
OS is coming every single time and they

2897
02:11:51,360 --> 02:11:56,639
match that plus they can see my Google

2898
02:11:54,000 --> 02:11:58,800
Chrome version here as well and the

2899
02:11:56,639 --> 02:12:01,380
version of chromium then they might say

2900
02:11:58,800 --> 02:12:03,260
okay this looks too similar even though

2901
02:12:01,380 --> 02:12:05,820
their user agent is changing every time

2902
02:12:03,260 --> 02:12:09,000
this looks suspicious and they might

2903
02:12:05,820 --> 02:12:10,500
flag my requests and block my requests

2904
02:12:09,000 --> 02:12:13,260
or at least they might not even block

2905
02:12:10,500 --> 02:12:16,739
them but they might throw up a capture

2906
02:12:13,260 --> 02:12:18,420
page so that if you're not actively

2907
02:12:16,739 --> 02:12:20,520
solving the capture

2908
02:12:18,420 --> 02:12:23,099
the requests are being blocked

2909
02:12:20,520 --> 02:12:26,460
so for the most comp for the more

2910
02:12:23,099 --> 02:12:29,099
complicated sites we need to also be

2911
02:12:26,460 --> 02:12:32,820
changing the entirety of the request

2912
02:12:29,099 --> 02:12:35,940
headers not just the user agents

2913
02:12:32,820 --> 02:12:37,980
so they're the kind of main things we

2914
02:12:35,940 --> 02:12:39,780
need to look at is how can we change

2915
02:12:37,980 --> 02:12:43,500
your IP address to stop getting blocked

2916
02:12:39,780 --> 02:12:46,199
how can we change our user agents and

2917
02:12:43,500 --> 02:12:48,420
also the entirety of the request headers

2918
02:12:46,199 --> 02:12:49,739
for the more complicated sites so that's

2919
02:12:48,420 --> 02:12:52,980
what we're going to be doing in part 8

2920
02:12:49,739 --> 02:12:55,860
and part nine is looking how to bypass

2921
02:12:52,980 --> 02:12:58,440
being blocked by changing these

2922
02:12:55,860 --> 02:13:01,739
different parts that are sent when we

2923
02:12:58,440 --> 02:13:03,000
make a request for a web page

2924
02:13:01,739 --> 02:13:05,040
okay

2925
02:13:03,000 --> 02:13:09,840
so now that we've gone through the

2926
02:13:05,040 --> 02:13:13,440
theory of it of what is in a header and

2927
02:13:09,840 --> 02:13:15,179
user agent and what details the websites

2928
02:13:13,440 --> 02:13:18,599
are looking at when we make a request

2929
02:13:15,179 --> 02:13:21,960
let's go to our spider and Implement

2930
02:13:18,599 --> 02:13:25,020
different user agents every time

2931
02:13:21,960 --> 02:13:27,480
how we can get multiple different user

2932
02:13:25,020 --> 02:13:30,300
agents insert them into our spider use

2933
02:13:27,480 --> 02:13:33,300
middlewares to do that and then also do

2934
02:13:30,300 --> 02:13:36,780
it for request headers

2935
02:13:33,300 --> 02:13:39,719
okay so if we go back to our spider

2936
02:13:36,780 --> 02:13:40,980
we're continuing on from part seven if

2937
02:13:39,719 --> 02:13:43,440
you're just joining us for this part

2938
02:13:40,980 --> 02:13:46,199
eight we have a link to download the

2939
02:13:43,440 --> 02:13:48,300
code so you can hop in and start right

2940
02:13:46,199 --> 02:13:49,520
here with us now

2941
02:13:48,300 --> 02:13:52,440
so

2942
02:13:49,520 --> 02:13:54,239
the first thing I'm going to do is I'm

2943
02:13:52,440 --> 02:13:57,599
going to

2944
02:13:54,239 --> 02:14:00,900
just go to my settings and disable

2945
02:13:57,599 --> 02:14:04,980
the pipeline which saves the data to

2946
02:14:00,900 --> 02:14:08,639
mySQL because I don't need to do that

2947
02:14:04,980 --> 02:14:11,219
for the purpose of this part H so the

2948
02:14:08,639 --> 02:14:12,900
next thing is I'm going to open up my

2949
02:14:11,219 --> 02:14:14,520
spider

2950
02:14:12,900 --> 02:14:19,380
and

2951
02:14:14,520 --> 02:14:21,179
I'm going to set a user agent so I'm

2952
02:14:19,380 --> 02:14:23,820
going to do that the simplest way to do

2953
02:14:21,179 --> 02:14:27,540
it is actually to go to our settings

2954
02:14:23,820 --> 02:14:29,280
and in the settings we can directly set

2955
02:14:27,540 --> 02:14:31,320
a user agent

2956
02:14:29,280 --> 02:14:35,599
so

2957
02:14:31,320 --> 02:14:40,199
just like I had shown you in the browser

2958
02:14:35,599 --> 02:14:43,679
it's user agent and then it contains all

2959
02:14:40,199 --> 02:14:48,060
the available information about

2960
02:14:43,679 --> 02:14:51,000
the user who is requesting the web data

2961
02:14:48,060 --> 02:14:53,460
so here you can see Mac OS X

2962
02:14:51,000 --> 02:14:56,820
you can see it's an iPad

2963
02:14:53,460 --> 02:14:58,739
etc etc etc so this is just an example

2964
02:14:56,820 --> 02:15:01,500
if you wanted to send this user agent

2965
02:14:58,739 --> 02:15:04,320
with every single request you can set it

2966
02:15:01,500 --> 02:15:06,300
in the settings now obviously that

2967
02:15:04,320 --> 02:15:10,440
doesn't make sense because

2968
02:15:06,300 --> 02:15:13,440
it's not changing So within 10 or 20

2969
02:15:10,440 --> 02:15:16,079
requests the website is going to say hey

2970
02:15:13,440 --> 02:15:18,480
this is the same person every time

2971
02:15:16,079 --> 02:15:20,880
they're making a lot of requests they're

2972
02:15:18,480 --> 02:15:24,000
probably web scraping and they'll ask

2973
02:15:20,880 --> 02:15:26,699
for capture audio blockers so this is

2974
02:15:24,000 --> 02:15:30,000
not sufficient this is just if you want

2975
02:15:26,699 --> 02:15:32,639
to set one specific user agent for every

2976
02:15:30,000 --> 02:15:33,960
single request so for now we'll remove

2977
02:15:32,639 --> 02:15:38,099
that again

2978
02:15:33,960 --> 02:15:42,179
and we will look at how we can create a

2979
02:15:38,099 --> 02:15:45,659
list and rotate through that list

2980
02:15:42,179 --> 02:15:52,679
so what we're going to do is go back to

2981
02:15:45,659 --> 02:15:56,400
our spider and we are going to create a

2982
02:15:52,679 --> 02:15:59,099
list of user agents

2983
02:15:56,400 --> 02:16:01,199
so here's one I've pasted in from our

2984
02:15:59,099 --> 02:16:04,739
article you can check out our article

2985
02:16:01,199 --> 02:16:07,260
and paste it in yourself as well

2986
02:16:04,739 --> 02:16:09,179
and we also have this available in the

2987
02:16:07,260 --> 02:16:12,000
GitHub repo so you don't have to type

2988
02:16:09,179 --> 02:16:15,119
out everything here yourself

2989
02:16:12,000 --> 02:16:16,440
so we've got a list the next thing we

2990
02:16:15,119 --> 02:16:20,360
want to do is

2991
02:16:16,440 --> 02:16:22,980
add the user agent into every single

2992
02:16:20,360 --> 02:16:25,199
request that we make

2993
02:16:22,980 --> 02:16:30,119
so to do that

2994
02:16:25,199 --> 02:16:31,679
we can go to where we make the requests

2995
02:16:30,119 --> 02:16:36,420
every time

2996
02:16:31,679 --> 02:16:41,099
and we can specify that we want our user

2997
02:16:36,420 --> 02:16:42,300
agent to be overwritten to do that we

2998
02:16:41,099 --> 02:16:44,760
just

2999
02:16:42,300 --> 02:16:46,080
go to where we have our callback and our

3000
02:16:44,760 --> 02:16:49,800
book URL

3001
02:16:46,080 --> 02:16:52,920
and we would do something like this

3002
02:16:49,800 --> 02:16:53,700
so we would have our headers and we're

3003
02:16:52,920 --> 02:16:56,280
saying

3004
02:16:53,700 --> 02:16:57,780
overwrite the user agent part of our

3005
02:16:56,280 --> 02:17:02,040
headers

3006
02:16:57,780 --> 02:17:03,240
and we'll do this with our user agent

3007
02:17:02,040 --> 02:17:07,620
list

3008
02:17:03,240 --> 02:17:09,179
so we just need to specify self dot user

3009
02:17:07,620 --> 02:17:10,800
agent list

3010
02:17:09,179 --> 02:17:14,700
and then

3011
02:17:10,800 --> 02:17:17,219
we'll import random so we can

3012
02:17:14,700 --> 02:17:21,300
switch between different things at

3013
02:17:17,219 --> 02:17:25,979
random and we'll do self.user agent list

3014
02:17:21,300 --> 02:17:29,700
here and I think that's all we need

3015
02:17:25,979 --> 02:17:32,460
so what this does is we're saying

3016
02:17:29,700 --> 02:17:35,460
add this user agent to our headers when

3017
02:17:32,460 --> 02:17:39,719
we make the request and pick

3018
02:17:35,460 --> 02:17:42,179
a random user agent from between 0 and

3019
02:17:39,719 --> 02:17:43,559
the length of the user agent so it's

3020
02:17:42,179 --> 02:17:46,800
going to pick

3021
02:17:43,559 --> 02:17:48,780
one of these guys at random and insert

3022
02:17:46,800 --> 02:17:51,599
it in to our header

3023
02:17:48,780 --> 02:17:54,840
now obviously you need to add this

3024
02:17:51,599 --> 02:17:57,179
line that we just added to everywhere

3025
02:17:54,840 --> 02:18:02,660
that is

3026
02:17:57,179 --> 02:18:02,660
making a response dot follow

3027
02:18:03,120 --> 02:18:09,059
so we can add it here as well

3028
02:18:06,300 --> 02:18:10,620
and I think that's the only two places

3029
02:18:09,059 --> 02:18:11,760
we have them

3030
02:18:10,620 --> 02:18:13,920
so

3031
02:18:11,760 --> 02:18:16,620
we should be able to run that

3032
02:18:13,920 --> 02:18:19,519
and it should send a different user

3033
02:18:16,620 --> 02:18:19,519
agent every time

3034
02:18:20,040 --> 02:18:26,880
but as you might guess

3035
02:18:22,859 --> 02:18:30,359
this isn't really enough to spoof a

3036
02:18:26,880 --> 02:18:32,760
large-scale website they'll see if

3037
02:18:30,359 --> 02:18:34,979
you're doing thousands of requests okay

3038
02:18:32,760 --> 02:18:39,179
there's only

3039
02:18:34,979 --> 02:18:42,359
five different user agents and they'll

3040
02:18:39,179 --> 02:18:46,620
say we need to block this user

3041
02:18:42,359 --> 02:18:50,040
so that brings us on to how we can use a

3042
02:18:46,620 --> 02:18:52,979
fake user agent API to give us a massive

3043
02:18:50,040 --> 02:18:56,580
list of thousands of user agents and

3044
02:18:52,979 --> 02:18:59,160
then we can Loop through that list of

3045
02:18:56,580 --> 02:19:01,139
thousands of user agents but instead of

3046
02:18:59,160 --> 02:19:03,899
having them all here directly in our

3047
02:19:01,139 --> 02:19:06,420
spider what we would do is we would

3048
02:19:03,899 --> 02:19:08,059
Implement and middleware

3049
02:19:06,420 --> 02:19:11,399
and we would add it into our

3050
02:19:08,059 --> 02:19:14,939
middlewares.py and in this middleware is

3051
02:19:11,399 --> 02:19:17,880
where we would rotate through all the

3052
02:19:14,939 --> 02:19:22,260
different fake user agents that we would

3053
02:19:17,880 --> 02:19:23,760
be getting from a fake user agent API

3054
02:19:22,260 --> 02:19:26,160
so that's what we're going to look at

3055
02:19:23,760 --> 02:19:29,099
next we're going to create a middleware

3056
02:19:26,160 --> 02:19:32,880
and we're going to ask for those fake

3057
02:19:29,099 --> 02:19:35,280
user Agents from the third-party website

3058
02:19:32,880 --> 02:19:38,820
get those user agents returned to us

3059
02:19:35,280 --> 02:19:40,080
again pass those in to our request

3060
02:19:38,820 --> 02:19:43,080
headers

3061
02:19:40,080 --> 02:19:45,840
so to get those request headers we can

3062
02:19:43,080 --> 02:19:48,179
go back to our browser and go to

3063
02:19:45,840 --> 02:19:50,399
scrapeups.io

3064
02:19:48,179 --> 02:19:53,220
where you can sign up for a free account

3065
02:19:50,399 --> 02:19:57,479
and then using the API key that you get

3066
02:19:53,220 --> 02:19:59,040
you can use the free headers generator

3067
02:19:57,479 --> 02:20:01,920
API

3068
02:19:59,040 --> 02:20:06,240
so this is what happens when I use their

3069
02:20:01,920 --> 02:20:10,560
headers API I can specify that I want

3070
02:20:06,240 --> 02:20:13,620
user agents I put in my API key

3071
02:20:10,560 --> 02:20:17,580
there make a request and it gives me

3072
02:20:13,620 --> 02:20:22,380
back a result with a bunch of different

3073
02:20:17,580 --> 02:20:24,960
randomly generated user agents so once

3074
02:20:22,380 --> 02:20:27,180
you're actually logged in

3075
02:20:24,960 --> 02:20:29,580
you can go to their

3076
02:20:27,180 --> 02:20:33,720
fake headers API section and that's

3077
02:20:29,580 --> 02:20:36,479
where it shows you your API key and if

3078
02:20:33,720 --> 02:20:40,439
you want to generate browser headers

3079
02:20:36,479 --> 02:20:41,939
you use this URL if you want to generate

3080
02:20:40,439 --> 02:20:46,800
user agents

3081
02:20:41,939 --> 02:20:49,920
you use that URL and then the response

3082
02:20:46,800 --> 02:20:50,760
comes back like this just as I showed

3083
02:20:49,920 --> 02:20:54,060
you here

3084
02:20:50,760 --> 02:20:56,760
so I can just

3085
02:20:54,060 --> 02:20:59,220
stick that in and it gives me the

3086
02:20:56,760 --> 02:21:02,580
results back so you can specify the

3087
02:20:59,220 --> 02:21:05,399
number of headers you want and it sends

3088
02:21:02,580 --> 02:21:07,260
you back all the user agents our browser

3089
02:21:05,399 --> 02:21:08,640
headers that we can then use in our

3090
02:21:07,260 --> 02:21:10,620
middleware

3091
02:21:08,640 --> 02:21:13,200
so depending on the language you're

3092
02:21:10,620 --> 02:21:14,939
using as well you can specify different

3093
02:21:13,200 --> 02:21:18,840
examples so

3094
02:21:14,939 --> 02:21:20,640
this is where I got the URL to use

3095
02:21:18,840 --> 02:21:24,660
for here

3096
02:21:20,640 --> 02:21:28,200
and if you're using node or PHP a ruby

3097
02:21:24,660 --> 02:21:30,899
you could use the other tabs to see the

3098
02:21:28,200 --> 02:21:33,420
examples but we're using python with

3099
02:21:30,899 --> 02:21:35,160
scraping so we have everything we need

3100
02:21:33,420 --> 02:21:37,859
here

3101
02:21:35,160 --> 02:21:41,580
so now that we have an API endpoint

3102
02:21:37,859 --> 02:21:43,979
where we can get our fake user agents

3103
02:21:41,580 --> 02:21:46,680
and fake headers

3104
02:21:43,979 --> 02:21:49,439
we can go back to our middlewares file

3105
02:21:46,680 --> 02:21:51,660
and we can start creating our middleware

3106
02:21:49,439 --> 02:21:55,620
that will handle everything to do with

3107
02:21:51,660 --> 02:21:57,420
the fake user agents so I'm just going

3108
02:21:55,620 --> 02:21:58,500
to scroll down to the bottom and I'm

3109
02:21:57,420 --> 02:22:00,540
going to start

3110
02:21:58,500 --> 02:22:02,640
a new class

3111
02:22:00,540 --> 02:22:04,439
and I'm going to be importing a couple

3112
02:22:02,640 --> 02:22:06,660
of things that we're going to need

3113
02:22:04,439 --> 02:22:09,300
so I'm going to import your link code

3114
02:22:06,660 --> 02:22:13,680
which will encode our urls

3115
02:22:09,300 --> 02:22:15,960
ran into pick a random integer so we can

3116
02:22:13,680 --> 02:22:17,460
use that to pick one from the list and

3117
02:22:15,960 --> 02:22:19,500
requests as well

3118
02:22:17,460 --> 02:22:21,780
so I've created a new class called

3119
02:22:19,500 --> 02:22:24,000
scrape UPS fake user agent middleware

3120
02:22:21,780 --> 02:22:25,740
that can be obviously whatever you want

3121
02:22:24,000 --> 02:22:29,460
it to be

3122
02:22:25,740 --> 02:22:32,760
and then we're going to set it up

3123
02:22:29,460 --> 02:22:35,880
so again we have our initial

3124
02:22:32,760 --> 02:22:38,040
function which gets kicked off when the

3125
02:22:35,880 --> 02:22:42,000
class is initialized

3126
02:22:38,040 --> 02:22:43,500
and in here we first off set some of our

3127
02:22:42,000 --> 02:22:45,780
settings

3128
02:22:43,500 --> 02:22:49,200
so we're going to want the scrape UPS

3129
02:22:45,780 --> 02:22:54,060
API key which is going to be the API key

3130
02:22:49,200 --> 02:22:56,220
that we get for free from

3131
02:22:54,060 --> 02:22:59,520
here so that's where we have our API key

3132
02:22:56,220 --> 02:23:02,160
we also have the URL

3133
02:22:59,520 --> 02:23:03,899
which you can see

3134
02:23:02,160 --> 02:23:05,580
here

3135
02:23:03,899 --> 02:23:07,640
and

3136
02:23:05,580 --> 02:23:10,439
so we've got our endpoint

3137
02:23:07,640 --> 02:23:12,600
and what else oh yeah do we want it to

3138
02:23:10,439 --> 02:23:16,319
be enabled or not and the number of

3139
02:23:12,600 --> 02:23:17,460
results we want to specify to come back

3140
02:23:16,319 --> 02:23:20,760
so

3141
02:23:17,460 --> 02:23:22,020
we go ahead and set those all up in our

3142
02:23:20,760 --> 02:23:26,100
settings

3143
02:23:22,020 --> 02:23:28,740
so I'm going to set my

3144
02:23:26,100 --> 02:23:30,720
API key

3145
02:23:28,740 --> 02:23:33,359
obviously you guys set that to whatever

3146
02:23:30,720 --> 02:23:36,680
your one is

3147
02:23:33,359 --> 02:23:36,680
set up the endpoint

3148
02:23:37,319 --> 02:23:43,520
and the end point is going to be user

3149
02:23:40,979 --> 02:23:43,520
agents

3150
02:23:44,520 --> 02:23:47,600
paste static

3151
02:23:47,700 --> 02:23:53,280
and then we have our

3152
02:23:50,399 --> 02:23:55,560
if we have enabled

3153
02:23:53,280 --> 02:23:57,540
so that's true

3154
02:23:55,560 --> 02:23:59,640
there you go

3155
02:23:57,540 --> 02:24:02,399
and

3156
02:23:59,640 --> 02:24:06,660
the num requests

3157
02:24:02,399 --> 02:24:09,720
which we can set to to 50.

3158
02:24:06,660 --> 02:24:13,920
you can save that and

3159
02:24:09,720 --> 02:24:16,319
save that as well so this part up here

3160
02:24:13,920 --> 02:24:19,380
just makes sure that we have access to

3161
02:24:16,319 --> 02:24:21,300
our crawler settings when d-class is

3162
02:24:19,380 --> 02:24:24,600
initialized

3163
02:24:21,300 --> 02:24:26,640
so as you can see here and here we've

3164
02:24:24,600 --> 02:24:28,800
got two functions

3165
02:24:26,640 --> 02:24:32,280
and that means when the class is

3166
02:24:28,800 --> 02:24:34,740
initialized get the user agents list and

3167
02:24:32,280 --> 02:24:37,319
enable it

3168
02:24:34,740 --> 02:24:41,399
so I'm going to first off get the user

3169
02:24:37,319 --> 02:24:43,800
agents list so I'm gonna add that one in

3170
02:24:41,399 --> 02:24:46,460
so that function looks like

3171
02:24:43,800 --> 02:24:46,460
following

3172
02:24:46,560 --> 02:24:52,260
we've got the payload set which is the

3173
02:24:50,399 --> 02:24:55,500
API key

3174
02:24:52,260 --> 02:24:59,100
and then we're saying if the number of

3175
02:24:55,500 --> 02:25:01,319
results is not none set the payload

3176
02:24:59,100 --> 02:25:04,080
number results

3177
02:25:01,319 --> 02:25:09,060
and then we want to make a get request

3178
02:25:04,080 --> 02:25:12,060
to the API endpoint with our

3179
02:25:09,060 --> 02:25:15,600
parameters which have been URL encoded

3180
02:25:12,060 --> 02:25:18,600
here using the URL encode function then

3181
02:25:15,600 --> 02:25:22,020
that goes off goes to the script ops

3182
02:25:18,600 --> 02:25:24,720
endpoint and then gets the user agents

3183
02:25:22,020 --> 02:25:26,640
and comes what comes back gets put into

3184
02:25:24,720 --> 02:25:30,600
the response

3185
02:25:26,640 --> 02:25:32,880
then we're using dot Json just to parse

3186
02:25:30,600 --> 02:25:35,280
it into a Json response

3187
02:25:32,880 --> 02:25:38,880
and then we can

3188
02:25:35,280 --> 02:25:41,899
have our user agents list saved into

3189
02:25:38,880 --> 02:25:41,899
user agents list

3190
02:25:42,240 --> 02:25:45,960
so once we've got that we're going to

3191
02:25:44,880 --> 02:25:48,479
just

3192
02:25:45,960 --> 02:25:50,160
create two more simple functions

3193
02:25:48,479 --> 02:25:54,479
underneath

3194
02:25:50,160 --> 02:25:56,880
the first one just get random user agent

3195
02:25:54,479 --> 02:25:59,340
fairly self-explanatory

3196
02:25:56,880 --> 02:26:02,040
let's get one and getting one user agent

3197
02:25:59,340 --> 02:26:06,479
from the list that's been returned and

3198
02:26:02,040 --> 02:26:08,220
returned that selected user agent

3199
02:26:06,479 --> 02:26:11,939
and then we have

3200
02:26:08,220 --> 02:26:15,319
just this check to see if the fake user

3201
02:26:11,939 --> 02:26:18,540
agents is either active or inactive

3202
02:26:15,319 --> 02:26:19,620
and then once all that's set we can

3203
02:26:18,540 --> 02:26:23,819
actually

3204
02:26:19,620 --> 02:26:27,060
put that into practice with our process

3205
02:26:23,819 --> 02:26:29,340
requests which is one of the scrapey

3206
02:26:27,060 --> 02:26:30,720
functions that's it's one of the

3207
02:26:29,340 --> 02:26:32,640
functions that scrapey will look for

3208
02:26:30,720 --> 02:26:33,979
when you're using middlewares

3209
02:26:32,640 --> 02:26:36,780
and

3210
02:26:33,979 --> 02:26:39,780
it then sees that we've specified

3211
02:26:36,780 --> 02:26:42,180
something to happen when it goes to

3212
02:26:39,780 --> 02:26:43,740
processor request and then it executes

3213
02:26:42,180 --> 02:26:46,620
the following

3214
02:26:43,740 --> 02:26:49,680
so when it sees process request it goes

3215
02:26:46,620 --> 02:26:53,580
in here it gets a random user agent

3216
02:26:49,680 --> 02:26:57,000
sticks it in here and then it sets the

3217
02:26:53,580 --> 02:27:00,359
header user agent to be the random user

3218
02:26:57,000 --> 02:27:02,880
agent so I hope that makes sense I think

3219
02:27:00,359 --> 02:27:05,280
it's fairly self-explanatory we're just

3220
02:27:02,880 --> 02:27:08,100
getting a list of user agents

3221
02:27:05,280 --> 02:27:11,520
and then with process request we are

3222
02:27:08,100 --> 02:27:14,280
getting a random user agent and we are

3223
02:27:11,520 --> 02:27:17,880
assigning that to our header request

3224
02:27:14,280 --> 02:27:20,520
header so when we go off and ask our

3225
02:27:17,880 --> 02:27:24,660
books to scrape site

3226
02:27:20,520 --> 02:27:27,540
for the book or the list of books it

3227
02:27:24,660 --> 02:27:32,520
sticks in dash random user agent

3228
02:27:27,540 --> 02:27:35,580
into the user agent of the request

3229
02:27:32,520 --> 02:27:38,340
the only thing to do then is to stick

3230
02:27:35,580 --> 02:27:40,080
our middleware this is a step you

3231
02:27:38,340 --> 02:27:42,600
mustn't forget it's very easy to forget

3232
02:27:40,080 --> 02:27:44,939
it I've forgotten it loads of times and

3233
02:27:42,600 --> 02:27:46,680
then sometimes you can be trying to

3234
02:27:44,939 --> 02:27:49,319
debug things after

3235
02:27:46,680 --> 02:27:50,580
so you want to go to your downloader

3236
02:27:49,319 --> 02:27:52,020
middlewares

3237
02:27:50,580 --> 02:27:57,500
open that

3238
02:27:52,020 --> 02:27:57,500
and add in your new middleware

3239
02:27:58,439 --> 02:28:04,380
so again as we did in the last one the

3240
02:28:01,800 --> 02:28:07,500
lower number has higher priority

3241
02:28:04,380 --> 02:28:09,840
so we don't need to have

3242
02:28:07,500 --> 02:28:11,340
the other downloader middleware just

3243
02:28:09,840 --> 02:28:15,000
want our

3244
02:28:11,340 --> 02:28:17,520
a middleware we just created our scrape

3245
02:28:15,000 --> 02:28:19,800
Ops fake user agent middleware

3246
02:28:17,520 --> 02:28:23,580
because this middleware up here is just

3247
02:28:19,800 --> 02:28:25,080
the generated one that we don't need at

3248
02:28:23,580 --> 02:28:28,859
the moment

3249
02:28:25,080 --> 02:28:31,080
okay we'll go ahead run our spider now

3250
02:28:28,859 --> 02:28:33,060
check that it's working

3251
02:28:31,080 --> 02:28:36,300
should be working fine

3252
02:28:33,060 --> 02:28:39,060
and then we will look at

3253
02:28:36,300 --> 02:28:40,740
just doing the same thing but instead of

3254
02:28:39,060 --> 02:28:44,760
having

3255
02:28:40,740 --> 02:28:47,340
a user agent come back from scrape-ups

3256
02:28:44,760 --> 02:28:49,260
We'll be asking for a list of fake

3257
02:28:47,340 --> 02:28:50,460
headers so we'll create a second

3258
02:28:49,260 --> 02:28:53,100
middleware

3259
02:28:50,460 --> 02:28:55,560
and we'll just have the whole fake

3260
02:28:53,100 --> 02:28:57,479
header as opposed to just the subsection

3261
02:28:55,560 --> 02:29:00,300
user agent part

3262
02:28:57,479 --> 02:29:02,340
so just to make sure that the headers

3263
02:29:00,300 --> 02:29:04,439
are being attached correctly because you

3264
02:29:02,340 --> 02:29:07,979
might say oh I know for sure that it's

3265
02:29:04,439 --> 02:29:11,040
working we can just add in a very simple

3266
02:29:07,979 --> 02:29:12,420
print to our process request

3267
02:29:11,040 --> 02:29:14,760
so

3268
02:29:12,420 --> 02:29:16,740
I'm just going to stick this in here two

3269
02:29:14,760 --> 02:29:19,620
print statements one sync new header

3270
02:29:16,740 --> 02:29:22,319
attached and the other one saying the

3271
02:29:19,620 --> 02:29:24,540
new user agent so it'll print out the

3272
02:29:22,319 --> 02:29:27,720
user agent that we've just attached the

3273
02:29:24,540 --> 02:29:29,819
random user agent so that should print

3274
02:29:27,720 --> 02:29:32,100
that out then to our console and we

3275
02:29:29,819 --> 02:29:34,200
should be able to see that a new user

3276
02:29:32,100 --> 02:29:36,899
agent is being attached every time that

3277
02:29:34,200 --> 02:29:38,100
request is being processed you out the

3278
02:29:36,899 --> 02:29:41,100
only other thing we have to do is go

3279
02:29:38,100 --> 02:29:43,740
back and remove what we added in earlier

3280
02:29:41,100 --> 02:29:46,260
which was this header's part here

3281
02:29:43,740 --> 02:29:48,960
because it's being done in middleware so

3282
02:29:46,260 --> 02:29:51,479
we don't have to specify it for every

3283
02:29:48,960 --> 02:29:53,880
request here so I'm just going to remove

3284
02:29:51,479 --> 02:29:56,700
that

3285
02:29:53,880 --> 02:29:58,080
and I'm going to remove it here as well

3286
02:29:56,700 --> 02:30:00,540
so as you can see there's always

3287
02:29:58,080 --> 02:30:02,060
multiple ways of doing it either kind of

3288
02:30:00,540 --> 02:30:05,220
manually adding things into the spider

3289
02:30:02,060 --> 02:30:07,500
setting them in the settings if you just

3290
02:30:05,220 --> 02:30:09,420
want it once off in the settings if you

3291
02:30:07,500 --> 02:30:10,979
want a simple case The Spider and if you

3292
02:30:09,420 --> 02:30:12,899
want something more complex in the

3293
02:30:10,979 --> 02:30:15,120
middleware so you've kind of got your

3294
02:30:12,899 --> 02:30:19,080
three different ways of adding

3295
02:30:15,120 --> 02:30:22,680
your user agents are headers in

3296
02:30:19,080 --> 02:30:25,260
so now that we've removed that we don't

3297
02:30:22,680 --> 02:30:28,500
need this user agents list up here

3298
02:30:25,260 --> 02:30:29,939
we can remove that let's save it

3299
02:30:28,500 --> 02:30:34,080
and

3300
02:30:29,939 --> 02:30:35,819
we should be able to just run our spider

3301
02:30:34,080 --> 02:30:38,040
again so

3302
02:30:35,819 --> 02:30:39,899
Scrappy crawl

3303
02:30:38,040 --> 02:30:42,560
and

3304
02:30:39,899 --> 02:30:42,560
book Spider

3305
02:30:42,720 --> 02:30:48,600
and if everything's gone to plan we

3306
02:30:45,720 --> 02:30:49,979
should see I'll just close the spider

3307
02:30:48,600 --> 02:30:52,140
straight away

3308
02:30:49,979 --> 02:30:53,760
and scroll up so we can have a look and

3309
02:30:52,140 --> 02:30:56,580
see to everything so it looks like all

3310
02:30:53,760 --> 02:30:58,439
the data is coming back as it was before

3311
02:30:56,580 --> 02:31:00,600
we don't expect anything to change there

3312
02:30:58,439 --> 02:31:04,439
because we we're not going to be getting

3313
02:31:00,600 --> 02:31:07,500
blocked by the books to scrape.com site

3314
02:31:04,439 --> 02:31:09,600
anyway but if we scroll up I think we

3315
02:31:07,500 --> 02:31:12,300
should see

3316
02:31:09,600 --> 02:31:15,420
here you go so here is where we can see

3317
02:31:12,300 --> 02:31:17,520
the all the new headers that are coming

3318
02:31:15,420 --> 02:31:19,260
in

3319
02:31:17,520 --> 02:31:22,740
so

3320
02:31:19,260 --> 02:31:24,420
you can see the Chrome version there is

3321
02:31:22,740 --> 02:31:26,460
different

3322
02:31:24,420 --> 02:31:28,620
sometimes it looks like it's using Edge

3323
02:31:26,460 --> 02:31:31,140
as well

3324
02:31:28,620 --> 02:31:33,780
so you can see there's multiple

3325
02:31:31,140 --> 02:31:37,680
different headers coming back

3326
02:31:33,780 --> 02:31:39,660
and then they are being attached in with

3327
02:31:37,680 --> 02:31:42,240
the process request

3328
02:31:39,660 --> 02:31:45,660
so that all seems to be working fine

3329
02:31:42,240 --> 02:31:48,720
just as we wanted to and we made sure

3330
02:31:45,660 --> 02:31:50,220
that we had it enabled in the settings

3331
02:31:48,720 --> 02:31:52,500
as well

3332
02:31:50,220 --> 02:31:54,479
in the downloader middlewares so we have

3333
02:31:52,500 --> 02:31:57,899
an enabled there

3334
02:31:54,479 --> 02:31:59,100
as well as enabled up here and the

3335
02:31:57,899 --> 02:32:01,439
number of

3336
02:31:59,100 --> 02:32:02,760
user agents coming back from the API set

3337
02:32:01,439 --> 02:32:05,460
here

3338
02:32:02,760 --> 02:32:07,620
so everything is

3339
02:32:05,460 --> 02:32:12,000
as it should be one other thing to note

3340
02:32:07,620 --> 02:32:14,580
is that this robot's txt underscore a

3341
02:32:12,000 --> 02:32:17,160
bay is set to true if we're starting to

3342
02:32:14,580 --> 02:32:18,780
do more complex sites we would set this

3343
02:32:17,160 --> 02:32:23,060
to false

3344
02:32:18,780 --> 02:32:26,160
so every site or most sites have a

3345
02:32:23,060 --> 02:32:28,500
robots.txt file

3346
02:32:26,160 --> 02:32:31,380
which is usually one of the first things

3347
02:32:28,500 --> 02:32:33,780
that a spider will look for so scrapey

3348
02:32:31,380 --> 02:32:37,020
does this automatically every time it

3349
02:32:33,780 --> 02:32:39,120
looks at a site it first goes off and

3350
02:32:37,020 --> 02:32:43,020
checks does the site have a robot dot

3351
02:32:39,120 --> 02:32:46,140
txt file and in that robots.txt file is

3352
02:32:43,020 --> 02:32:49,680
usually specified things like the pages

3353
02:32:46,140 --> 02:32:52,260
on the site is this site open to being

3354
02:32:49,680 --> 02:32:55,200
scraped and if it's open to being

3355
02:32:52,260 --> 02:32:57,120
scraped watch pages are not allowed to

3356
02:32:55,200 --> 02:33:00,359
be scraped are what pages are allowed to

3357
02:32:57,120 --> 02:33:02,580
be scraped now obviously any crawlers

3358
02:33:00,359 --> 02:33:06,840
that go out crawl different websites

3359
02:33:02,580 --> 02:33:09,120
don't have to obey this robots.txt file

3360
02:33:06,840 --> 02:33:10,500
you know it's it's a piece of code it's

3361
02:33:09,120 --> 02:33:13,260
going to go off and do what you tell us

3362
02:33:10,500 --> 02:33:17,100
this is up to you to decide you know do

3363
02:33:13,260 --> 02:33:21,479
you want your spider to obey it or not a

3364
02:33:17,100 --> 02:33:23,760
lot of big sites will have like okay if

3365
02:33:21,479 --> 02:33:26,880
you're a Google spider you're allowed

3366
02:33:23,760 --> 02:33:30,300
crawl and scrape our data if you're not

3367
02:33:26,880 --> 02:33:32,280
a Google spider don't scrape our data so

3368
02:33:30,300 --> 02:33:35,340
if they have that in their robots.txt

3369
02:33:32,280 --> 02:33:38,520
and you have it set to obey is equal to

3370
02:33:35,340 --> 02:33:40,260
True your spider will go see it's not

3371
02:33:38,520 --> 02:33:43,200
supposed to scrape the site and it'll

3372
02:33:40,260 --> 02:33:45,479
shut down so if you're having issues and

3373
02:33:43,200 --> 02:33:46,439
you have this set to True try setting it

3374
02:33:45,479 --> 02:33:49,520
to false

3375
02:33:46,439 --> 02:33:53,340
okay so now that we've gone through what

3376
02:33:49,520 --> 02:33:56,100
robots.txt entails let's go and create

3377
02:33:53,340 --> 02:33:58,260
our next middleware

3378
02:33:56,100 --> 02:34:00,960
so this time instead of

3379
02:33:58,260 --> 02:34:03,359
having middleware that just replaces the

3380
02:34:00,960 --> 02:34:05,819
user agent every time we're going to

3381
02:34:03,359 --> 02:34:07,979
create a new middleware so go to

3382
02:34:05,819 --> 02:34:11,880
middlewares.py

3383
02:34:07,979 --> 02:34:16,979
and this new middleware is going to

3384
02:34:11,880 --> 02:34:20,040
create a new header every time using the

3385
02:34:16,979 --> 02:34:23,280
data that it gets back from the fake

3386
02:34:20,040 --> 02:34:25,200
browser header endpoint

3387
02:34:23,280 --> 02:34:26,700
so I'm just going to paste in the code

3388
02:34:25,200 --> 02:34:28,819
here and then talk through it for you

3389
02:34:26,700 --> 02:34:28,819
guys

3390
02:34:28,920 --> 02:34:31,460
so

3391
02:34:31,800 --> 02:34:36,420
if we go up to the top we start off

3392
02:34:33,840 --> 02:34:38,460
again with just a simple class name

3393
02:34:36,420 --> 02:34:42,060
scrape UPS fake browser header agent

3394
02:34:38,460 --> 02:34:46,140
middleware that can be whatever you want

3395
02:34:42,060 --> 02:34:48,240
we pull in the settings that we need

3396
02:34:46,140 --> 02:34:49,800
this will get everything from the

3397
02:34:48,240 --> 02:34:52,979
settings file

3398
02:34:49,800 --> 02:34:56,100
and then it kicks off get headers list

3399
02:34:52,979 --> 02:34:58,500
which calls out to the API endpoint

3400
02:34:56,100 --> 02:35:01,020
it does a get request here

3401
02:34:58,500 --> 02:35:04,040
to the scrape UPS endpoint

3402
02:35:01,020 --> 02:35:09,240
Returns the response converts it to Json

3403
02:35:04,040 --> 02:35:12,359
and then we've got a list of headers

3404
02:35:09,240 --> 02:35:14,580
and then in the process request which

3405
02:35:12,359 --> 02:35:16,859
gets processed with every request

3406
02:35:14,580 --> 02:35:21,180
we have this get random browser header

3407
02:35:16,859 --> 02:35:23,100
function which will get the random

3408
02:35:21,180 --> 02:35:26,760
browser header from the list that we

3409
02:35:23,100 --> 02:35:29,220
just asked for from scrape UPS so then

3410
02:35:26,760 --> 02:35:30,540
we have this random browser header and

3411
02:35:29,220 --> 02:35:33,180
we can assign

3412
02:35:30,540 --> 02:35:34,920
all the other headers not just user

3413
02:35:33,180 --> 02:35:37,859
agent like we did in the middleware

3414
02:35:34,920 --> 02:35:40,439
above but we can also modify all these

3415
02:35:37,859 --> 02:35:42,960
other parameters in the header

3416
02:35:40,439 --> 02:35:45,540
so you don't have to modify all of them

3417
02:35:42,960 --> 02:35:46,920
you can modify certain ones again this

3418
02:35:45,540 --> 02:35:48,359
is up to you to play around with and

3419
02:35:46,920 --> 02:35:49,620
decide which ones you need which ones

3420
02:35:48,359 --> 02:35:51,660
you don't need

3421
02:35:49,620 --> 02:35:53,640
with scraping everything is really a

3422
02:35:51,660 --> 02:35:55,260
case-by-case basis because every website

3423
02:35:53,640 --> 02:35:57,780
is different

3424
02:35:55,260 --> 02:35:59,880
but we're giving you here everything you

3425
02:35:57,780 --> 02:36:02,580
need to play around with

3426
02:35:59,880 --> 02:36:05,760
so you might find certain headers need

3427
02:36:02,580 --> 02:36:09,060
to be modified more than others we have

3428
02:36:05,760 --> 02:36:10,979
our request.headers

3429
02:36:09,060 --> 02:36:16,020
being updated

3430
02:36:10,979 --> 02:36:17,520
and then that is just like the one above

3431
02:36:16,020 --> 02:36:20,340
everything we need

3432
02:36:17,520 --> 02:36:22,140
let's go ahead now and add the settings

3433
02:36:20,340 --> 02:36:25,560
that we need

3434
02:36:22,140 --> 02:36:29,340
so we have some of the settings already

3435
02:36:25,560 --> 02:36:31,620
set here so we have the browser endpoint

3436
02:36:29,340 --> 02:36:34,859
there's a default set there we can set

3437
02:36:31,620 --> 02:36:38,700
our fake browser header enabled to True

3438
02:36:34,859 --> 02:36:42,600
here to make sure it runs and then the

3439
02:36:38,700 --> 02:36:46,380
num requests is going to be the same as

3440
02:36:42,600 --> 02:36:48,780
here it's already set and the API key is

3441
02:36:46,380 --> 02:36:51,859
already set as well so again you get

3442
02:36:48,780 --> 02:36:55,380
your own API key for that

3443
02:36:51,859 --> 02:36:58,200
and I think we should be able to just go

3444
02:36:55,380 --> 02:37:00,720
to settings now and make sure we have

3445
02:36:58,200 --> 02:37:02,580
the middleware enabled so I'm copying

3446
02:37:00,720 --> 02:37:05,640
the class name

3447
02:37:02,580 --> 02:37:08,939
going to settings going down to

3448
02:37:05,640 --> 02:37:12,060
downloader middlewares and where we had

3449
02:37:08,939 --> 02:37:14,160
the fake user agent middleware I'm just

3450
02:37:12,060 --> 02:37:16,740
going to overwrite that so we have our

3451
02:37:14,160 --> 02:37:20,399
fake browser header agent middleware

3452
02:37:16,740 --> 02:37:24,060
so I'm going to save that and then just

3453
02:37:20,399 --> 02:37:28,979
go and run the spider again

3454
02:37:24,060 --> 02:37:31,859
and it should work correctly

3455
02:37:28,979 --> 02:37:34,140
so yeah there seems to be

3456
02:37:31,859 --> 02:37:36,899
lots of books getting scraped

3457
02:37:34,140 --> 02:37:40,979
so it seems to be working correctly I

3458
02:37:36,899 --> 02:37:43,680
can stop it and just as we did before

3459
02:37:40,979 --> 02:37:46,260
to double check that the

3460
02:37:43,680 --> 02:37:48,359
headers are being

3461
02:37:46,260 --> 02:37:50,580
set correctly

3462
02:37:48,359 --> 02:37:53,040
we can just stick underneath

3463
02:37:50,580 --> 02:37:56,700
a simple print statement

3464
02:37:53,040 --> 02:38:00,479
that shows the headers are set

3465
02:37:56,700 --> 02:38:02,120
to what we wanted them to be set to

3466
02:38:00,479 --> 02:38:06,600
so

3467
02:38:02,120 --> 02:38:09,920
if we run our spider it should show that

3468
02:38:06,600 --> 02:38:09,920
there's multiple different headers

3469
02:38:10,200 --> 02:38:14,520
so I'm stopping it again and looking at

3470
02:38:12,720 --> 02:38:19,040
the output

3471
02:38:14,520 --> 02:38:19,040
once we get past the book data

3472
02:38:19,080 --> 02:38:23,760
okay so here there's some headers so new

3473
02:38:22,200 --> 02:38:25,560
header attached which is what we have

3474
02:38:23,760 --> 02:38:29,880
here and then

3475
02:38:25,560 --> 02:38:32,040
we can see we have things like let's see

3476
02:38:29,880 --> 02:38:36,800
accept

3477
02:38:32,040 --> 02:38:41,100
the user agent so okay so we have

3478
02:38:36,800 --> 02:38:43,680
the user agent Mozilla 5

3479
02:38:41,100 --> 02:38:48,420
do we have accept

3480
02:38:43,680 --> 02:38:51,780
so we have accept the text HTML

3481
02:38:48,420 --> 02:38:54,120
everything we wanted there

3482
02:38:51,780 --> 02:38:56,399
I'm just trying to see can we see in

3483
02:38:54,120 --> 02:38:57,899
here that both of them are different to

3484
02:38:56,399 --> 02:39:00,840
each other

3485
02:38:57,899 --> 02:39:05,580
yeah so here for example you can see

3486
02:39:00,840 --> 02:39:08,939
this user agent is using Chrome 103.0

3487
02:39:05,580 --> 02:39:11,700
50 60.134

3488
02:39:08,939 --> 02:39:13,620
and up here it's using a different

3489
02:39:11,700 --> 02:39:16,560
version of Chrome

3490
02:39:13,620 --> 02:39:20,340
so it's using one zero three point zero

3491
02:39:16,560 --> 02:39:23,040
point fifty sixty one one four

3492
02:39:20,340 --> 02:39:26,880
so you can see it is

3493
02:39:23,040 --> 02:39:29,520
changing in each request

3494
02:39:26,880 --> 02:39:31,319
so that's exactly what we want to show

3495
02:39:29,520 --> 02:39:34,680
you how to do

3496
02:39:31,319 --> 02:39:37,500
so I think that concludes part eight

3497
02:39:34,680 --> 02:39:39,300
in part nine we'll be going into how to

3498
02:39:37,500 --> 02:39:43,020
use proxies

3499
02:39:39,300 --> 02:39:45,240
to bypass the anti-bot blockers that

3500
02:39:43,020 --> 02:39:47,460
websites have as well so instead of kind

3501
02:39:45,240 --> 02:39:52,560
of handling everything ourselves and

3502
02:39:47,460 --> 02:39:54,840
trying to bypass the antibots by

3503
02:39:52,560 --> 02:39:57,180
updating our own headers we can see that

3504
02:39:54,840 --> 02:39:59,819
there's commercial things out there for

3505
02:39:57,180 --> 02:40:03,720
you to do that with and there's also

3506
02:39:59,819 --> 02:40:05,460
things like proxy lists that are free to

3507
02:40:03,720 --> 02:40:07,500
use as well and we look at how to

3508
02:40:05,460 --> 02:40:09,479
integrate those into your scrapey

3509
02:40:07,500 --> 02:40:13,460
project as well

3510
02:40:09,479 --> 02:40:13,460
so see you in part nine guys

3511
02:40:16,020 --> 02:40:19,979
so in part nine of the scrapey beginners

3512
02:40:18,540 --> 02:40:22,800
course we're going to be looking at

3513
02:40:19,979 --> 02:40:24,420
everything to do with proxies so we're

3514
02:40:22,800 --> 02:40:26,580
going to be going through what are

3515
02:40:24,420 --> 02:40:28,560
proxies and why do we need them and then

3516
02:40:26,580 --> 02:40:30,720
we're going to be looking at the three

3517
02:40:28,560 --> 02:40:33,780
most popular ways to integrate proxies

3518
02:40:30,720 --> 02:40:35,939
into your projects

3519
02:40:33,780 --> 02:40:38,340
so let's get started

3520
02:40:35,939 --> 02:40:41,939
so in part 8 we're looking at user

3521
02:40:38,340 --> 02:40:44,580
agents and requests and the headers we

3522
02:40:41,939 --> 02:40:46,680
pass in when we're making the requests

3523
02:40:44,580 --> 02:40:49,740
to the website you're looking to scrape

3524
02:40:46,680 --> 02:40:53,580
we discussed and looked at how if you

3525
02:40:49,740 --> 02:40:56,880
change your headers and change your user

3526
02:40:53,580 --> 02:40:59,700
agents you can basically make it look as

3527
02:40:56,880 --> 02:41:01,800
if you were multiple people accessing

3528
02:40:59,700 --> 02:41:04,319
the website you're trying to scrape

3529
02:41:01,800 --> 02:41:06,840
now there's one thing we also mentioned

3530
02:41:04,319 --> 02:41:09,180
in part eight which is that the data

3531
02:41:06,840 --> 02:41:11,760
that also is being transmitted with your

3532
02:41:09,180 --> 02:41:15,240
request is usually your IP address

3533
02:41:11,760 --> 02:41:16,560
so this IP address is it's your unique

3534
02:41:15,240 --> 02:41:19,740
identifier

3535
02:41:16,560 --> 02:41:22,920
and that's what is used to make sure the

3536
02:41:19,740 --> 02:41:24,960
data comes back to your machine

3537
02:41:22,920 --> 02:41:28,680
so every machine will have an IP address

3538
02:41:24,960 --> 02:41:30,660
and that's how the requests get to and

3539
02:41:28,680 --> 02:41:32,460
from your machine think of it as like

3540
02:41:30,660 --> 02:41:34,560
your house as an address your computer

3541
02:41:32,460 --> 02:41:39,000
must also need an address and this is

3542
02:41:34,560 --> 02:41:41,040
your IP address so if we change the user

3543
02:41:39,000 --> 02:41:43,500
agents every time when we're sending the

3544
02:41:41,040 --> 02:41:45,359
requests that's fine but if we changing

3545
02:41:43,500 --> 02:41:47,880
the user agents every time but we still

3546
02:41:45,359 --> 02:41:50,580
have the same IP address then the site

3547
02:41:47,880 --> 02:41:54,240
that we're scraping is very likely to

3548
02:41:50,580 --> 02:41:56,399
know that we are the same machine that

3549
02:41:54,240 --> 02:41:57,780
is requesting their data every time so

3550
02:41:56,399 --> 02:41:58,620
they're very likely to block us straight

3551
02:41:57,780 --> 02:42:03,120
away

3552
02:41:58,620 --> 02:42:06,899
so that's why changing our IP address as

3553
02:42:03,120 --> 02:42:09,899
well as our user agent and headers is

3554
02:42:06,899 --> 02:42:12,300
very important so just the user agent

3555
02:42:09,899 --> 02:42:14,640
and headers might work if it's not very

3556
02:42:12,300 --> 02:42:15,780
sophisticated type of website that

3557
02:42:14,640 --> 02:42:18,540
you're trying to scrape but if you're

3558
02:42:15,780 --> 02:42:21,500
going to anything that's complex at all

3559
02:42:18,540 --> 02:42:26,880
you will need to rotate your IP address

3560
02:42:21,500 --> 02:42:28,220
and that's where proxies come into play

3561
02:42:26,880 --> 02:42:31,560
so

3562
02:42:28,220 --> 02:42:34,260
let's first off look at the first method

3563
02:42:31,560 --> 02:42:38,939
is that we're going to be looking at is

3564
02:42:34,260 --> 02:42:42,479
using proxy lists so these are lists of

3565
02:42:38,939 --> 02:42:45,180
IP addresses and ports that belong to

3566
02:42:42,479 --> 02:42:48,060
multiple different servers all over the

3567
02:42:45,180 --> 02:42:51,540
world so there's lots of these machines

3568
02:42:48,060 --> 02:42:54,300
that are available to bypass our

3569
02:42:51,540 --> 02:42:57,840
requests will go via that machine before

3570
02:42:54,300 --> 02:43:00,120
it goes to the website we're trying to

3571
02:42:57,840 --> 02:43:02,819
scrape and then it'll come all the way

3572
02:43:00,120 --> 02:43:05,399
back via that machine as well now

3573
02:43:02,819 --> 02:43:07,740
there's pros and cons to every one of

3574
02:43:05,399 --> 02:43:09,780
the of the three integration methods

3575
02:43:07,740 --> 02:43:12,260
we're going to look at so the pros of

3576
02:43:09,780 --> 02:43:15,300
proxy lists like this are that obviously

3577
02:43:12,260 --> 02:43:17,939
the proxies that list that you can get

3578
02:43:15,300 --> 02:43:18,920
online are last number free so like this

3579
02:43:17,939 --> 02:43:21,479
one here

3580
02:43:18,920 --> 02:43:23,819
freeproxylist.net you can go there and

3581
02:43:21,479 --> 02:43:27,300
you can select from a list of different

3582
02:43:23,819 --> 02:43:30,600
countries select your protocols and you

3583
02:43:27,300 --> 02:43:33,359
can check the uptime there's also

3584
02:43:30,600 --> 02:43:37,080
another list which is very handy on

3585
02:43:33,359 --> 02:43:40,439
juno.com forward slash free proxy list

3586
02:43:37,080 --> 02:43:44,220
here you also have IP address Port

3587
02:43:40,439 --> 02:43:49,200
country uptime response Etc

3588
02:43:44,220 --> 02:43:52,020
so here there is 9 000 proxies online in

3589
02:43:49,200 --> 02:43:53,640
136 countries but the downside is of

3590
02:43:52,020 --> 02:43:56,100
using these lists is that because

3591
02:43:53,640 --> 02:43:59,100
they're free so many people are using

3592
02:43:56,100 --> 02:44:01,560
them that they're very likely to either

3593
02:43:59,100 --> 02:44:04,080
have very poor response times and take

3594
02:44:01,560 --> 02:44:07,260
very long time to actually root your

3595
02:44:04,080 --> 02:44:09,060
traffic through them or else they can be

3596
02:44:07,260 --> 02:44:11,340
already blacklisted because think of it

3597
02:44:09,060 --> 02:44:13,859
if someone has already used them to

3598
02:44:11,340 --> 02:44:15,960
scrape millions of pages from maybe the

3599
02:44:13,859 --> 02:44:18,540
same site that you're going to look to

3600
02:44:15,960 --> 02:44:21,420
scrape data from then there's a very

3601
02:44:18,540 --> 02:44:24,359
high likelihood that that website could

3602
02:44:21,420 --> 02:44:26,040
already have discovered this IP address

3603
02:44:24,359 --> 02:44:28,620
and blocked it

3604
02:44:26,040 --> 02:44:30,780
so the pro is that it's free the cons

3605
02:44:28,620 --> 02:44:33,540
are that it can take a long time and

3606
02:44:30,780 --> 02:44:35,640
there's a very high likelihood that if

3607
02:44:33,540 --> 02:44:38,340
it's free that it's already been used

3608
02:44:35,640 --> 02:44:40,800
too much and it could be blocked

3609
02:44:38,340 --> 02:44:43,080
so we're going to go ahead anyway and

3610
02:44:40,800 --> 02:44:47,040
try with a few of these IP addresses and

3611
02:44:43,080 --> 02:44:48,600
a few ports from these two sites and the

3612
02:44:47,040 --> 02:44:51,720
way we want to integrate them into our

3613
02:44:48,600 --> 02:44:54,420
project is we're going to use this

3614
02:44:51,720 --> 02:44:57,359
GitHub project which integrates with

3615
02:44:54,420 --> 02:44:58,740
scrapey and it's called scrippy rotating

3616
02:44:57,359 --> 02:45:00,840
proxies

3617
02:44:58,740 --> 02:45:03,660
so we'll have a link to this

3618
02:45:00,840 --> 02:45:06,420
available but you can just do pip

3619
02:45:03,660 --> 02:45:08,399
install scrapey Dash rotating Dash

3620
02:45:06,420 --> 02:45:11,160
proxies

3621
02:45:08,399 --> 02:45:12,479
and go to your terminal

3622
02:45:11,160 --> 02:45:14,700
and

3623
02:45:12,479 --> 02:45:17,100
paste that in and run it

3624
02:45:14,700 --> 02:45:18,420
now I already have it installed so it's

3625
02:45:17,100 --> 02:45:20,640
going to say requirement already

3626
02:45:18,420 --> 02:45:22,979
satisfied for me but for you guys you

3627
02:45:20,640 --> 02:45:27,180
should see it installed in there

3628
02:45:22,979 --> 02:45:29,160
now we're continuing on part nine from

3629
02:45:27,180 --> 02:45:31,560
part eight so if you're looking for the

3630
02:45:29,160 --> 02:45:33,420
code for where we are starting at now

3631
02:45:31,560 --> 02:45:35,700
we'll have that available in a GitHub

3632
02:45:33,420 --> 02:45:38,939
repo which we link to

3633
02:45:35,700 --> 02:45:41,880
so you can continue on from here with us

3634
02:45:38,939 --> 02:45:43,800
so now that we have that installed we

3635
02:45:41,880 --> 02:45:49,740
can go ahead and

3636
02:45:43,800 --> 02:45:51,359
we can add our proxy list in so as you

3637
02:45:49,740 --> 02:45:53,580
probably guess everything is going to go

3638
02:45:51,359 --> 02:45:56,819
into our settings file

3639
02:45:53,580 --> 02:45:58,200
as with everything else that's part of

3640
02:45:56,819 --> 02:46:02,340
our project

3641
02:45:58,200 --> 02:46:04,740
and these are just dummy

3642
02:46:02,340 --> 02:46:07,200
domain IP addresses

3643
02:46:04,740 --> 02:46:09,420
but this is the idea so you can have

3644
02:46:07,200 --> 02:46:12,660
this many you could have a hundred

3645
02:46:09,420 --> 02:46:14,520
different IP address and ports in here

3646
02:46:12,660 --> 02:46:15,420
but we're just going to put three or

3647
02:46:14,520 --> 02:46:18,060
four

3648
02:46:15,420 --> 02:46:19,740
just for the purpose of showing you how

3649
02:46:18,060 --> 02:46:23,819
it works

3650
02:46:19,740 --> 02:46:26,280
so let's go back to our free proxy list

3651
02:46:23,819 --> 02:46:29,359
and take

3652
02:46:26,280 --> 02:46:29,359
two of these guys

3653
02:46:31,500 --> 02:46:37,160
so we want the IP address in the port

3654
02:46:43,859 --> 02:46:47,399
now obviously depending on your use case

3655
02:46:46,080 --> 02:46:49,680
you might need

3656
02:46:47,399 --> 02:46:52,080
a

3657
02:46:49,680 --> 02:46:53,939
and you might need a proxy from a

3658
02:46:52,080 --> 02:46:56,220
specific country

3659
02:46:53,939 --> 02:46:59,819
or you might need

3660
02:46:56,220 --> 02:47:02,540
a proxy with a very good uptime or

3661
02:46:59,819 --> 02:47:02,540
response time

3662
02:47:02,700 --> 02:47:12,859
so that's for you guys to search in the

3663
02:47:08,280 --> 02:47:12,859
search boxes on the site here

3664
02:47:14,160 --> 02:47:20,399
okay so I've got three of them there

3665
02:47:18,260 --> 02:47:24,120
and the next thing I want to do is

3666
02:47:20,399 --> 02:47:26,700
enable the middleware so this project

3667
02:47:24,120 --> 02:47:29,340
that I've just installed this scrapey

3668
02:47:26,700 --> 02:47:30,359
rotating proxies will have installed a

3669
02:47:29,340 --> 02:47:32,939
middleware

3670
02:47:30,359 --> 02:47:35,460
but to actually make sure that the

3671
02:47:32,939 --> 02:47:37,439
middleware works we need to add it to

3672
02:47:35,460 --> 02:47:39,960
our downloader middlewares

3673
02:47:37,439 --> 02:47:42,660
and that's where we can

3674
02:47:39,960 --> 02:47:44,460
make sure it's enabled

3675
02:47:42,660 --> 02:47:48,240
so I've just done gone ahead and done

3676
02:47:44,460 --> 02:47:51,000
that there I've added them in so

3677
02:47:48,240 --> 02:47:53,399
and as you can see I've left the other

3678
02:47:51,000 --> 02:47:56,340
two middlewares that we had from part 8

3679
02:47:53,399 --> 02:47:59,040
in here as well so obviously they don't

3680
02:47:56,340 --> 02:48:00,840
have to be here I can also remove them

3681
02:47:59,040 --> 02:48:02,100
but I might as well leave them in for

3682
02:48:00,840 --> 02:48:04,080
now they're not going to do any harm

3683
02:48:02,100 --> 02:48:07,680
they're just adding in

3684
02:48:04,080 --> 02:48:10,319
a different request header

3685
02:48:07,680 --> 02:48:13,620
so let's save that

3686
02:48:10,319 --> 02:48:16,500
and then the other thing I wanted to

3687
02:48:13,620 --> 02:48:20,100
quickly show you is that if you had all

3688
02:48:16,500 --> 02:48:22,319
the proxies already in a file you could

3689
02:48:20,100 --> 02:48:25,620
do something as simple as

3690
02:48:22,319 --> 02:48:28,859
just saying the rotating proxy list path

3691
02:48:25,620 --> 02:48:31,620
is equal to and then the path to your

3692
02:48:28,859 --> 02:48:35,100
file obviously we don't have a file with

3693
02:48:31,620 --> 02:48:37,140
a bunch of ips and ports but that's

3694
02:48:35,100 --> 02:48:39,240
where you would put it if that's what

3695
02:48:37,140 --> 02:48:42,600
you're wanting to do

3696
02:48:39,240 --> 02:48:45,660
so let's just quickly remove that

3697
02:48:42,600 --> 02:48:49,439
and now we can go ahead and we can run

3698
02:48:45,660 --> 02:48:51,540
our spider and see the results so I'm

3699
02:48:49,439 --> 02:48:53,220
just going to do scrippy crawl and the

3700
02:48:51,540 --> 02:48:55,080
name of the spider

3701
02:48:53,220 --> 02:48:57,359
I'm just going to make sure and in my

3702
02:48:55,080 --> 02:48:59,460
project

3703
02:48:57,359 --> 02:49:02,180
it's great big crawl

3704
02:48:59,460 --> 02:49:02,180
the book Spider

3705
02:49:03,000 --> 02:49:06,720
so it's going to go ahead and run you

3706
02:49:05,520 --> 02:49:10,380
can see

3707
02:49:06,720 --> 02:49:13,500
the header that was attached from part 8

3708
02:49:10,380 --> 02:49:16,280
where we were adding the new header

3709
02:49:13,500 --> 02:49:20,460
and now this can take a good bit of time

3710
02:49:16,280 --> 02:49:23,880
so as you can see here this rotating

3711
02:49:20,460 --> 02:49:26,399
proxies Dot middlewares

3712
02:49:23,880 --> 02:49:29,340
has printed out that

3713
02:49:26,399 --> 02:49:32,160
it has zero good proxies

3714
02:49:29,340 --> 02:49:33,720
zero dead proxies and three unchecked

3715
02:49:32,160 --> 02:49:35,280
proxies

3716
02:49:33,720 --> 02:49:37,439
so that means that it's going to go

3717
02:49:35,280 --> 02:49:40,859
ahead and it's first going to try and

3718
02:49:37,439 --> 02:49:44,160
see can it actually send any requests

3719
02:49:40,859 --> 02:49:46,620
through the proxies that we've listed

3720
02:49:44,160 --> 02:49:49,319
here so this can take a good bit of time

3721
02:49:46,620 --> 02:49:52,500
depending on the quality of the proxies

3722
02:49:49,319 --> 02:49:55,260
that we've got obviously I have no idea

3723
02:49:52,500 --> 02:49:58,620
how good the ones in that free list are

3724
02:49:55,260 --> 02:50:00,600
because they change every day every hour

3725
02:49:58,620 --> 02:50:02,760
there's new ones added and there's ones

3726
02:50:00,600 --> 02:50:05,220
removed and then as soon as they're

3727
02:50:02,760 --> 02:50:08,399
added they're being used by hundreds if

3728
02:50:05,220 --> 02:50:09,540
not thousands of other users

3729
02:50:08,399 --> 02:50:11,340
so

3730
02:50:09,540 --> 02:50:14,040
this is the good thing about this

3731
02:50:11,340 --> 02:50:16,880
middleware is that it checks cannot

3732
02:50:14,040 --> 02:50:20,520
actually use it and as you can see here

3733
02:50:16,880 --> 02:50:23,520
it's just retrying our books to scrape

3734
02:50:20,520 --> 02:50:26,819
URL with another proxy because one of

3735
02:50:23,520 --> 02:50:29,880
them failed to work

3736
02:50:26,819 --> 02:50:32,939
so it's just a process of waiting so

3737
02:50:29,880 --> 02:50:36,240
it's moved one of the proxies into our

3738
02:50:32,939 --> 02:50:37,500
dead pool and it's still got two that it

3739
02:50:36,240 --> 02:50:40,380
wants to check

3740
02:50:37,500 --> 02:50:43,020
so this is just a process of waiting and

3741
02:50:40,380 --> 02:50:45,359
letting the middleware do its work so

3742
02:50:43,020 --> 02:50:47,280
I'm gonna leave that run for a few

3743
02:50:45,359 --> 02:50:50,700
minutes and come back to it and we'll

3744
02:50:47,280 --> 02:50:51,540
see did it actually manage to use any of

3745
02:50:50,700 --> 02:50:54,000
those

3746
02:50:51,540 --> 02:50:56,280
free proxies that were on that free

3747
02:50:54,000 --> 02:50:59,220
proxy list

3748
02:50:56,280 --> 02:51:02,340
so I'll come back in one second and

3749
02:50:59,220 --> 02:51:04,920
we'll see where we are okay so I've just

3750
02:51:02,340 --> 02:51:07,620
come back a few minutes later and

3751
02:51:04,920 --> 02:51:10,560
it still hasn't managed to

3752
02:51:07,620 --> 02:51:11,760
get any of our three proxies in the list

3753
02:51:10,560 --> 02:51:15,020
to work

3754
02:51:11,760 --> 02:51:16,700
he's got two dead now one it's trying to

3755
02:51:15,020 --> 02:51:21,060
reanimate

3756
02:51:16,700 --> 02:51:22,740
it's not looking good so obviously the

3757
02:51:21,060 --> 02:51:26,340
ones I've picked were probably already

3758
02:51:22,740 --> 02:51:28,620
overly used already could be blocked by

3759
02:51:26,340 --> 02:51:31,319
the site we're trying to scrape

3760
02:51:28,620 --> 02:51:34,380
so here you can see obviously the major

3761
02:51:31,319 --> 02:51:36,120
disadvantage of using free proxy lists

3762
02:51:34,380 --> 02:51:39,359
online now there's lots of different

3763
02:51:36,120 --> 02:51:42,479
places to get them so depending on your

3764
02:51:39,359 --> 02:51:45,180
source of the proxies you may have much

3765
02:51:42,479 --> 02:51:46,859
better luck at getting them to work but

3766
02:51:45,180 --> 02:51:50,819
it's really a process of trial and error

3767
02:51:46,859 --> 02:51:53,520
and while it's free it can be painful to

3768
02:51:50,819 --> 02:51:55,859
actually get up and running consistently

3769
02:51:53,520 --> 02:51:57,060
and correctly

3770
02:51:55,859 --> 02:51:59,580
so

3771
02:51:57,060 --> 02:52:02,220
we've had a look at how we can just

3772
02:51:59,580 --> 02:52:04,859
plug in a bunch of different IP

3773
02:52:02,220 --> 02:52:07,380
addresses and ports into our rotating

3774
02:52:04,859 --> 02:52:12,000
proxy list and how we can use this

3775
02:52:07,380 --> 02:52:15,240
middleware to use our proxies in our

3776
02:52:12,000 --> 02:52:17,520
scrapey project but another way we can

3777
02:52:15,240 --> 02:52:21,720
do this is

3778
02:52:17,520 --> 02:52:25,740
using a proxy port so what we would be

3779
02:52:21,720 --> 02:52:27,660
using is a service which is provided by

3780
02:52:25,740 --> 02:52:32,399
a proxy provider

3781
02:52:27,660 --> 02:52:36,380
and they would give us a proxy IP

3782
02:52:32,399 --> 02:52:40,439
address and port and they would handle

3783
02:52:36,380 --> 02:52:41,640
changing the IP address every time and

3784
02:52:40,439 --> 02:52:45,540
we wouldn't have to worry about

3785
02:52:41,640 --> 02:52:47,340
compiling a list of proxies ourselves so

3786
02:52:45,540 --> 02:52:51,840
that's what we're going to look at next

3787
02:52:47,340 --> 02:52:56,160
so we can still be looking after our own

3788
02:52:51,840 --> 02:52:58,439
user agents and our own headers but the

3789
02:52:56,160 --> 02:53:01,560
proxy provider would be dealing with

3790
02:52:58,439 --> 02:53:03,779
everything to do with rotating a proxy

3791
02:53:01,560 --> 02:53:07,740
list for us and making sure that the

3792
02:53:03,779 --> 02:53:10,319
proxy list is of good quality and

3793
02:53:07,740 --> 02:53:12,359
available all the time so we wouldn't

3794
02:53:10,319 --> 02:53:14,040
have to worry about that now there's

3795
02:53:12,359 --> 02:53:18,000
lots of them out there and we're just

3796
02:53:14,040 --> 02:53:21,840
going to look at one of them now

3797
02:53:18,000 --> 02:53:25,439
the one I've just going to show you now

3798
02:53:21,840 --> 02:53:27,300
is called smart proxy you can check them

3799
02:53:25,439 --> 02:53:31,260
out at smartproxy.com

3800
02:53:27,300 --> 02:53:33,899
and as they say effortlessly scrape web

3801
02:53:31,260 --> 02:53:38,160
data you need so they've got some great

3802
02:53:33,899 --> 02:53:39,720
deals and offers and as you can see they

3803
02:53:38,160 --> 02:53:42,899
you know do things like bypassing

3804
02:53:39,720 --> 02:53:45,359
captures IP bands they've got millions

3805
02:53:42,899 --> 02:53:50,460
of proxies from millions of locations

3806
02:53:45,359 --> 02:53:53,100
and the plans they do entail residential

3807
02:53:50,460 --> 02:53:55,620
and data center proxies so we haven't

3808
02:53:53,100 --> 02:53:59,460
talked about that yet but residential

3809
02:53:55,620 --> 02:54:02,340
proxies would be basically the data will

3810
02:53:59,460 --> 02:54:04,380
be forwarded through residential IP

3811
02:54:02,340 --> 02:54:08,819
addresses so these are IP addresses that

3812
02:54:04,380 --> 02:54:10,800
are mainly used by people's homes so

3813
02:54:08,819 --> 02:54:14,220
think of it if someone is watching

3814
02:54:10,800 --> 02:54:17,279
Netflix and browsing Facebook and

3815
02:54:14,220 --> 02:54:20,640
looking at Google search and then

3816
02:54:17,279 --> 02:54:24,720
one or two of your requests are going

3817
02:54:20,640 --> 02:54:26,460
via that IP address then the website

3818
02:54:24,720 --> 02:54:29,100
you're trying to scrape let's say Amazon

3819
02:54:26,460 --> 02:54:30,779
is going to say oh well I saw that IP

3820
02:54:29,100 --> 02:54:32,760
address yesterday they just bought

3821
02:54:30,779 --> 02:54:35,760
something from me so they're much more

3822
02:54:32,760 --> 02:54:38,220
likely just to let that request go

3823
02:54:35,760 --> 02:54:40,560
through without any issue so that would

3824
02:54:38,220 --> 02:54:43,020
be what residential proxies are then

3825
02:54:40,560 --> 02:54:44,939
data center proxies would be think of

3826
02:54:43,020 --> 02:54:48,260
you know your traditional data centers

3827
02:54:44,939 --> 02:54:51,420
with thousands of servers in a big room

3828
02:54:48,260 --> 02:54:53,819
and your requests will be routed through

3829
02:54:51,420 --> 02:54:56,279
a Data Center and through the IP address

3830
02:54:53,819 --> 02:54:58,439
that is belonging to one of the machines

3831
02:54:56,279 --> 02:55:02,100
in the data center

3832
02:54:58,439 --> 02:55:05,220
so you have access to a lot more IP

3833
02:55:02,100 --> 02:55:07,380
addresses in the residential side but

3834
02:55:05,220 --> 02:55:09,779
then the data center side are much

3835
02:55:07,380 --> 02:55:11,819
quicker and there tends to be not as

3836
02:55:09,779 --> 02:55:14,460
many data limits and they tend to be a

3837
02:55:11,819 --> 02:55:17,340
bit cheaper as well so that's the

3838
02:55:14,460 --> 02:55:20,460
difference between residential and data

3839
02:55:17,340 --> 02:55:23,220
center proxies so you can sign up with

3840
02:55:20,460 --> 02:55:27,060
them most proxy providers also give you

3841
02:55:23,220 --> 02:55:28,859
a week or two of a free trial or a

3842
02:55:27,060 --> 02:55:32,880
certain amount of free credits that you

3843
02:55:28,859 --> 02:55:34,680
can use to test out their service so if

3844
02:55:32,880 --> 02:55:36,899
you guys go ahead you can click get

3845
02:55:34,680 --> 02:55:40,020
started there sign up for an account and

3846
02:55:36,899 --> 02:55:43,500
then once you're logged in

3847
02:55:40,020 --> 02:55:44,580
if you go to the residential tab

3848
02:55:43,500 --> 02:55:48,420
because we're going to be using

3849
02:55:44,580 --> 02:55:51,479
residential proxies for our next part

3850
02:55:48,420 --> 02:55:53,100
now so click the residential Tab and

3851
02:55:51,479 --> 02:55:55,859
then you can either check out a pay as

3852
02:55:53,100 --> 02:55:58,620
you go plan where you pay per gigabytes

3853
02:55:55,859 --> 02:56:01,979
of data that's transferred or you can go

3854
02:55:58,620 --> 02:56:04,200
into regular or Enterprise as well so I

3855
02:56:01,979 --> 02:56:06,060
already have a plan set up with them so

3856
02:56:04,200 --> 02:56:07,680
I'm going to go directly to the proxy

3857
02:56:06,060 --> 02:56:10,439
setup next

3858
02:56:07,680 --> 02:56:13,620
and this is where we will get our

3859
02:56:10,439 --> 02:56:16,920
details which we'll then put into our

3860
02:56:13,620 --> 02:56:19,920
spider first off we want to generate our

3861
02:56:16,920 --> 02:56:23,040
username and password so we can put in

3862
02:56:19,920 --> 02:56:26,520
any kind of combination of

3863
02:56:23,040 --> 02:56:29,640
letters and numbers here a password

3864
02:56:26,520 --> 02:56:31,800
and click create and it creates a

3865
02:56:29,640 --> 02:56:36,080
username and password once you have your

3866
02:56:31,800 --> 02:56:39,840
username and password you can grab these

3867
02:56:36,080 --> 02:56:43,140
and put those into the username and

3868
02:56:39,840 --> 02:56:46,500
password field here then our proxy

3869
02:56:43,140 --> 02:56:48,840
location if it's important for your

3870
02:56:46,500 --> 02:56:51,140
spider that you are scraping from a

3871
02:56:48,840 --> 02:56:54,359
certain country for example if you're

3872
02:56:51,140 --> 02:56:57,060
scraping an e-commerce site that will

3873
02:56:54,359 --> 02:56:59,040
only show you specific products if

3874
02:56:57,060 --> 02:57:01,500
you're living in a certain country then

3875
02:56:59,040 --> 02:57:04,800
it is important to select the country

3876
02:57:01,500 --> 02:57:07,920
here from this list so for us it doesn't

3877
02:57:04,800 --> 02:57:10,500
matter so we can leave it at random

3878
02:57:07,920 --> 02:57:13,740
then for the session type we want

3879
02:57:10,500 --> 02:57:17,100
rotating because we don't want a fixed

3880
02:57:13,740 --> 02:57:20,700
session every request can come from a

3881
02:57:17,100 --> 02:57:23,700
different IP address and that doesn't

3882
02:57:20,700 --> 02:57:26,040
matter for us right now and for the

3883
02:57:23,700 --> 02:57:27,899
output format we're just going to pick

3884
02:57:26,040 --> 02:57:30,840
http

3885
02:57:27,899 --> 02:57:34,859
so that's going to then give us this

3886
02:57:30,840 --> 02:57:37,800
string here which we can copy and use in

3887
02:57:34,859 --> 02:57:40,500
our project so this is the endpoint

3888
02:57:37,800 --> 02:57:43,140
where we're going to send our requests

3889
02:57:40,500 --> 02:57:46,439
to Smart proxy is going to handle all

3890
02:57:43,140 --> 02:57:49,020
the IP address rotation and all that

3891
02:57:46,439 --> 02:57:51,660
stuff and it's going to then send us

3892
02:57:49,020 --> 02:57:53,160
back the response from the website we

3893
02:57:51,660 --> 02:57:56,160
are trying to scrape

3894
02:57:53,160 --> 02:57:58,680
now that we have our endpoint

3895
02:57:56,160 --> 02:58:02,279
from Smart proxy the next thing we want

3896
02:57:58,680 --> 02:58:05,040
to do is go back to our project

3897
02:58:02,279 --> 02:58:07,620
we want to disable the middleware we

3898
02:58:05,040 --> 02:58:09,240
were using because that will no longer

3899
02:58:07,620 --> 02:58:11,359
be needed because

3900
02:58:09,240 --> 02:58:15,720
smart proxy is going to be looking after

3901
02:58:11,359 --> 02:58:17,340
rotating our proxies and it's going to

3902
02:58:15,720 --> 02:58:18,660
be looking after some band detection

3903
02:58:17,340 --> 02:58:21,300
stuff as well

3904
02:58:18,660 --> 02:58:24,960
so we can disable the two of them

3905
02:58:21,300 --> 02:58:26,640
and the next thing we can do is go to

3906
02:58:24,960 --> 02:58:30,740
our spider

3907
02:58:26,640 --> 02:58:34,680
and we can go to where we have

3908
02:58:30,740 --> 02:58:36,120
response.follow and in here we'll simply

3909
02:58:34,680 --> 02:58:39,920
add in

3910
02:58:36,120 --> 02:58:44,520
one more field which

3911
02:58:39,920 --> 02:58:47,520
is going to be meta and then the proxy

3912
02:58:44,520 --> 02:58:51,180
information so matter is equal to

3913
02:58:47,520 --> 02:58:52,380
proxy and then our proxy details will go

3914
02:58:51,180 --> 02:58:55,020
in here

3915
02:58:52,380 --> 02:58:59,819
so I can go back

3916
02:58:55,020 --> 02:59:01,560
grab my proxy endpoint and paste it in

3917
02:58:59,819 --> 02:59:03,479
here

3918
02:59:01,560 --> 02:59:06,000
so

3919
02:59:03,479 --> 02:59:10,920
that looks correct

3920
02:59:06,000 --> 02:59:15,359
and then I can also copy this

3921
02:59:10,920 --> 02:59:17,520
and put it down where we also have

3922
02:59:15,359 --> 02:59:19,140
response dot follow

3923
02:59:17,520 --> 02:59:22,380
below

3924
02:59:19,140 --> 02:59:24,060
so I'll add it in here too

3925
02:59:22,380 --> 02:59:26,399
and

3926
02:59:24,060 --> 02:59:28,319
that should be the two main places I

3927
02:59:26,399 --> 02:59:30,479
needed for now

3928
02:59:28,319 --> 02:59:32,760
the other example which I'll show you in

3929
02:59:30,479 --> 02:59:36,240
a second is that we can create a custom

3930
02:59:32,760 --> 02:59:39,359
middleware which would insert the

3931
02:59:36,240 --> 02:59:42,060
endpoint and as well so we'll do that

3932
02:59:39,359 --> 02:59:45,240
once we get this to run correctly

3933
02:59:42,060 --> 02:59:49,800
we can now do Scrapy crawl book spider

3934
02:59:45,240 --> 02:59:52,260
and it should work for us so Scrapy

3935
02:59:49,800 --> 02:59:57,260
crawl book Spider

3936
02:59:52,260 --> 02:59:57,260
and hopefully we have no issues

3937
02:59:57,600 --> 03:00:00,899
as I can see there's some things coming

3938
02:59:59,340 --> 03:00:05,460
through

3939
03:00:00,899 --> 03:00:08,640
I can go to my books data.json

3940
03:00:05,460 --> 03:00:11,700
and I can see that there is

3941
03:00:08,640 --> 03:00:13,439
the data coming through so it looks like

3942
03:00:11,700 --> 03:00:14,819
it's working correctly and it's all

3943
03:00:13,439 --> 03:00:16,080
going via

3944
03:00:14,819 --> 03:00:18,779
the

3945
03:00:16,080 --> 03:00:21,240
smart proxy endpoint so I can

3946
03:00:18,779 --> 03:00:23,819
close down my spider and the next thing

3947
03:00:21,240 --> 03:00:27,120
we can do is we can create a custom

3948
03:00:23,819 --> 03:00:29,420
middleware version so just adding it in

3949
03:00:27,120 --> 03:00:31,260
to our

3950
03:00:29,420 --> 03:00:33,840
meta

3951
03:00:31,260 --> 03:00:35,880
value here and adding in our proxy

3952
03:00:33,840 --> 03:00:37,080
endpoint is fine if you've got a small

3953
03:00:35,880 --> 03:00:39,060
project

3954
03:00:37,080 --> 03:00:40,920
but if you've got a larger project it

3955
03:00:39,060 --> 03:00:43,319
probably makes more sense just to make a

3956
03:00:40,920 --> 03:00:45,600
custom middleware for it so I'm going to

3957
03:00:43,319 --> 03:00:47,460
show you how to do that next

3958
03:00:45,600 --> 03:00:50,220
so we'll scroll down to the bottom

3959
03:00:47,460 --> 03:00:52,200
because we have our other middle words

3960
03:00:50,220 --> 03:00:56,240
in here already

3961
03:00:52,200 --> 03:00:59,760
and we'll create a new middleware

3962
03:00:56,240 --> 03:01:01,859
where we will be adding our endpoint

3963
03:00:59,760 --> 03:01:03,779
details

3964
03:01:01,859 --> 03:01:06,600
so

3965
03:01:03,779 --> 03:01:07,920
we just make a new class called my proxy

3966
03:01:06,600 --> 03:01:11,160
middleware

3967
03:01:07,920 --> 03:01:13,740
it's going to again pull in our crawler

3968
03:01:11,160 --> 03:01:16,140
settings and then it's going to get our

3969
03:01:13,740 --> 03:01:19,380
proxy user proxy password proxy endpoint

3970
03:01:16,140 --> 03:01:21,600
and proxy port from our settings so we

3971
03:01:19,380 --> 03:01:22,680
need to go ahead and set those in our

3972
03:01:21,600 --> 03:01:25,979
settings

3973
03:01:22,680 --> 03:01:28,319
and then once it's got those it makes

3974
03:01:25,979 --> 03:01:30,359
the user credentials it puts those

3975
03:01:28,319 --> 03:01:33,779
credentials into

3976
03:01:30,359 --> 03:01:34,920
a proxy authorization header for the

3977
03:01:33,779 --> 03:01:39,000
request

3978
03:01:34,920 --> 03:01:41,760
and then it has the URL which is made

3979
03:01:39,000 --> 03:01:46,979
with the endpoint and port

3980
03:01:41,760 --> 03:01:48,960
and that then goes into the request dot

3981
03:01:46,979 --> 03:01:51,660
meta

3982
03:01:48,960 --> 03:01:53,700
so let's go ahead now and in our

3983
03:01:51,660 --> 03:01:56,939
settings fill out our proxy user

3984
03:01:53,700 --> 03:02:00,920
password endpoint and port

3985
03:01:56,939 --> 03:02:04,500
so let me just go here

3986
03:02:00,920 --> 03:02:07,439
and add them in now

3987
03:02:04,500 --> 03:02:10,560
so username password endpoint and Port

3988
03:02:07,439 --> 03:02:12,779
so I just need to change the password

3989
03:02:10,560 --> 03:02:15,920
obviously your username and password are

3990
03:02:12,779 --> 03:02:19,380
going to be whatever you guys have made

3991
03:02:15,920 --> 03:02:23,359
in your own dashboards with smart proxy

3992
03:02:19,380 --> 03:02:23,359
I'm just copying my details from here

3993
03:02:24,479 --> 03:02:29,399
and that looks fine so I should be able

3994
03:02:27,120 --> 03:02:33,420
to save that

3995
03:02:29,399 --> 03:02:36,420
and the next thing I need to do is to

3996
03:02:33,420 --> 03:02:37,620
make sure my middleware is enabled so

3997
03:02:36,420 --> 03:02:39,660
again

3998
03:02:37,620 --> 03:02:41,580
going to my

3999
03:02:39,660 --> 03:02:45,300
downloader middlewares

4000
03:02:41,580 --> 03:02:47,160
and I'm going to add in my

4001
03:02:45,300 --> 03:02:49,800
new

4002
03:02:47,160 --> 03:02:53,279
middleware

4003
03:02:49,800 --> 03:02:56,220
so I can add that in there and save it

4004
03:02:53,279 --> 03:02:58,319
the next thing we want to do is try and

4005
03:02:56,220 --> 03:03:02,279
run our spider again and see does it

4006
03:02:58,319 --> 03:03:04,439
work but we will obviously remove

4007
03:03:02,279 --> 03:03:08,040
what we did a second ago

4008
03:03:04,439 --> 03:03:09,420
so that we can show it's all going via

4009
03:03:08,040 --> 03:03:12,260
r

4010
03:03:09,420 --> 03:03:12,260
new middleware

4011
03:03:13,140 --> 03:03:17,880
so let's just

4012
03:03:15,899 --> 03:03:20,520
remove that

4013
03:03:17,880 --> 03:03:21,960
save it

4014
03:03:20,520 --> 03:03:25,380
and then

4015
03:03:21,960 --> 03:03:28,020
try run or spider again

4016
03:03:25,380 --> 03:03:31,620
and it looks like the

4017
03:03:28,020 --> 03:03:35,040
book details are coming through again so

4018
03:03:31,620 --> 03:03:38,220
I'm just going to close my spider

4019
03:03:35,040 --> 03:03:40,140
now we can have a look at smart proxy

4020
03:03:38,220 --> 03:03:41,819
and see

4021
03:03:40,140 --> 03:03:43,800
the

4022
03:03:41,819 --> 03:03:46,620
traffic usage

4023
03:03:43,800 --> 03:03:50,660
and as you can see

4024
03:03:46,620 --> 03:03:50,660
weave requests coming through

4025
03:03:51,960 --> 03:03:57,500
so there you go

4026
03:03:54,540 --> 03:03:57,500
we've got the user

4027
03:04:00,060 --> 03:04:05,819
and we've got our

4028
03:04:03,060 --> 03:04:07,319
usage by gigabyte

4029
03:04:05,819 --> 03:04:09,720
so

4030
03:04:07,319 --> 03:04:12,180
it's working just

4031
03:04:09,720 --> 03:04:15,060
as we wanted it to work

4032
03:04:12,180 --> 03:04:17,580
our requests are going through the

4033
03:04:15,060 --> 03:04:20,180
inspired proxy endpoint smart proxy is

4034
03:04:17,580 --> 03:04:24,080
looking after the IP address rotation

4035
03:04:20,180 --> 03:04:27,479
and it is sending us back the request

4036
03:04:24,080 --> 03:04:31,319
then scrapey is able to take the

4037
03:04:27,479 --> 03:04:33,899
information out of the HTML and we have

4038
03:04:31,319 --> 03:04:35,939
the data that we need

4039
03:04:33,899 --> 03:04:37,800
I think that's given you a very good

4040
03:04:35,939 --> 03:04:42,720
overview of how

4041
03:04:37,800 --> 03:04:46,080
we would use proxy port endpoints

4042
03:04:42,720 --> 03:04:47,939
so there's just one last thing I wanted

4043
03:04:46,080 --> 03:04:50,399
to show you guys which is

4044
03:04:47,939 --> 03:04:52,920
proxy API endpoints

4045
03:04:50,399 --> 03:04:56,460
so this is if you want to go just a step

4046
03:04:52,920 --> 03:05:00,060
further and not have to deal with

4047
03:04:56,460 --> 03:05:03,359
the browser headers are the user agents

4048
03:05:00,060 --> 03:05:06,420
or any things like that and maybe you

4049
03:05:03,359 --> 03:05:08,939
are scraping something which requires a

4050
03:05:06,420 --> 03:05:11,040
headless browser to do JavaScript

4051
03:05:08,939 --> 03:05:16,319
running for you

4052
03:05:11,040 --> 03:05:18,779
we can get that by using a proxy API so

4053
03:05:16,319 --> 03:05:20,760
again it's a service where there's an

4054
03:05:18,779 --> 03:05:23,939
endpoint we're sending our request

4055
03:05:20,760 --> 03:05:26,160
through that service and then that

4056
03:05:23,939 --> 03:05:29,340
service is making sure that certain

4057
03:05:26,160 --> 03:05:34,740
things are enabled to make sure that the

4058
03:05:29,340 --> 03:05:37,260
request gets us the page data

4059
03:05:34,740 --> 03:05:38,939
so what we're going to do is we are

4060
03:05:37,260 --> 03:05:42,260
going to

4061
03:05:38,939 --> 03:05:45,180
show you how to use that now

4062
03:05:42,260 --> 03:05:46,620
and that is also going to be a paid

4063
03:05:45,180 --> 03:05:51,740
service

4064
03:05:46,620 --> 03:05:55,740
and you can sign up for that by going to

4065
03:05:51,740 --> 03:05:58,560
scrapeups.io clicking get free account

4066
03:05:55,740 --> 03:06:01,319
signing up for it you've got

4067
03:05:58,560 --> 03:06:03,540
a thousand free credits there and if you

4068
03:06:01,319 --> 03:06:08,100
then once you're logged in

4069
03:06:03,540 --> 03:06:09,899
go to the proxy aggregator page go to

4070
03:06:08,100 --> 03:06:13,620
the request Builder

4071
03:06:09,899 --> 03:06:18,080
and you then have an API

4072
03:06:13,620 --> 03:06:21,600
key which you can use and you've got the

4073
03:06:18,080 --> 03:06:23,279
proxy endpoint which you can use as well

4074
03:06:21,600 --> 03:06:26,220
in your spider

4075
03:06:23,279 --> 03:06:29,160
so once you've got your API key we can

4076
03:06:26,220 --> 03:06:33,660
move back to our book Spider

4077
03:06:29,160 --> 03:06:37,319
file and we will start adding in a new

4078
03:06:33,660 --> 03:06:41,760
function which will help us send the

4079
03:06:37,319 --> 03:06:43,979
traffic first to our new proxy provider

4080
03:06:41,760 --> 03:06:46,200
so this new function is going to be

4081
03:06:43,979 --> 03:06:49,620
called get underscore proxy underscore

4082
03:06:46,200 --> 03:06:51,000
URL and we're going to pass in a URL to

4083
03:06:49,620 --> 03:06:54,540
that function

4084
03:06:51,000 --> 03:06:58,319
and then we're going to have an API key

4085
03:06:54,540 --> 03:07:00,479
as part of this payload object and

4086
03:06:58,319 --> 03:07:02,760
we're obviously going to put in

4087
03:07:00,479 --> 03:07:05,460
an API key where we

4088
03:07:02,760 --> 03:07:07,620
have our own API key that we got from

4089
03:07:05,460 --> 03:07:08,760
scrape UPS so I'm going to add mine in

4090
03:07:07,620 --> 03:07:13,020
quickly now

4091
03:07:08,760 --> 03:07:14,819
I'm just going to copy and paste that in

4092
03:07:13,020 --> 03:07:17,819
and

4093
03:07:14,819 --> 03:07:20,819
that then is going to slot in here this

4094
03:07:17,819 --> 03:07:24,540
payload is going to get URL encoded so I

4095
03:07:20,819 --> 03:07:27,540
need to import this URL encode from URL

4096
03:07:24,540 --> 03:07:30,240
lib and then it's going to create this

4097
03:07:27,540 --> 03:07:34,319
new proxy URL and then it's going to

4098
03:07:30,240 --> 03:07:36,180
return that proxy URL so this function

4099
03:07:34,319 --> 03:07:39,600
is going to

4100
03:07:36,180 --> 03:07:41,520
get the URL of the site that we want to

4101
03:07:39,600 --> 03:07:43,680
scrape and it's going to encode it along

4102
03:07:41,520 --> 03:07:47,340
with our API key and it's going to send

4103
03:07:43,680 --> 03:07:50,880
it to this API endpoint

4104
03:07:47,340 --> 03:07:52,500
once we've got that function created the

4105
03:07:50,880 --> 03:07:54,540
next thing we want to do is we want to

4106
03:07:52,500 --> 03:07:58,020
add it in

4107
03:07:54,540 --> 03:08:00,600
to where we use

4108
03:07:58,020 --> 03:08:07,200
our current scrapey Dash request

4109
03:08:00,600 --> 03:08:12,120
function so we will have URL is equal to

4110
03:08:07,200 --> 03:08:14,040
and then get proxy URL and then the same

4111
03:08:12,120 --> 03:08:17,819
down here

4112
03:08:14,040 --> 03:08:22,200
we'll be doing get proxy URL

4113
03:08:17,819 --> 03:08:25,040
with our next page URL to and the only

4114
03:08:22,200 --> 03:08:26,899
other thing we need to add in now

4115
03:08:25,040 --> 03:08:31,979
is

4116
03:08:26,899 --> 03:08:33,779
a new function called start requests

4117
03:08:31,979 --> 03:08:35,880
so we'll add this in under our custom

4118
03:08:33,779 --> 03:08:38,660
settings

4119
03:08:35,880 --> 03:08:38,660
and

4120
03:08:39,000 --> 03:08:44,279
I'll explain now what this does so

4121
03:08:41,880 --> 03:08:46,439
scrapey looks for this function when you

4122
03:08:44,279 --> 03:08:49,740
start up your spider if you don't have

4123
03:08:46,439 --> 03:08:54,060
it it doesn't need it to run it'll work

4124
03:08:49,740 --> 03:08:58,020
off of your start URLs list here but if

4125
03:08:54,060 --> 03:09:00,240
you do have it it will go in and work

4126
03:08:58,020 --> 03:09:03,060
off of what you have in here

4127
03:09:00,240 --> 03:09:06,140
so what I've asked it to do is I'm

4128
03:09:03,060 --> 03:09:08,640
saying okay when the spider starts up

4129
03:09:06,140 --> 03:09:12,120
run this function

4130
03:09:08,640 --> 03:09:15,120
and inside this function run our guess

4131
03:09:12,120 --> 03:09:18,300
proxy URL the same as we do down here

4132
03:09:15,120 --> 03:09:21,000
because we want the very first URL to

4133
03:09:18,300 --> 03:09:24,540
also go to our proxy so if we didn't

4134
03:09:21,000 --> 03:09:27,600
have this function in here what would

4135
03:09:24,540 --> 03:09:30,779
happen is that the very first URL would

4136
03:09:27,600 --> 03:09:34,020
actually not be sent to our proxy

4137
03:09:30,779 --> 03:09:35,880
provider endpoint URL so that would mean

4138
03:09:34,020 --> 03:09:38,880
that there's a chance that the first

4139
03:09:35,880 --> 03:09:41,040
very first request would get blocked

4140
03:09:38,880 --> 03:09:44,240
so that's why we have this function so

4141
03:09:41,040 --> 03:09:47,340
that the very first URL is properly

4142
03:09:44,240 --> 03:09:50,580
encoded and sent off using this get

4143
03:09:47,340 --> 03:09:52,319
proxy URL and once so that's where we

4144
03:09:50,580 --> 03:09:56,279
have start URLs and we're taking the

4145
03:09:52,319 --> 03:09:58,859
first string inside of our start urls

4146
03:09:56,279 --> 03:10:01,080
and then we're doing the Callback is

4147
03:09:58,859 --> 03:10:03,380
going to be our parse function and then

4148
03:10:01,080 --> 03:10:06,840
it's going to go on and it's going to

4149
03:10:03,380 --> 03:10:08,939
work perfectly because it'll be going

4150
03:10:06,840 --> 03:10:09,960
through get proxy URL for the rest of

4151
03:10:08,939 --> 03:10:12,300
the

4152
03:10:09,960 --> 03:10:15,420
requests as well

4153
03:10:12,300 --> 03:10:19,500
so that takes care of the very first

4154
03:10:15,420 --> 03:10:22,140
call and this get proxy URL function get

4155
03:10:19,500 --> 03:10:25,620
takes care of making sure that all the

4156
03:10:22,140 --> 03:10:30,479
requests are going to go via this proxy

4157
03:10:25,620 --> 03:10:33,540
API endpoint so then the request will

4158
03:10:30,479 --> 03:10:36,180
come back with the response and the

4159
03:10:33,540 --> 03:10:40,319
response will be able to be parsed in

4160
03:10:36,180 --> 03:10:42,600
our parse book page like it was before

4161
03:10:40,319 --> 03:10:43,680
so we should be able to go ahead and run

4162
03:10:42,600 --> 03:10:46,560
that

4163
03:10:43,680 --> 03:10:49,080
if we just do scrapey crawl book Spider

4164
03:10:46,560 --> 03:10:50,939
again

4165
03:10:49,080 --> 03:10:53,700
and

4166
03:10:50,939 --> 03:10:56,220
run this

4167
03:10:53,700 --> 03:10:58,260
oh there's one other thing so it just

4168
03:10:56,220 --> 03:11:01,260
did one request and stopped straight

4169
03:10:58,260 --> 03:11:04,439
away and that's because it's a very easy

4170
03:11:01,260 --> 03:11:09,000
mistake to make the allowed domains does

4171
03:11:04,439 --> 03:11:12,300
not contain our proxy dot scrapups.io so

4172
03:11:09,000 --> 03:11:14,100
let's just add that in

4173
03:11:12,300 --> 03:11:18,060
and

4174
03:11:14,100 --> 03:11:22,220
if we rerun it it should

4175
03:11:18,060 --> 03:11:25,080
there you go so we just close our spider

4176
03:11:22,220 --> 03:11:26,760
and I'll show you that we have all the

4177
03:11:25,080 --> 03:11:29,040
data is coming through so we've got our

4178
03:11:26,760 --> 03:11:31,140
product type books

4179
03:11:29,040 --> 03:11:33,660
the title

4180
03:11:31,140 --> 03:11:36,300
the description it's all there

4181
03:11:33,660 --> 03:11:40,700
so that's working perfectly

4182
03:11:36,300 --> 03:11:43,439
it's going via our proxy API endpoint

4183
03:11:40,700 --> 03:11:45,840
and the next I want to do is I want to

4184
03:11:43,439 --> 03:11:48,420
show you guys how instead of integrating

4185
03:11:45,840 --> 03:11:50,340
this directly into our spider we can use

4186
03:11:48,420 --> 03:11:53,580
a proxy middleware

4187
03:11:50,340 --> 03:11:55,439
that's been created especially by scrape

4188
03:11:53,580 --> 03:11:59,520
UPS so we can just quickly pip install

4189
03:11:55,439 --> 03:12:01,319
it and it makes things a bit easier if

4190
03:11:59,520 --> 03:12:04,080
you are adding it to a project and you

4191
03:12:01,319 --> 03:12:07,140
don't want to have to add this special

4192
03:12:04,080 --> 03:12:10,920
get proxy URL function

4193
03:12:07,140 --> 03:12:15,960
so what we would do in this case is

4194
03:12:10,920 --> 03:12:19,560
we would just pip install and then our

4195
03:12:15,960 --> 03:12:25,819
new python module scrape up stash

4196
03:12:19,560 --> 03:12:25,819
scrapey Dash proxy SDK and install that

4197
03:12:25,979 --> 03:12:31,740
and then the next thing we would want to

4198
03:12:28,319 --> 03:12:34,500
do is we would go to our settings and

4199
03:12:31,740 --> 03:12:38,220
like we always do add more settings

4200
03:12:34,500 --> 03:12:40,560
so let's just go down here

4201
03:12:38,220 --> 03:12:43,140
and add in the settings we want so it

4202
03:12:40,560 --> 03:12:46,100
would be again our API key

4203
03:12:43,140 --> 03:12:49,620
if the scriptop's proxy is enabled yes

4204
03:12:46,100 --> 03:12:51,720
and we'd be adding this line to our

4205
03:12:49,620 --> 03:12:55,500
downloader middleware

4206
03:12:51,720 --> 03:12:59,660
so I already have downloader middlewares

4207
03:12:55,500 --> 03:12:59,660
so I'm just going to add it here

4208
03:12:59,880 --> 03:13:07,859
so if I save that

4209
03:13:03,200 --> 03:13:12,380
and I add my API key

4210
03:13:07,859 --> 03:13:12,380
from here into my settings

4211
03:13:12,840 --> 03:13:21,060
okay so I just wanted to hear perfect

4212
03:13:16,279 --> 03:13:25,279
and I should just remove

4213
03:13:21,060 --> 03:13:25,279
the get proxy URL

4214
03:13:25,979 --> 03:13:30,479
from the places that we've been using

4215
03:13:28,200 --> 03:13:33,020
that function because we don't need it

4216
03:13:30,479 --> 03:13:33,020
anymore

4217
03:13:33,600 --> 03:13:39,779
and then it should run fine going

4218
03:13:36,840 --> 03:13:42,060
through our

4219
03:13:39,779 --> 03:13:45,779
middleware

4220
03:13:42,060 --> 03:13:49,920
so let's try and run that one more time

4221
03:13:45,779 --> 03:13:53,600
and see does it work going by the scrape

4222
03:13:49,920 --> 03:13:53,600
UPS proxy middleware

4223
03:13:55,500 --> 03:14:01,500
and it looks like

4224
03:13:57,359 --> 03:14:04,859
you have lots of requests going through

4225
03:14:01,500 --> 03:14:08,160
so if I just cancel that

4226
03:14:04,859 --> 03:14:10,319
and we can check

4227
03:14:08,160 --> 03:14:13,040
in our

4228
03:14:10,319 --> 03:14:13,040
dashboard

4229
03:14:13,319 --> 03:14:20,040
if we have

4230
03:14:15,479 --> 03:14:23,180
so we have 123 requests so that looks

4231
03:14:20,040 --> 03:14:25,380
like they all went to our books to

4232
03:14:23,180 --> 03:14:28,380
scrape.com site

4233
03:14:25,380 --> 03:14:31,380
so we have 123 requests made so it's

4234
03:14:28,380 --> 03:14:33,479
working like it should

4235
03:14:31,380 --> 03:14:34,800
so that makes it very easy if you want

4236
03:14:33,479 --> 03:14:37,020
to get started with it all you need to

4237
03:14:34,800 --> 03:14:41,100
do is do that pip install

4238
03:14:37,020 --> 03:14:42,899
and scrape UPS Scrappy Dash proxy SDK

4239
03:14:41,100 --> 03:14:45,180
add to

4240
03:14:42,899 --> 03:14:47,880
the two lines in here and the one line

4241
03:14:45,180 --> 03:14:53,220
into your downloader middlewares

4242
03:14:47,880 --> 03:14:57,359
and then it will just send all your URLs

4243
03:14:53,220 --> 03:14:59,279
via the scrape UPS proxy endpoint

4244
03:14:57,359 --> 03:15:01,819
now obviously you can make your own

4245
03:14:59,279 --> 03:15:06,060
custom download or middleware as well

4246
03:15:01,819 --> 03:15:07,220
and like we did for our smart proxy

4247
03:15:06,060 --> 03:15:10,620
example

4248
03:15:07,220 --> 03:15:13,140
that might be a bit more complex than

4249
03:15:10,620 --> 03:15:15,600
needs be for for this because there's

4250
03:15:13,140 --> 03:15:17,520
already the middleware that you can just

4251
03:15:15,600 --> 03:15:21,180
pip install

4252
03:15:17,520 --> 03:15:24,660
so we will leave the example with our

4253
03:15:21,180 --> 03:15:26,340
downloader middleware in our article so

4254
03:15:24,660 --> 03:15:28,439
if you want to check out a long version

4255
03:15:26,340 --> 03:15:32,220
of how to implement the downloader

4256
03:15:28,439 --> 03:15:34,500
middleware using the scrape UPS proxy

4257
03:15:32,220 --> 03:15:36,600
API endpoint we'll have that in our

4258
03:15:34,500 --> 03:15:40,020
article and you can copy and paste that

4259
03:15:36,600 --> 03:15:43,380
into your code and play around with that

4260
03:15:40,020 --> 03:15:47,760
so there's just one other thing if you

4261
03:15:43,380 --> 03:15:50,819
wanted to add some more and

4262
03:15:47,760 --> 03:15:53,040
functionality to the scrape-ups proxy

4263
03:15:50,819 --> 03:15:56,279
endpoint you could add in

4264
03:15:53,040 --> 03:15:58,260
for example the following so scrape Ops

4265
03:15:56,279 --> 03:16:03,479
underscore proxy underscore settings is

4266
03:15:58,260 --> 03:16:05,760
equal to Country us so this would send

4267
03:16:03,479 --> 03:16:09,720
all the traffic

4268
03:16:05,760 --> 03:16:12,000
via US IP addresses so

4269
03:16:09,720 --> 03:16:13,740
if you're for example scraping an

4270
03:16:12,000 --> 03:16:16,859
e-commerce website that needed to be

4271
03:16:13,740 --> 03:16:19,439
only loaded via the US so we'd show us

4272
03:16:16,859 --> 03:16:21,000
items only you would do something like

4273
03:16:19,439 --> 03:16:24,000
this country

4274
03:16:21,000 --> 03:16:25,319
us in your scriptops underscore proxy

4275
03:16:24,000 --> 03:16:28,200
underscore settings

4276
03:16:25,319 --> 03:16:30,479
they also have other functions such as

4277
03:16:28,200 --> 03:16:33,180
you can pass in if you wanted to be the

4278
03:16:30,479 --> 03:16:35,700
page to be JavaScript rendered and

4279
03:16:33,180 --> 03:16:37,740
there's many other different parameters

4280
03:16:35,700 --> 03:16:40,080
that you can pass in which will mean

4281
03:16:37,740 --> 03:16:42,420
that certain things are switched on on

4282
03:16:40,080 --> 03:16:45,420
your proxy provider site

4283
03:16:42,420 --> 03:16:48,420
so that way instead of you having to do

4284
03:16:45,420 --> 03:16:50,220
all this stuff on your side the proxy

4285
03:16:48,420 --> 03:16:53,939
provider will take care of it as long as

4286
03:16:50,220 --> 03:16:56,580
you pass in the correct parameters

4287
03:16:53,939 --> 03:16:58,260
and each proxy provider will have their

4288
03:16:56,580 --> 03:16:59,939
own page with all the different

4289
03:16:58,260 --> 03:17:02,279
parameters that they allow you to pass

4290
03:16:59,939 --> 03:17:04,620
in to them

4291
03:17:02,279 --> 03:17:06,000
okay so that's everything I wanted to

4292
03:17:04,620 --> 03:17:07,800
cover in

4293
03:17:06,000 --> 03:17:11,540
part nine

4294
03:17:07,800 --> 03:17:16,080
in part 10 11 and 12 we'll be looking at

4295
03:17:11,540 --> 03:17:19,500
how you deploy so basically get your

4296
03:17:16,080 --> 03:17:20,520
spiders to run on a on a server in the

4297
03:17:19,500 --> 03:17:22,620
cloud

4298
03:17:20,520 --> 03:17:24,779
so how you deploy the code to your

4299
03:17:22,620 --> 03:17:29,040
server in the cloud and then how you can

4300
03:17:24,779 --> 03:17:31,920
schedule and run your spiders to scrape

4301
03:17:29,040 --> 03:17:34,740
at certain times of the day or the week

4302
03:17:31,920 --> 03:17:36,600
so you can collect data on a periodic

4303
03:17:34,740 --> 03:17:38,880
basis without having to have everything

4304
03:17:36,600 --> 03:17:40,319
running off of your home so we're going

4305
03:17:38,880 --> 03:17:42,660
to go through some different options

4306
03:17:40,319 --> 03:17:45,000
there what's available we're going to

4307
03:17:42,660 --> 03:17:48,779
look at some open sourced options free

4308
03:17:45,000 --> 03:17:52,439
options and paid options and just have a

4309
03:17:48,779 --> 03:17:53,760
look at the different uis and give you a

4310
03:17:52,439 --> 03:17:56,220
bunch of different options and we'll go

4311
03:17:53,760 --> 03:17:58,620
through the pros and cons of why you

4312
03:17:56,220 --> 03:18:01,979
should pick one service over another

4313
03:17:58,620 --> 03:18:04,140
service and we look at how complex and

4314
03:18:01,979 --> 03:18:06,300
easy they are to use as well so that's

4315
03:18:04,140 --> 03:18:09,240
what we'll be doing in the next three

4316
03:18:06,300 --> 03:18:13,460
sections okay

4317
03:18:09,240 --> 03:18:13,460
see you in the next part 10 News

4318
03:18:16,160 --> 03:18:22,140
so in part 10 we're going to look at the

4319
03:18:19,560 --> 03:18:25,080
different tools we can use to deploy and

4320
03:18:22,140 --> 03:18:28,080
schedule our spiders online and the

4321
03:18:25,080 --> 03:18:30,240
tools we can use to monitor how well our

4322
03:18:28,080 --> 03:18:32,939
jobs are doing how much data is being

4323
03:18:30,240 --> 03:18:35,279
scraped and if we're missing any Pages

4324
03:18:32,939 --> 03:18:37,500
or items when we are actually running

4325
03:18:35,279 --> 03:18:39,899
our spiders

4326
03:18:37,500 --> 03:18:42,840
so you might be asking yourselves what

4327
03:18:39,899 --> 03:18:45,779
is this deployment in scheduling so

4328
03:18:42,840 --> 03:18:49,020
deployment is basically us putting the

4329
03:18:45,779 --> 03:18:50,700
spider that we've just created onto a

4330
03:18:49,020 --> 03:18:52,920
server that's always going to be online

4331
03:18:50,700 --> 03:18:55,020
so that we don't have to have our own

4332
03:18:52,920 --> 03:18:58,200
machine our own laptop or computer

4333
03:18:55,020 --> 03:19:00,240
running 24 7 at home we can just put

4334
03:18:58,200 --> 03:19:02,580
that onto a virtual machine somewhere on

4335
03:19:00,240 --> 03:19:05,399
cloud and then we can actually schedule

4336
03:19:02,580 --> 03:19:07,500
it to run at a certain point of time

4337
03:19:05,399 --> 03:19:10,080
once a week once a day once every hour

4338
03:19:07,500 --> 03:19:13,859
once every minute however often we want

4339
03:19:10,080 --> 03:19:15,779
to run our spider to collect the data

4340
03:19:13,859 --> 03:19:19,200
so that's the deployment is the act of

4341
03:19:15,779 --> 03:19:22,439
getting the spider on the machine and

4342
03:19:19,200 --> 03:19:25,319
the scheduling is scheduling to run at a

4343
03:19:22,439 --> 03:19:27,960
certain time of day or time of week and

4344
03:19:25,319 --> 03:19:31,319
then the monitoring is just seeing how

4345
03:19:27,960 --> 03:19:33,479
well our scraping is doing either seeing

4346
03:19:31,319 --> 03:19:35,279
have the jobs actually completed did the

4347
03:19:33,479 --> 03:19:36,660
Spider Run correctly did the Spider Run

4348
03:19:35,279 --> 03:19:38,640
for the correct amount of time that we

4349
03:19:36,660 --> 03:19:41,520
thought it was going to run for did it

4350
03:19:38,640 --> 03:19:43,380
get all the pages that we thought it

4351
03:19:41,520 --> 03:19:45,479
should be getting obviously if you see

4352
03:19:43,380 --> 03:19:46,319
Zero Pages script you know there's an

4353
03:19:45,479 --> 03:19:48,660
issue

4354
03:19:46,319 --> 03:19:50,580
so that's where the monitoring comes in

4355
03:19:48,660 --> 03:19:52,859
and it's very important that we do have

4356
03:19:50,580 --> 03:19:55,080
some monitoring setup because obviously

4357
03:19:52,859 --> 03:19:57,600
if you don't your spider can be running

4358
03:19:55,080 --> 03:19:59,640
every day and you could be missing huge

4359
03:19:57,600 --> 03:20:01,920
amounts of data

4360
03:19:59,640 --> 03:20:04,680
so that's deployment scheduling and

4361
03:20:01,920 --> 03:20:06,300
Mantra that's the first part we're going

4362
03:20:04,680 --> 03:20:07,500
to do in this part 10 is we're just

4363
03:20:06,300 --> 03:20:09,479
going to look at the different tools

4364
03:20:07,500 --> 03:20:11,939
available we look at free tools open

4365
03:20:09,479 --> 03:20:14,700
source tools and paid tools

4366
03:20:11,939 --> 03:20:17,819
so the first is square PD which is free

4367
03:20:14,700 --> 03:20:20,880
and open sourced anyone can download it

4368
03:20:17,819 --> 03:20:24,000
and contribute to it as well on GitHub

4369
03:20:20,880 --> 03:20:25,979
so the pros of this are it's obviously

4370
03:20:24,000 --> 03:20:28,800
free and open source there's

4371
03:20:25,979 --> 03:20:31,680
plenty of third-party libraries for it

4372
03:20:28,800 --> 03:20:33,779
as well there is optional uis from

4373
03:20:31,680 --> 03:20:37,439
different providers

4374
03:20:33,779 --> 03:20:40,260
and the downsides to it are things like

4375
03:20:37,439 --> 03:20:41,880
you need your own server ideally because

4376
03:20:40,260 --> 03:20:44,520
if you're running it on your own

4377
03:20:41,880 --> 03:20:46,859
computer or laptop you would have to

4378
03:20:44,520 --> 03:20:49,439
have your computer laptop online at all

4379
03:20:46,859 --> 03:20:52,200
times if you want to have this for

4380
03:20:49,439 --> 03:20:53,880
example running something every day at a

4381
03:20:52,200 --> 03:20:55,800
set time

4382
03:20:53,880 --> 03:20:58,800
so it also doesn't actually have a

4383
03:20:55,800 --> 03:21:01,680
scheduler so some of the other tools we

4384
03:20:58,800 --> 03:21:03,960
look at such as using scraps or scrapey

4385
03:21:01,680 --> 03:21:08,100
Cloud there with those tools you can

4386
03:21:03,960 --> 03:21:10,620
actually set a scheduled job to run at a

4387
03:21:08,100 --> 03:21:12,720
specific time every day

4388
03:21:10,620 --> 03:21:16,800
but with Scrapy D you'd have to use this

4389
03:21:12,720 --> 03:21:19,439
Cron job to hit the API endpoint to get

4390
03:21:16,800 --> 03:21:22,620
scrapey to run your job at a specific

4391
03:21:19,439 --> 03:21:24,899
time so Scrapy D is good because it's

4392
03:21:22,620 --> 03:21:27,479
free and open sourced but the downside

4393
03:21:24,899 --> 03:21:29,460
is there's a bit more configuration it's

4394
03:21:27,479 --> 03:21:31,260
a bit harder to install but we'll show

4395
03:21:29,460 --> 03:21:32,520
you exactly how to install it if you

4396
03:21:31,260 --> 03:21:35,700
want to install it

4397
03:21:32,520 --> 03:21:38,399
so the second option just using scrape

4398
03:21:35,700 --> 03:21:42,180
Ops to deploy a schedule and monitor

4399
03:21:38,399 --> 03:21:45,600
your jobs the upsides of data is it's

4400
03:21:42,180 --> 03:21:48,899
got a good a UI interface to use

4401
03:21:45,600 --> 03:21:51,600
simple to use and understand it's got

4402
03:21:48,899 --> 03:21:54,540
built-in monitoring for your jobs and

4403
03:21:51,600 --> 03:21:56,100
spiders it's easy to schedule stuff but

4404
03:21:54,540 --> 03:21:59,640
the downside would be you'd need your

4405
03:21:56,100 --> 03:22:02,340
own server as well like with square PD

4406
03:21:59,640 --> 03:22:05,640
and the third option with scrapey cloud

4407
03:22:02,340 --> 03:22:07,859
is it's a paid service they have a

4408
03:22:05,640 --> 03:22:10,380
freemium kind of version so you can just

4409
03:22:07,859 --> 03:22:12,779
check it out if you want with that it's

4410
03:22:10,380 --> 03:22:16,380
easy to set up you can just download

4411
03:22:12,779 --> 03:22:19,500
their CLI tool use that to deploy your

4412
03:22:16,380 --> 03:22:21,479
spider into their Scrappy cloud service

4413
03:22:19,500 --> 03:22:24,180
and then once it's deployed there you

4414
03:22:21,479 --> 03:22:26,040
can quickly and easily run it and you

4415
03:22:24,180 --> 03:22:28,800
don't need to have your own server set

4416
03:22:26,040 --> 03:22:31,200
up with another third-party provider

4417
03:22:28,800 --> 03:22:32,220
so they're the three main options we're

4418
03:22:31,200 --> 03:22:33,779
going to look at

4419
03:22:32,220 --> 03:22:36,840
scrapeyd

4420
03:22:33,779 --> 03:22:39,600
scrape ups and Scrappy cloud

4421
03:22:36,840 --> 03:22:40,800
so let's first off have a look at

4422
03:22:39,600 --> 03:22:44,300
scrapeyd

4423
03:22:40,800 --> 03:22:46,740
and then we look at two different UI

4424
03:22:44,300 --> 03:22:48,479
dashboards that we can install so we

4425
03:22:46,740 --> 03:22:51,660
don't have to control everything using

4426
03:22:48,479 --> 03:22:53,580
their API endpoints because well that

4427
03:22:51,660 --> 03:22:55,979
can be useful for some people most

4428
03:22:53,580 --> 03:22:58,500
people want to interact with their

4429
03:22:55,979 --> 03:23:02,700
spiders and run them and schedule them

4430
03:22:58,500 --> 03:23:06,359
using a nice front-end UI

4431
03:23:02,700 --> 03:23:09,840
okay so Scrapy D is available to

4432
03:23:06,359 --> 03:23:13,020
download as I said we need a third-party

4433
03:23:09,840 --> 03:23:15,420
server set up first to install scrapeyd

4434
03:23:13,020 --> 03:23:19,020
on so we're going to go ahead and create

4435
03:23:15,420 --> 03:23:22,800
that with digitalocean now so

4436
03:23:19,020 --> 03:23:25,020
digitalocean is a server provider which

4437
03:23:22,800 --> 03:23:27,600
enables you to quickly set up virtual

4438
03:23:25,020 --> 03:23:30,479
machines and then install everything you

4439
03:23:27,600 --> 03:23:33,180
need on them so you can also use any

4440
03:23:30,479 --> 03:23:35,399
other VM provider such as

4441
03:23:33,180 --> 03:23:38,220
vulture for example these are another

4442
03:23:35,399 --> 03:23:40,739
good provider and they have very cheap

4443
03:23:38,220 --> 03:23:42,660
servers as well

4444
03:23:40,739 --> 03:23:45,540
so

4445
03:23:42,660 --> 03:23:48,180
if you go off and create your own

4446
03:23:45,540 --> 03:23:51,840
account with vulture or digitalocean or

4447
03:23:48,180 --> 03:23:54,060
AWS go off create your virtual machine

4448
03:23:51,840 --> 03:23:56,700
I'm going to do that right now with

4449
03:23:54,060 --> 03:23:58,560
digitalocean and if you want to use this

4450
03:23:56,700 --> 03:24:01,800
you can just follow the steps that I'm

4451
03:23:58,560 --> 03:24:04,680
using so you just log in

4452
03:24:01,800 --> 03:24:08,160
go up to the create click droplets

4453
03:24:04,680 --> 03:24:10,680
select the country or region you want to

4454
03:24:08,160 --> 03:24:13,979
select it usually works best when you

4455
03:24:10,680 --> 03:24:16,260
select a region that you're close to

4456
03:24:13,979 --> 03:24:19,920
select Ubuntu for the operating system

4457
03:24:16,260 --> 03:24:19,920
version 22.10

4458
03:24:20,340 --> 03:24:24,660
we can select

4459
03:24:21,960 --> 03:24:26,399
the cheapest virtual machine they have

4460
03:24:24,660 --> 03:24:29,120
available which if you just click basic

4461
03:24:26,399 --> 03:24:32,220
and then go to regular for the SSD type

4462
03:24:29,120 --> 03:24:34,020
and then they have a four dollar a month

4463
03:24:32,220 --> 03:24:36,720
server there

4464
03:24:34,020 --> 03:24:37,800
so once you've selected the server you

4465
03:24:36,720 --> 03:24:40,439
want

4466
03:24:37,800 --> 03:24:42,899
you can either add an SSH key or a

4467
03:24:40,439 --> 03:24:44,580
password to log in that's not that

4468
03:24:42,899 --> 03:24:48,180
important now because for this you can

4469
03:24:44,580 --> 03:24:49,859
also log in Via their console which can

4470
03:24:48,180 --> 03:24:51,000
be accessed via the browser so that's

4471
03:24:49,859 --> 03:24:53,580
how we're going to do everything now

4472
03:24:51,000 --> 03:24:55,140
we're just going to use the browser to

4473
03:24:53,580 --> 03:24:57,300
log into the machine and install

4474
03:24:55,140 --> 03:24:59,279
everything we need makes it very simple

4475
03:24:57,300 --> 03:25:01,920
and easy to use

4476
03:24:59,279 --> 03:25:04,380
so that's all we need to do

4477
03:25:01,920 --> 03:25:06,660
once we get to the bottom we just click

4478
03:25:04,380 --> 03:25:08,700
create droplet and that will go ahead

4479
03:25:06,660 --> 03:25:10,800
and create the droplet for us

4480
03:25:08,700 --> 03:25:13,680
the droplet is just their term for

4481
03:25:10,800 --> 03:25:15,239
virtual machine I can see it's

4482
03:25:13,680 --> 03:25:17,220
creating

4483
03:25:15,239 --> 03:25:19,920
so I just give it a minute or two to

4484
03:25:17,220 --> 03:25:22,080
finish creating and then we can access

4485
03:25:19,920 --> 03:25:24,479
the console over here

4486
03:25:22,080 --> 03:25:25,859
so as you can see the droplets being

4487
03:25:24,479 --> 03:25:28,620
created now

4488
03:25:25,859 --> 03:25:29,880
we can click the console button which

4489
03:25:28,620 --> 03:25:32,760
will open up

4490
03:25:29,880 --> 03:25:35,580
a new window for us where we can access

4491
03:25:32,760 --> 03:25:37,200
the console and type in all the

4492
03:25:35,580 --> 03:25:39,300
instructions to get everything installed

4493
03:25:37,200 --> 03:25:42,300
correctly

4494
03:25:39,300 --> 03:25:44,700
so as you can see we're logged in to the

4495
03:25:42,300 --> 03:25:46,979
virtual machine and now we can start

4496
03:25:44,700 --> 03:25:49,380
running our commands

4497
03:25:46,979 --> 03:25:53,399
so first things first is we want to run

4498
03:25:49,380 --> 03:25:55,439
sudo apt update that just updates all

4499
03:25:53,399 --> 03:25:58,500
the packages on the machine to make sure

4500
03:25:55,439 --> 03:26:02,420
everything we install will be the most

4501
03:25:58,500 --> 03:26:02,420
up-to-date versions of things

4502
03:26:02,520 --> 03:26:08,760
so we'll give that a second to run

4503
03:26:06,239 --> 03:26:11,760
that's finished and the next thing we

4504
03:26:08,760 --> 03:26:15,779
want is to install python pip so we can

4505
03:26:11,760 --> 03:26:17,700
pip install all the packages we need for

4506
03:26:15,779 --> 03:26:20,040
python

4507
03:26:17,700 --> 03:26:24,600
so that's just sudo

4508
03:26:20,040 --> 03:26:26,640
apt install Python 3 Dash pip

4509
03:26:24,600 --> 03:26:28,979
this Command right here

4510
03:26:26,640 --> 03:26:31,800
we'll have all these commands easily

4511
03:26:28,979 --> 03:26:34,920
available for you to copy and paste from

4512
03:26:31,800 --> 03:26:37,200
our article as well so you don't have to

4513
03:26:34,920 --> 03:26:39,000
be pausing the video at every point in

4514
03:26:37,200 --> 03:26:42,479
time

4515
03:26:39,000 --> 03:26:45,359
Okay so ill office you ask do you want

4516
03:26:42,479 --> 03:26:49,279
to install this x amount of space will

4517
03:26:45,359 --> 03:26:49,279
be used we're just going to say yes

4518
03:26:49,439 --> 03:26:53,939
and sometimes it'll ask as well to

4519
03:26:51,720 --> 03:26:55,620
restart certain Services which we can

4520
03:26:53,939 --> 03:26:58,319
also say yes to

4521
03:26:55,620 --> 03:27:00,540
so while that installs I'm just going to

4522
03:26:58,319 --> 03:27:04,439
show you the project that we're going to

4523
03:27:00,540 --> 03:27:08,460
be using so the project is the part 6

4524
03:27:04,439 --> 03:27:10,739
code so obviously you may or may not

4525
03:27:08,460 --> 03:27:13,460
have done that part if you haven't you

4526
03:27:10,739 --> 03:27:18,359
can just git clone our project from here

4527
03:27:13,460 --> 03:27:20,640
and just type git space clone space this

4528
03:27:18,359 --> 03:27:22,920
URL so that this is the project that

4529
03:27:20,640 --> 03:27:25,260
we're going to be using from part six of

4530
03:27:22,920 --> 03:27:27,899
this course

4531
03:27:25,260 --> 03:27:30,300
okay so let's see is

4532
03:27:27,899 --> 03:27:31,500
wait it's just asking us to restart the

4533
03:27:30,300 --> 03:27:34,560
services

4534
03:27:31,500 --> 03:27:37,620
and now we'll go ahead and get clone our

4535
03:27:34,560 --> 03:27:43,620
project so as I said it's just git clone

4536
03:27:37,620 --> 03:27:43,620
and then the free code Camp Dash part-6

4537
03:27:44,580 --> 03:27:51,140
so that's installed

4538
03:27:46,620 --> 03:27:51,140
we'll now just see the into our project

4539
03:27:51,439 --> 03:27:56,460
and we will install a virtual

4540
03:27:54,239 --> 03:28:00,120
environment

4541
03:27:56,460 --> 03:28:02,640
using pip install virtualenf

4542
03:28:00,120 --> 03:28:05,700
so that's after installing virtual EnV

4543
03:28:02,640 --> 03:28:09,479
now we can actually create our own

4544
03:28:05,700 --> 03:28:11,520
VN folder where all the python packages

4545
03:28:09,479 --> 03:28:15,720
can be installed into

4546
03:28:11,520 --> 03:28:18,420
so we'll do that with virtual ends vnf

4547
03:28:15,720 --> 03:28:19,800
and as you can see this folder has been

4548
03:28:18,420 --> 03:28:23,100
created

4549
03:28:19,800 --> 03:28:25,920
so now we just need to activate it

4550
03:28:23,100 --> 03:28:28,080
so we do that with Source vnf bin

4551
03:28:25,920 --> 03:28:29,880
activate

4552
03:28:28,080 --> 03:28:33,000
it's activated now

4553
03:28:29,880 --> 03:28:34,739
and now we can install the project

4554
03:28:33,000 --> 03:28:36,960
requirements

4555
03:28:34,739 --> 03:28:41,279
so

4556
03:28:36,960 --> 03:28:43,319
this requirements.txt file contains all

4557
03:28:41,279 --> 03:28:45,540
a list of all the things we need to get

4558
03:28:43,319 --> 03:28:49,279
this project running so we can just do

4559
03:28:45,540 --> 03:28:53,160
pip install space Dash r space

4560
03:28:49,279 --> 03:28:55,319
requirements.txt and it'll go ahead and

4561
03:28:53,160 --> 03:28:58,260
install all the

4562
03:28:55,319 --> 03:29:00,060
packages that are needed to run the

4563
03:28:58,260 --> 03:29:01,680
project correctly

4564
03:29:00,060 --> 03:29:03,180
so we just

4565
03:29:01,680 --> 03:29:05,279
give it a minute or two to install

4566
03:29:03,180 --> 03:29:08,100
everything and then we should be able to

4567
03:29:05,279 --> 03:29:11,640
run our scrippy spider

4568
03:29:08,100 --> 03:29:13,560
next thing we do we just CD into our

4569
03:29:11,640 --> 03:29:19,160
book scraper

4570
03:29:13,560 --> 03:29:19,160
and we'll see if we can run scrapey list

4571
03:29:19,380 --> 03:29:26,160
so that ran correctly

4572
03:29:22,560 --> 03:29:27,660
and now we can run scrapey crawl book

4573
03:29:26,160 --> 03:29:32,040
Spider

4574
03:29:27,660 --> 03:29:34,739
so if we run that

4575
03:29:32,040 --> 03:29:35,819
should see Scrapy starting up and as you

4576
03:29:34,739 --> 03:29:39,720
can see

4577
03:29:35,819 --> 03:29:41,939
all our pages are being scraped

4578
03:29:39,720 --> 03:29:43,859
so our pages are being scraped and the

4579
03:29:41,939 --> 03:29:47,520
data is being extracted from the page

4580
03:29:43,859 --> 03:29:50,100
just like we were doing in part six

4581
03:29:47,520 --> 03:29:52,620
so that's perfect we don't have to wait

4582
03:29:50,100 --> 03:29:55,439
for that to run and complete the next

4583
03:29:52,620 --> 03:29:57,840
thing we're going to look at is how we

4584
03:29:55,439 --> 03:30:01,739
can install scrape PD

4585
03:29:57,840 --> 03:30:04,880
okay so we just pip install scrapeyd

4586
03:30:01,739 --> 03:30:04,880
to install that

4587
03:30:06,479 --> 03:30:13,080
and then the next thing we can do is

4588
03:30:09,960 --> 03:30:16,680
just run scrapeyd so to do that it's

4589
03:30:13,080 --> 03:30:19,680
just Scrapy D now I've added on a bit

4590
03:30:16,680 --> 03:30:23,220
extra after it just Scrapy D because I

4591
03:30:19,680 --> 03:30:25,560
want all the output that usually gets

4592
03:30:23,220 --> 03:30:28,859
displayed to the screen to go into this

4593
03:30:25,560 --> 03:30:31,880
scrapeyd logs.txt file

4594
03:30:28,859 --> 03:30:35,220
so put all the logs into scripty

4595
03:30:31,880 --> 03:30:36,720
logs.txt and run this command in the

4596
03:30:35,220 --> 03:30:40,439
background

4597
03:30:36,720 --> 03:30:42,300
so we can go ahead and do that and now

4598
03:30:40,439 --> 03:30:45,239
let's check that it's actually up and

4599
03:30:42,300 --> 03:30:50,939
running so to do that we will be using

4600
03:30:45,239 --> 03:30:55,620
curl to Ping the Damon status dot Json

4601
03:30:50,939 --> 03:30:58,319
endpoint that lets us know if Scrappy D

4602
03:30:55,620 --> 03:30:59,700
is running correctly so when we run that

4603
03:30:58,319 --> 03:31:03,779
command

4604
03:30:59,700 --> 03:31:06,899
it says status is okay there is zero

4605
03:31:03,779 --> 03:31:09,239
jobs pending zero jobs running zero jobs

4606
03:31:06,899 --> 03:31:11,760
finished because obviously we've just

4607
03:31:09,239 --> 03:31:13,680
ran Square PD we haven't run any spiders

4608
03:31:11,760 --> 03:31:15,660
yet

4609
03:31:13,680 --> 03:31:19,380
we have scripted setup

4610
03:31:15,660 --> 03:31:21,540
we can hit the end point using curl

4611
03:31:19,380 --> 03:31:24,600
the next thing we want to do is we want

4612
03:31:21,540 --> 03:31:28,500
to package up our spider

4613
03:31:24,600 --> 03:31:31,260
and then deploy it to scrapeyd because

4614
03:31:28,500 --> 03:31:33,540
if we don't do that square PD will not

4615
03:31:31,260 --> 03:31:35,939
have access to our project and will not

4616
03:31:33,540 --> 03:31:39,540
be able to run the spider

4617
03:31:35,939 --> 03:31:40,800
so to do that we can install scrapeyd

4618
03:31:39,540 --> 03:31:43,800
client

4619
03:31:40,800 --> 03:31:44,760
so again using pip we just do pip

4620
03:31:43,800 --> 03:31:48,500
install

4621
03:31:44,760 --> 03:31:48,500
Scrapy D client

4622
03:31:48,720 --> 03:31:52,560
that will go off and install the

4623
03:31:50,580 --> 03:31:54,779
scrapeyd client

4624
03:31:52,560 --> 03:31:56,040
the next thing we need to do is we need

4625
03:31:54,779 --> 03:31:58,580
to

4626
03:31:56,040 --> 03:32:01,620
go into our

4627
03:31:58,580 --> 03:32:05,160
scrapey.cfg file

4628
03:32:01,620 --> 03:32:07,939
so that should be

4629
03:32:05,160 --> 03:32:10,859
here this guy

4630
03:32:07,939 --> 03:32:12,359
scrapey.cfg so we want to edit that we

4631
03:32:10,859 --> 03:32:16,080
can use

4632
03:32:12,359 --> 03:32:18,479
Vim or VI

4633
03:32:16,080 --> 03:32:21,080
and we can

4634
03:32:18,479 --> 03:32:24,239
all we need to do for this is just

4635
03:32:21,080 --> 03:32:25,859
uncomment out this line so that it

4636
03:32:24,239 --> 03:32:29,120
deploys it to

4637
03:32:25,859 --> 03:32:33,060
Scrapy D which is running on localhost

4638
03:32:29,120 --> 03:32:35,700
Port 6800

4639
03:32:33,060 --> 03:32:37,500
so it's handy it's already there all we

4640
03:32:35,700 --> 03:32:39,600
have to do is go in

4641
03:32:37,500 --> 03:32:42,239
comment that out

4642
03:32:39,600 --> 03:32:45,540
so to save this

4643
03:32:42,239 --> 03:32:48,300
we just I'll show you one second

4644
03:32:45,540 --> 03:32:51,479
we just type in

4645
03:32:48,300 --> 03:32:54,359
the double dots to get up

4646
03:32:51,479 --> 03:32:58,020
so we can actually save it correctly and

4647
03:32:54,359 --> 03:33:00,960
then WQ exclamation

4648
03:32:58,020 --> 03:33:02,040
enter and that saves it

4649
03:33:00,960 --> 03:33:03,899
so

4650
03:33:02,040 --> 03:33:08,540
now that it's saved

4651
03:33:03,899 --> 03:33:08,540
we can do a scrape Ed deploy

4652
03:33:09,600 --> 03:33:14,819
and default is the name I've just picked

4653
03:33:13,080 --> 03:33:18,300
for the project

4654
03:33:14,819 --> 03:33:19,800
because scrapeyd works with the concept

4655
03:33:18,300 --> 03:33:22,080
of projects

4656
03:33:19,800 --> 03:33:25,620
so it needs a project name and then as

4657
03:33:22,080 --> 03:33:27,720
you can see it's deployed okay

4658
03:33:25,620 --> 03:33:31,380
and it now has

4659
03:33:27,720 --> 03:33:35,220
one spider available so we can now

4660
03:33:31,380 --> 03:33:38,520
go ahead and run our spider right now

4661
03:33:35,220 --> 03:33:40,880
again using curl so curl it's going to

4662
03:33:38,520 --> 03:33:45,620
hit this localhost

4663
03:33:40,880 --> 03:33:48,000
6800 port and the forward slash

4664
03:33:45,620 --> 03:33:51,300
schedule.json endpoint

4665
03:33:48,000 --> 03:33:54,540
we're adding the project name of default

4666
03:33:51,300 --> 03:33:57,479
and the spider name of book Spider

4667
03:33:54,540 --> 03:33:59,160
so because we've deployed it we should

4668
03:33:57,479 --> 03:34:00,660
be able to run this

4669
03:33:59,160 --> 03:34:04,200
and

4670
03:34:00,660 --> 03:34:08,279
it comes back with a job ID if it has

4671
03:34:04,200 --> 03:34:10,680
run correctly so it said status okay and

4672
03:34:08,279 --> 03:34:14,040
we're giving you back a job ID to show

4673
03:34:10,680 --> 03:34:15,720
that the job has been started now this

4674
03:34:14,040 --> 03:34:19,140
doesn't mean that it finished running so

4675
03:34:15,720 --> 03:34:22,439
if there is ever issues sometimes you

4676
03:34:19,140 --> 03:34:24,060
need to do further investigation

4677
03:34:22,439 --> 03:34:27,060
so

4678
03:34:24,060 --> 03:34:31,620
we've gone through how to use Scrapy D

4679
03:34:27,060 --> 03:34:34,979
and Scrapy D deploy to package up and

4680
03:34:31,620 --> 03:34:38,040
deploy our spider to scrape Ed and then

4681
03:34:34,979 --> 03:34:42,479
how we can use Curl to schedule our

4682
03:34:38,040 --> 03:34:44,100
spider using the curl command followed

4683
03:34:42,479 --> 03:34:47,580
by the schedule

4684
03:34:44,100 --> 03:34:50,220
endpoint so obviously if you wanted to

4685
03:34:47,580 --> 03:34:54,600
just run this yourself you could just

4686
03:34:50,220 --> 03:34:57,479
set up a cron and using your crown you

4687
03:34:54,600 --> 03:35:00,420
could say schedule this to run every day

4688
03:34:57,479 --> 03:35:03,479
at whatever time you want and then you

4689
03:35:00,420 --> 03:35:06,359
would just be running this command

4690
03:35:03,479 --> 03:35:07,739
obviously we want to make this easier

4691
03:35:06,359 --> 03:35:10,260
for people to use

4692
03:35:07,739 --> 03:35:12,779
so now we're going to look at the two

4693
03:35:10,260 --> 03:35:15,120
dashboards that we can install

4694
03:35:12,779 --> 03:35:18,420
the first being Scrappy dweb

4695
03:35:15,120 --> 03:35:20,760
and The Following being the scrape pups

4696
03:35:18,420 --> 03:35:21,899
Scrapy D integration

4697
03:35:20,760 --> 03:35:25,200
so

4698
03:35:21,899 --> 03:35:28,439
for scrape edweb scripted web is also a

4699
03:35:25,200 --> 03:35:30,180
third party open sourced application

4700
03:35:28,439 --> 03:35:32,520
that you can install

4701
03:35:30,180 --> 03:35:34,680
and we're going to go ahead now and

4702
03:35:32,520 --> 03:35:38,399
install that

4703
03:35:34,680 --> 03:35:39,600
so I'm just going to go back up to the

4704
03:35:38,399 --> 03:35:41,819
top level

4705
03:35:39,600 --> 03:35:44,460
that we were at and

4706
03:35:41,819 --> 03:35:47,580
just pip install

4707
03:35:44,460 --> 03:35:49,739
and then the Scrapy d-web

4708
03:35:47,580 --> 03:35:52,220
so that's going to go ahead and install

4709
03:35:49,739 --> 03:35:52,220
that

4710
03:35:52,979 --> 03:35:57,720
now

4711
03:35:54,720 --> 03:36:00,960
it may need a certain

4712
03:35:57,720 --> 03:36:03,660
specific version of specific packages to

4713
03:36:00,960 --> 03:36:05,760
be installed so I've gone ahead and

4714
03:36:03,660 --> 03:36:08,279
found out that

4715
03:36:05,760 --> 03:36:10,380
when I was making this video that four

4716
03:36:08,279 --> 03:36:13,260
specific packages needs to be installed

4717
03:36:10,380 --> 03:36:14,760
with a specific version number are the

4718
03:36:13,260 --> 03:36:17,100
installation wouldn't go and work

4719
03:36:14,760 --> 03:36:18,840
correctly on the version that of Ubuntu

4720
03:36:17,100 --> 03:36:19,859
operating system that we're using right

4721
03:36:18,840 --> 03:36:22,920
now

4722
03:36:19,859 --> 03:36:24,540
so it's easy enough all we're going to

4723
03:36:22,920 --> 03:36:26,819
do is specify

4724
03:36:24,540 --> 03:36:28,319
for four different packages the version

4725
03:36:26,819 --> 03:36:31,380
that we need

4726
03:36:28,319 --> 03:36:33,239
so the first one is just flask SQL

4727
03:36:31,380 --> 03:36:34,859
Alchemy

4728
03:36:33,239 --> 03:36:36,960
so we're just going to pip install that

4729
03:36:34,859 --> 03:36:39,300
specific version

4730
03:36:36,960 --> 03:36:41,880
the next is

4731
03:36:39,300 --> 03:36:44,420
we're going to do pip install SQL

4732
03:36:41,880 --> 03:36:47,040
Alchemy

4733
03:36:44,420 --> 03:36:50,239
then we're just going to install a

4734
03:36:47,040 --> 03:36:50,239
specific version of flask

4735
03:36:50,700 --> 03:36:58,620
and finally we're going to pip install a

4736
03:36:55,020 --> 03:37:01,319
specific version of work zoic

4737
03:36:58,620 --> 03:37:04,500
once those are all installed we can now

4738
03:37:01,319 --> 03:37:06,899
check and see the scripted web

4739
03:37:04,500 --> 03:37:08,939
run correctly so we need to do is type

4740
03:37:06,899 --> 03:37:12,840
in scrippity web

4741
03:37:08,939 --> 03:37:15,660
okay it's giving us an issue is it

4742
03:37:12,840 --> 03:37:18,779
let's just try and rerun

4743
03:37:15,660 --> 03:37:24,180
scripted web again

4744
03:37:18,779 --> 03:37:27,359
yeah so this time it ran correctly so

4745
03:37:24,180 --> 03:37:30,600
I think it just needed to create the

4746
03:37:27,359 --> 03:37:33,060
settings file initially

4747
03:37:30,600 --> 03:37:36,739
so you can you know it's running

4748
03:37:33,060 --> 03:37:40,140
correctly when it stays up and you have

4749
03:37:36,739 --> 03:37:43,439
the URLs where you can access it showing

4750
03:37:40,140 --> 03:37:47,520
here so we can just go ahead

4751
03:37:43,439 --> 03:37:51,479
and copy the URL that it has given us

4752
03:37:47,520 --> 03:37:53,100
so this is the IP address of our server

4753
03:37:51,479 --> 03:37:56,819
which you can also get in the

4754
03:37:53,100 --> 03:38:01,319
digitalocean dashboard followed by 5000

4755
03:37:56,819 --> 03:38:05,580
which is the port that scrippy web is

4756
03:38:01,319 --> 03:38:07,859
running on if we copy that go up paste

4757
03:38:05,580 --> 03:38:09,840
that into a browser

4758
03:38:07,859 --> 03:38:12,960
we should see

4759
03:38:09,840 --> 03:38:15,060
the scrapery web dashboard showing up

4760
03:38:12,960 --> 03:38:16,680
correctly

4761
03:38:15,060 --> 03:38:20,100
so

4762
03:38:16,680 --> 03:38:21,540
we can see the jobs that I've already

4763
03:38:20,100 --> 03:38:23,819
run

4764
03:38:21,540 --> 03:38:25,920
so we can see the job that was run

4765
03:38:23,819 --> 03:38:28,859
earlier that we ran when we ran it

4766
03:38:25,920 --> 03:38:31,439
manually via the command line we had the

4767
03:38:28,859 --> 03:38:33,899
default project and the spider book

4768
03:38:31,439 --> 03:38:35,340
Spider and the job ID that was returned

4769
03:38:33,899 --> 03:38:36,540
to us

4770
03:38:35,340 --> 03:38:39,420
so

4771
03:38:36,540 --> 03:38:43,620
it doesn't have the pages and the items

4772
03:38:39,420 --> 03:38:46,800
because it needs this log parser

4773
03:38:43,620 --> 03:38:49,439
module to be installed so we're going to

4774
03:38:46,800 --> 03:38:52,859
go ahead install the log parser so we

4775
03:38:49,439 --> 03:38:56,279
can see the pages and items and more

4776
03:38:52,859 --> 03:38:59,520
statistics and we're also going to add

4777
03:38:56,279 --> 03:39:02,160
in a username and password just some

4778
03:38:59,520 --> 03:39:04,680
basic authentication right now anyone

4779
03:39:02,160 --> 03:39:07,439
can hit this endpoint and start running

4780
03:39:04,680 --> 03:39:09,420
my jobs I don't want that to be

4781
03:39:07,439 --> 03:39:12,479
happening and I'm pretty sure you guys

4782
03:39:09,420 --> 03:39:14,040
don't either so you're paying for the

4783
03:39:12,479 --> 03:39:16,020
server you don't want anyone to be able

4784
03:39:14,040 --> 03:39:18,540
to come on and start messing around with

4785
03:39:16,020 --> 03:39:20,100
your dashboard so we're going to quickly

4786
03:39:18,540 --> 03:39:22,200
go and do that now

4787
03:39:20,100 --> 03:39:25,080
so we'll start by just copying and

4788
03:39:22,200 --> 03:39:27,120
pasting this pip install log parser

4789
03:39:25,080 --> 03:39:30,060
command

4790
03:39:27,120 --> 03:39:32,760
I'm just going to shut down scrape

4791
03:39:30,060 --> 03:39:34,200
Beauty web for a second

4792
03:39:32,760 --> 03:39:38,120
so

4793
03:39:34,200 --> 03:39:38,120
run pip install log parser

4794
03:39:39,600 --> 03:39:45,840
then once that's installed I want to

4795
03:39:42,540 --> 03:39:47,819
edit my scriptd web settings

4796
03:39:45,840 --> 03:39:49,500
so I can use

4797
03:39:47,819 --> 03:39:55,260
VI again

4798
03:39:49,500 --> 03:39:57,779
and I'm going to first enable the off

4799
03:39:55,260 --> 03:39:59,880
so as you can see here it's currently

4800
03:39:57,779 --> 03:40:02,700
set to false

4801
03:39:59,880 --> 03:40:05,160
I'm going to set it to true

4802
03:40:02,700 --> 03:40:06,180
and then I'm going to set a username of

4803
03:40:05,160 --> 03:40:08,640
test

4804
03:40:06,180 --> 03:40:11,220
and a password of one two three four

4805
03:40:08,640 --> 03:40:13,439
five six seven eight obviously set

4806
03:40:11,220 --> 03:40:15,479
better username and passwords than that

4807
03:40:13,439 --> 03:40:17,640
please for your own projects

4808
03:40:15,479 --> 03:40:20,160
and your own servers the next thing we

4809
03:40:17,640 --> 03:40:21,359
need to do is come to our script e

4810
03:40:20,160 --> 03:40:24,000
servers list

4811
03:40:21,359 --> 03:40:25,920
and just comment out

4812
03:40:24,000 --> 03:40:28,620
this line here

4813
03:40:25,920 --> 03:40:33,420
because we don't have a server running

4814
03:40:28,620 --> 03:40:35,220
on Port 6801 we just have our Square PD

4815
03:40:33,420 --> 03:40:36,600
running on

4816
03:40:35,220 --> 03:40:40,319
this

4817
03:40:36,600 --> 03:40:42,300
Port 6800 on localhost

4818
03:40:40,319 --> 03:40:45,779
so once that's done

4819
03:40:42,300 --> 03:40:48,300
the next thing to do is to add in our

4820
03:40:45,779 --> 03:40:50,279
logs directory and to enable the log

4821
03:40:48,300 --> 03:40:53,100
parser here

4822
03:40:50,279 --> 03:40:55,200
so let's just enable the loud parser by

4823
03:40:53,100 --> 03:40:58,800
setting that to true

4824
03:40:55,200 --> 03:41:00,180
true and for us

4825
03:40:58,800 --> 03:41:03,739
the

4826
03:41:00,180 --> 03:41:03,739
directory is going to be

4827
03:41:04,979 --> 03:41:12,899
just root then the name of the project

4828
03:41:09,180 --> 03:41:15,720
frequent Camp part 6 and the name of

4829
03:41:12,899 --> 03:41:17,640
these spiders the folder containing the

4830
03:41:15,720 --> 03:41:20,220
spider which is book scraper and then

4831
03:41:17,640 --> 03:41:22,439
it's got a logs folder in there which is

4832
03:41:20,220 --> 03:41:24,300
where the log parser is going to read

4833
03:41:22,439 --> 03:41:26,040
the logs from

4834
03:41:24,300 --> 03:41:28,439
so obviously if you've got a different

4835
03:41:26,040 --> 03:41:30,300
project name and a different spider name

4836
03:41:28,439 --> 03:41:31,859
you need to just make sure that that is

4837
03:41:30,300 --> 03:41:34,080
correct

4838
03:41:31,859 --> 03:41:36,779
but it will always have a folder in

4839
03:41:34,080 --> 03:41:39,359
there with logs already so just find out

4840
03:41:36,779 --> 03:41:42,420
where your logs folder is and paste in

4841
03:41:39,359 --> 03:41:44,399
the directory here

4842
03:41:42,420 --> 03:41:48,000
and then the last thing we need to do is

4843
03:41:44,399 --> 03:41:50,040
just set our scripted server which is

4844
03:41:48,000 --> 03:41:51,380
just the default again which you can see

4845
03:41:50,040 --> 03:41:54,600
from above

4846
03:41:51,380 --> 03:41:58,380
127.0.0.1 is usually the default and

4847
03:41:54,600 --> 03:42:00,420
it's running on Port 6800 so now that we

4848
03:41:58,380 --> 03:42:01,800
have all that I'm just going to save the

4849
03:42:00,420 --> 03:42:04,680
file

4850
03:42:01,800 --> 03:42:08,100
and now that is saved

4851
03:42:04,680 --> 03:42:09,840
we should be able to run

4852
03:42:08,100 --> 03:42:12,779
scripted web again

4853
03:42:09,840 --> 03:42:15,840
this time I'm going to do like I did

4854
03:42:12,779 --> 03:42:18,000
with scrape Ed and get the logs to save

4855
03:42:15,840 --> 03:42:20,160
into a separate log file so they're not

4856
03:42:18,000 --> 03:42:21,660
coming up on the screen

4857
03:42:20,160 --> 03:42:22,859
and I'm going to run it on the

4858
03:42:21,660 --> 03:42:25,680
background

4859
03:42:22,859 --> 03:42:28,680
so that should be running we can check

4860
03:42:25,680 --> 03:42:34,880
if everything is running correctly using

4861
03:42:28,680 --> 03:42:34,880
the following using the sudo SS command

4862
03:42:35,040 --> 03:42:40,939
so we can see

4863
03:42:37,260 --> 03:42:43,739
that we have scrape PD running on Port

4864
03:42:40,939 --> 03:42:46,140
6800 of localhost

4865
03:42:43,739 --> 03:42:50,640
and we can see that Scrapy web is

4866
03:42:46,140 --> 03:42:52,920
running on localhost Port 5000.

4867
03:42:50,640 --> 03:42:55,080
so we can see both of them are running

4868
03:42:52,920 --> 03:42:56,520
we can see the ports they're running on

4869
03:42:55,080 --> 03:42:59,580
and we have

4870
03:42:56,520 --> 03:43:03,600
the process ID which we can use to kill

4871
03:42:59,580 --> 03:43:05,760
the process so if we wanted to stop one

4872
03:43:03,600 --> 03:43:07,140
of them from running for example I might

4873
03:43:05,760 --> 03:43:08,580
as well just stop Scrapy D from running

4874
03:43:07,140 --> 03:43:11,880
and show you guys

4875
03:43:08,580 --> 03:43:14,399
you can just type in Kill and then we

4876
03:43:11,880 --> 03:43:15,239
will get the scrappy D process ID from

4877
03:43:14,399 --> 03:43:19,319
here

4878
03:43:15,239 --> 03:43:20,100
and we will just paste that in and press

4879
03:43:19,319 --> 03:43:23,700
enter

4880
03:43:20,100 --> 03:43:27,180
and then you guys can see that

4881
03:43:23,700 --> 03:43:29,279
Scrapy D is no longer running so that is

4882
03:43:27,180 --> 03:43:32,300
how you can kill them if they're running

4883
03:43:29,279 --> 03:43:37,279
in the background as a process like that

4884
03:43:32,300 --> 03:43:37,279
so I'll just start up Scrappy D again

4885
03:43:39,479 --> 03:43:45,239
so if we check

4886
03:43:43,140 --> 03:43:46,979
we should see Scrapy D running and

4887
03:43:45,239 --> 03:43:50,819
scripted web running

4888
03:43:46,979 --> 03:43:53,479
perfect okay so because we killed and

4889
03:43:50,819 --> 03:43:56,819
restarted Scrapy D we need to just

4890
03:43:53,479 --> 03:43:59,640
re-deploy our project again using the

4891
03:43:56,819 --> 03:44:02,640
Scrapy D deploy because if we go back to

4892
03:43:59,640 --> 03:44:04,920
our Scrapy dashboard we won't be able to

4893
03:44:02,640 --> 03:44:06,600
see our spider

4894
03:44:04,920 --> 03:44:10,500
and we won't be able to run our spider

4895
03:44:06,600 --> 03:44:12,660
so we need to package up and redeploy

4896
03:44:10,500 --> 03:44:15,000
the project again

4897
03:44:12,660 --> 03:44:16,439
so we can just do that with square PD

4898
03:44:15,000 --> 03:44:19,080
deploy

4899
03:44:16,439 --> 03:44:20,819
so it's just Scrappy Dash deploy and

4900
03:44:19,080 --> 03:44:22,920
we're just picking the project name of

4901
03:44:20,819 --> 03:44:24,800
default again

4902
03:44:22,920 --> 03:44:29,340
so that'll just package up our spider

4903
03:44:24,800 --> 03:44:33,000
and add it again to scrippy D

4904
03:44:29,340 --> 03:44:35,340
so that's been added again and we can go

4905
03:44:33,000 --> 03:44:37,979
back to our endpoint now it asks us to

4906
03:44:35,340 --> 03:44:40,020
sign in so if I try and sign in it says

4907
03:44:37,979 --> 03:44:42,120
no you need to add in the username and

4908
03:44:40,020 --> 03:44:43,680
password so

4909
03:44:42,120 --> 03:44:46,140
I'll add in my username and password

4910
03:44:43,680 --> 03:44:48,540
that I set in the config file

4911
03:44:46,140 --> 03:44:52,340
and sign in

4912
03:44:48,540 --> 03:44:52,340
if I go to run spider

4913
03:44:54,720 --> 03:45:02,580
we can then see the default

4914
03:45:00,120 --> 03:45:04,380
default latest version

4915
03:45:02,580 --> 03:45:06,840
book Spider

4916
03:45:04,380 --> 03:45:09,060
if you need to set any specific settings

4917
03:45:06,840 --> 03:45:12,420
and Arguments for your project you can

4918
03:45:09,060 --> 03:45:15,660
do it there if you want to set it to run

4919
03:45:12,420 --> 03:45:18,899
at a specific day of the week

4920
03:45:15,660 --> 03:45:21,660
hour our minute you can do it here

4921
03:45:18,899 --> 03:45:23,460
so I'm not going to set it for a

4922
03:45:21,660 --> 03:45:25,620
specific time in the future I want it to

4923
03:45:23,460 --> 03:45:28,620
run right now the next thing I need to

4924
03:45:25,620 --> 03:45:30,899
do is Click just check command

4925
03:45:28,620 --> 03:45:33,720
that will paste in a default command

4926
03:45:30,899 --> 03:45:35,580
which as you can see here is just going

4927
03:45:33,720 --> 03:45:39,540
to do the curl

4928
03:45:35,580 --> 03:45:41,640
to the endpoint that we did earlier so

4929
03:45:39,540 --> 03:45:44,340
all scripted web is doing is running

4930
03:45:41,640 --> 03:45:46,560
this command so everything is correctly

4931
03:45:44,340 --> 03:45:49,680
set the project the version The Spider

4932
03:45:46,560 --> 03:45:51,540
and we just want it to run switch click

4933
03:45:49,680 --> 03:45:54,600
run spider and that's going to go off

4934
03:45:51,540 --> 03:45:57,200
and kick off our s our job and it's

4935
03:45:54,600 --> 03:45:57,200
going to start running

4936
03:45:57,600 --> 03:46:04,620
so that should be running now and we

4937
03:46:02,340 --> 03:46:07,140
should soon be able to see the

4938
03:46:04,620 --> 03:46:08,819
statistics coming back for a number of

4939
03:46:07,140 --> 03:46:10,859
pages and items

4940
03:46:08,819 --> 03:46:12,420
so let's give that a minute or two to

4941
03:46:10,859 --> 03:46:16,020
run

4942
03:46:12,420 --> 03:46:19,739
so as you can see it has finished

4943
03:46:16,020 --> 03:46:22,859
and it took 24 seconds to run and you

4944
03:46:19,739 --> 03:46:24,660
can see some other

4945
03:46:22,859 --> 03:46:27,180
stats and pieces like that now we're

4946
03:46:24,660 --> 03:46:29,819
still missing the pages and items and

4947
03:46:27,180 --> 03:46:32,460
this sign is still up here saying that

4948
03:46:29,819 --> 03:46:34,439
we need to install loud parser so I

4949
03:46:32,460 --> 03:46:37,560
think I might have actually put in the

4950
03:46:34,439 --> 03:46:40,800
incorrect path to where the logs are

4951
03:46:37,560 --> 03:46:44,340
stored so let's fix that and then we can

4952
03:46:40,800 --> 03:46:45,960
show you how the pages and items show up

4953
03:46:44,340 --> 03:46:51,420
so if we go back

4954
03:46:45,960 --> 03:46:54,180
I've discovered that I need to change my

4955
03:46:51,420 --> 03:46:58,460
path in the scripty web settings

4956
03:46:54,180 --> 03:46:58,460
so I can open up my scripty web settings

4957
03:46:58,680 --> 03:47:02,120
so here you go so

4958
03:47:02,279 --> 03:47:08,580
I think we just need to fix this so as I

4959
03:47:06,720 --> 03:47:11,000
said depending on your project you just

4960
03:47:08,580 --> 03:47:15,060
need to find where the log folder is

4961
03:47:11,000 --> 03:47:17,220
kept so I think that should do it it's

4962
03:47:15,060 --> 03:47:20,399
just forward slash root forward slash

4963
03:47:17,220 --> 03:47:23,220
Frankel cam part six and then locks so

4964
03:47:20,399 --> 03:47:27,620
I'm just going to save that

4965
03:47:23,220 --> 03:47:31,500
and I'm going to restart

4966
03:47:27,620 --> 03:47:33,420
scrivity web unless it's after fixing

4967
03:47:31,500 --> 03:47:37,439
itself but I think we need to restart it

4968
03:47:33,420 --> 03:47:40,640
yep so just go back

4969
03:47:37,439 --> 03:47:44,160
run the sudo

4970
03:47:40,640 --> 03:47:46,979
ss-t-u-n-l-p command again and we will

4971
03:47:44,160 --> 03:47:49,560
kill our scripty web

4972
03:47:46,979 --> 03:47:53,880
using the kill command

4973
03:47:49,560 --> 03:47:57,479
and we'll run scriptyweb again

4974
03:47:53,880 --> 03:48:01,500
so yep so as you can see

4975
03:47:57,479 --> 03:48:04,859
the log parser ran eight seconds ago

4976
03:48:01,500 --> 03:48:06,359
and it was last updated

4977
03:48:04,859 --> 03:48:10,620
at this time

4978
03:48:06,359 --> 03:48:12,420
so if we re-run our

4979
03:48:10,620 --> 03:48:15,720
spider

4980
03:48:12,420 --> 03:48:17,880
we're going back to run spider

4981
03:48:15,720 --> 03:48:20,460
and

4982
03:48:17,880 --> 03:48:23,160
we'll put the default project the latest

4983
03:48:20,460 --> 03:48:26,939
version book Spider put in the command

4984
03:48:23,160 --> 03:48:29,160
run spider and it'll run again

4985
03:48:26,939 --> 03:48:33,239
in the meantime it's gone through and

4986
03:48:29,160 --> 03:48:35,160
it's parsed the last logs that we ran a

4987
03:48:33,239 --> 03:48:37,140
couple of minutes ago and you can see

4988
03:48:35,160 --> 03:48:40,920
the pages and items have actually been

4989
03:48:37,140 --> 03:48:44,279
populated here so it scraped 1051 pages

4990
03:48:40,920 --> 03:48:46,739
and there was a thousand items and we

4991
03:48:44,279 --> 03:48:48,420
can see some more stats by clicking the

4992
03:48:46,739 --> 03:48:50,580
stats button

4993
03:48:48,420 --> 03:48:53,700
so you can see

4994
03:48:50,580 --> 03:48:56,220
what else to have warnings and errors so

4995
03:48:53,700 --> 03:48:59,580
there's one warning and you can see the

4996
03:48:56,220 --> 03:49:02,880
latest item that was scraped and as we

4997
03:48:59,580 --> 03:49:05,640
can see it was this URL

4998
03:49:02,880 --> 03:49:08,760
this was the book title

4999
03:49:05,640 --> 03:49:11,340
we can see the price tax and all the

5000
03:49:08,760 --> 03:49:15,600
other stuff that we've already selected

5001
03:49:11,340 --> 03:49:18,359
in part five and six so

5002
03:49:15,600 --> 03:49:21,060
this is kind of the basics of scribdy

5003
03:49:18,359 --> 03:49:23,700
web and how you would install the

5004
03:49:21,060 --> 03:49:25,140
scribby web dashboard to work with

5005
03:49:23,700 --> 03:49:27,060
scrapyt

5006
03:49:25,140 --> 03:49:29,939
obviously you can also

5007
03:49:27,060 --> 03:49:32,040
see the full logs

5008
03:49:29,939 --> 03:49:33,660
and you know

5009
03:49:32,040 --> 03:49:37,080
if you need to

5010
03:49:33,660 --> 03:49:38,760
see exactly a specific error or you need

5011
03:49:37,080 --> 03:49:41,520
to nail down further into the logs

5012
03:49:38,760 --> 03:49:42,779
you've got the full logs available there

5013
03:49:41,520 --> 03:49:45,899
as well

5014
03:49:42,779 --> 03:49:48,899
and it's picked out the warnings and if

5015
03:49:45,899 --> 03:49:51,899
there was any errors it would show them

5016
03:49:48,899 --> 03:49:54,239
as well so it's very handy it's free

5017
03:49:51,899 --> 03:49:55,380
it's open sourced you can install it

5018
03:49:54,239 --> 03:49:57,720
yourself

5019
03:49:55,380 --> 03:50:00,540
and as you can see there's a bit of

5020
03:49:57,720 --> 03:50:02,880
configuring in the settings and there's

5021
03:50:00,540 --> 03:50:05,160
a little bit more knowledge required

5022
03:50:02,880 --> 03:50:07,620
into okay you need to deploy this

5023
03:50:05,160 --> 03:50:09,060
project like this you need to run the

5024
03:50:07,620 --> 03:50:12,359
spider like that

5025
03:50:09,060 --> 03:50:15,060
there is some help pieces as well

5026
03:50:12,359 --> 03:50:17,160
available for example if you forget how

5027
03:50:15,060 --> 03:50:20,279
to deploy it they do have a help section

5028
03:50:17,160 --> 03:50:23,580
here and you can

5029
03:50:20,279 --> 03:50:27,420
follow the instructions as to oh how do

5030
03:50:23,580 --> 03:50:29,520
I deploy my project so that scripty web

5031
03:50:27,420 --> 03:50:31,439
and Scrappy D can use it again they've

5032
03:50:29,520 --> 03:50:34,319
got the commands you need to run here

5033
03:50:31,439 --> 03:50:38,520
and the steps you need to follow are you

5034
03:50:34,319 --> 03:50:41,040
can put in your project directory and

5035
03:50:38,520 --> 03:50:44,399
they can Auto package it up

5036
03:50:41,040 --> 03:50:47,819
using scriptyweb as well so

5037
03:50:44,399 --> 03:50:50,460
the next dashboard we look at is the

5038
03:50:47,819 --> 03:50:51,899
scrape UPS integration with scrapeyd so

5039
03:50:50,460 --> 03:50:54,840
there's two different dashboards for

5040
03:50:51,899 --> 03:50:56,279
scrapeyd Scrapy web and the scraps

5041
03:50:54,840 --> 03:50:57,300
dashboard

5042
03:50:56,279 --> 03:50:59,880
so

5043
03:50:57,300 --> 03:51:03,120
for that you need to go off and create a

5044
03:50:59,880 --> 03:51:04,680
scrape UPS account so you can just go to

5045
03:51:03,120 --> 03:51:08,340
scrapeups.io

5046
03:51:04,680 --> 03:51:10,080
and if you've been using it for any of

5047
03:51:08,340 --> 03:51:13,319
the other parts we've gone through

5048
03:51:10,080 --> 03:51:15,420
already in our course you can use the

5049
03:51:13,319 --> 03:51:17,640
existing API key you have if you're just

5050
03:51:15,420 --> 03:51:20,520
joining us for this section you can sign

5051
03:51:17,640 --> 03:51:21,960
up register for free and get your own

5052
03:51:20,520 --> 03:51:25,560
API key

5053
03:51:21,960 --> 03:51:28,800
so you will just need the API key

5054
03:51:25,560 --> 03:51:31,620
and then you will just need to follow

5055
03:51:28,800 --> 03:51:33,660
the monitoring steps so

5056
03:51:31,620 --> 03:51:34,680
we'll just click monitoring it should be

5057
03:51:33,660 --> 03:51:37,620
scrapey

5058
03:51:34,680 --> 03:51:40,220
so I need to do pip install scrape up

5059
03:51:37,620 --> 03:51:40,220
scrapey

5060
03:51:40,680 --> 03:51:46,100
and we'll do pip install scrape up stash

5061
03:51:43,560 --> 03:51:46,100
scrapey

5062
03:51:46,920 --> 03:51:54,239
so we've installed the scrape off SDK we

5063
03:51:52,020 --> 03:51:57,000
just need to add our API key to the

5064
03:51:54,239 --> 03:51:58,380
scrapey project settings

5065
03:51:57,000 --> 03:52:01,560
so

5066
03:51:58,380 --> 03:52:05,340
I can copy this line

5067
03:52:01,560 --> 03:52:08,279
go now into

5068
03:52:05,340 --> 03:52:10,260
my folder

5069
03:52:08,279 --> 03:52:14,880
and

5070
03:52:10,260 --> 03:52:17,760
I want to edit my settings.py file

5071
03:52:14,880 --> 03:52:20,580
and I'm going to just add

5072
03:52:17,760 --> 03:52:22,439
the API key in

5073
03:52:20,580 --> 03:52:26,939
here

5074
03:52:22,439 --> 03:52:28,560
and check what else I need to install I

5075
03:52:26,939 --> 03:52:32,819
also need to install

5076
03:52:28,560 --> 03:52:34,140
the extension and the downloader

5077
03:52:32,819 --> 03:52:35,880
middlewares

5078
03:52:34,140 --> 03:52:37,859
so

5079
03:52:35,880 --> 03:52:40,020
go

5080
03:52:37,859 --> 03:52:42,660
down to my

5081
03:52:40,020 --> 03:52:44,100
downloader middlewares

5082
03:52:42,660 --> 03:52:48,239
and

5083
03:52:44,100 --> 03:52:49,439
I'll add those in here

5084
03:52:48,239 --> 03:52:52,319
so

5085
03:52:49,439 --> 03:52:54,180
copy that

5086
03:52:52,319 --> 03:52:55,920
paste that in

5087
03:52:54,180 --> 03:52:58,500
and

5088
03:52:55,920 --> 03:53:01,020
to the extensions

5089
03:52:58,500 --> 03:53:04,220
I'll just put that under the existing

5090
03:53:01,020 --> 03:53:04,220
extensions there

5091
03:53:04,500 --> 03:53:06,739
so

5092
03:53:09,540 --> 03:53:14,040
paste that in as well so we've got the

5093
03:53:11,640 --> 03:53:17,700
download the Download Remember words the

5094
03:53:14,040 --> 03:53:21,680
extension and the API key

5095
03:53:17,700 --> 03:53:21,680
so I should be able to now save that

5096
03:53:22,560 --> 03:53:25,340
and

5097
03:53:25,800 --> 03:53:30,680
just check that there's nothing else to

5098
03:53:28,380 --> 03:53:30,680
do there

5099
03:53:31,560 --> 03:53:36,779
I think that's all correct so that's to

5100
03:53:34,920 --> 03:53:39,840
install the monitoring so everything

5101
03:53:36,779 --> 03:53:43,279
will show up in the dashboard but we now

5102
03:53:39,840 --> 03:53:46,620
want to install the actual

5103
03:53:43,279 --> 03:53:48,960
scheduling side of things so for that we

5104
03:53:46,620 --> 03:53:52,439
will go to the

5105
03:53:48,960 --> 03:53:54,479
servers and deployments section

5106
03:53:52,439 --> 03:53:57,660
and we will

5107
03:53:54,479 --> 03:54:01,260
add a new Scrapy d

5108
03:53:57,660 --> 03:54:02,220
server so we just need the name of our

5109
03:54:01,260 --> 03:54:04,680
server

5110
03:54:02,220 --> 03:54:06,779
we just do test

5111
03:54:04,680 --> 03:54:08,939
obviously you can name your server

5112
03:54:06,779 --> 03:54:10,439
whatever you want we need the server IP

5113
03:54:08,939 --> 03:54:12,899
address so

5114
03:54:10,439 --> 03:54:15,479
we will go to

5115
03:54:12,899 --> 03:54:18,479
our digitalocean

5116
03:54:15,479 --> 03:54:23,640
dashboard and copy the IP address here

5117
03:54:18,479 --> 03:54:26,160
so the ipv4 so copy that

5118
03:54:23,640 --> 03:54:28,260
and

5119
03:54:26,160 --> 03:54:32,000
just need to paste that in

5120
03:54:28,260 --> 03:54:32,000
and save the details

5121
03:54:32,220 --> 03:54:36,840
and then

5122
03:54:34,020 --> 03:54:39,720
it's saying you should sh into your

5123
03:54:36,840 --> 03:54:42,359
server we're already in on our console

5124
03:54:39,720 --> 03:54:46,020
and it says run the command in your

5125
03:54:42,359 --> 03:54:49,199
terminal so we will copy this command

5126
03:54:46,020 --> 03:54:52,739
and go back to our console

5127
03:54:49,199 --> 03:54:56,160
and I'll just go back up to

5128
03:54:52,739 --> 03:54:59,160
the top

5129
03:54:56,160 --> 03:55:02,040
and paste that in

5130
03:54:59,160 --> 03:55:04,319
it's installing

5131
03:55:02,040 --> 03:55:08,399
everything it needs it might need to

5132
03:55:04,319 --> 03:55:10,160
restart some Services again we can say

5133
03:55:08,399 --> 03:55:15,359
yes to that

5134
03:55:10,160 --> 03:55:18,479
once it's finished we will say yes to

5135
03:55:15,359 --> 03:55:19,979
that and okay so everything seems to be

5136
03:55:18,479 --> 03:55:22,380
finished

5137
03:55:19,979 --> 03:55:24,720
so we can now

5138
03:55:22,380 --> 03:55:27,779
go back and check our

5139
03:55:24,720 --> 03:55:31,439
servers list and we can see our server

5140
03:55:27,779 --> 03:55:34,020
name is there and it's connected

5141
03:55:31,439 --> 03:55:37,020
and we can

5142
03:55:34,020 --> 03:55:40,020
check our server perfect so if we need

5143
03:55:37,020 --> 03:55:41,640
to edit or delete the details there and

5144
03:55:40,020 --> 03:55:43,500
it says your server is now set up

5145
03:55:41,640 --> 03:55:47,220
correctly you can schedule your jobs on

5146
03:55:43,500 --> 03:55:51,000
the scheduler page here I'll click that

5147
03:55:47,220 --> 03:55:53,939
and click schedule job

5148
03:55:51,000 --> 03:55:57,120
and I have my server name

5149
03:55:53,939 --> 03:56:01,020
I've got my spider book Spider

5150
03:55:57,120 --> 03:56:03,540
I will run the spider now and I can

5151
03:56:01,020 --> 03:56:07,199
click if I wanted to run it every

5152
03:56:03,540 --> 03:56:10,260
month every day all I can select

5153
03:56:07,199 --> 03:56:12,600
you know a specific month or whatever

5154
03:56:10,260 --> 03:56:16,739
we'll do that in a second

5155
03:56:12,600 --> 03:56:18,720
for now I just want to run it now and I

5156
03:56:16,739 --> 03:56:21,359
don't have any settings and arguments to

5157
03:56:18,720 --> 03:56:22,920
add in so I can submit the job and the

5158
03:56:21,359 --> 03:56:24,359
job is scheduled

5159
03:56:22,920 --> 03:56:28,439
so

5160
03:56:24,359 --> 03:56:32,000
in a few seconds that should

5161
03:56:28,439 --> 03:56:32,000
show up in the jobs list

5162
03:56:32,220 --> 03:56:42,239
in the meantime let's go and schedule

5163
03:56:36,060 --> 03:56:44,040
a job to run let's say once a week

5164
03:56:42,239 --> 03:56:46,020
so let's say

5165
03:56:44,040 --> 03:56:48,300
every Monday

5166
03:56:46,020 --> 03:56:50,819
at

5167
03:56:48,300 --> 03:56:54,479
7am

5168
03:56:50,819 --> 03:56:57,300
and obviously that's in UTC time zone

5169
03:56:54,479 --> 03:56:58,680
crons are usually run in UTC

5170
03:56:57,300 --> 03:57:00,840
so you need to make sure that that

5171
03:56:58,680 --> 03:57:03,060
corresponds to your own time zone

5172
03:57:00,840 --> 03:57:05,100
correctly so we're saying every Monday

5173
03:57:03,060 --> 03:57:08,520
at 7am

5174
03:57:05,100 --> 03:57:11,279
please run the book spider spider

5175
03:57:08,520 --> 03:57:14,220
on the test server

5176
03:57:11,279 --> 03:57:18,540
and if we submit that

5177
03:57:14,220 --> 03:57:21,840
it should then show up in our list so

5178
03:57:18,540 --> 03:57:24,840
there you go at 7am only on Mondays

5179
03:57:21,840 --> 03:57:27,600
it'll run that's very useful if you need

5180
03:57:24,840 --> 03:57:30,239
to set up your spider to run every day

5181
03:57:27,600 --> 03:57:32,460
every hour or whatever you can

5182
03:57:30,239 --> 03:57:34,560
View and edit them here and you can

5183
03:57:32,460 --> 03:57:37,979
enable them disable them

5184
03:57:34,560 --> 03:57:40,080
so if we go back to our dashboard

5185
03:57:37,979 --> 03:57:41,220
we don't have any details coming through

5186
03:57:40,080 --> 03:57:42,660
yet

5187
03:57:41,220 --> 03:57:45,120
there might have been an issue with

5188
03:57:42,660 --> 03:57:48,120
running it as well

5189
03:57:45,120 --> 03:57:50,520
but we'll explore that now so when there

5190
03:57:48,120 --> 03:57:51,960
is an issue like this where we can't see

5191
03:57:50,520 --> 03:57:54,859
any data coming into a dashboard the

5192
03:57:51,960 --> 03:57:58,439
best thing to do is to try and just run

5193
03:57:54,859 --> 03:58:02,520
the scrapey list are the square B crawl

5194
03:57:58,439 --> 03:58:04,939
command manually from our server so I'm

5195
03:58:02,520 --> 03:58:11,399
just going to go back to

5196
03:58:04,939 --> 03:58:14,220
the console and go back into the project

5197
03:58:11,399 --> 03:58:16,020
and

5198
03:58:14,220 --> 03:58:18,960
from inside

5199
03:58:16,020 --> 03:58:22,199
the free code Camp part 6 book scraper

5200
03:58:18,960 --> 03:58:23,880
folder I'm going to run a scrapey list

5201
03:58:22,199 --> 03:58:26,279
first

5202
03:58:23,880 --> 03:58:28,260
and if there's an issue with scrapey or

5203
03:58:26,279 --> 03:58:31,080
there is an issue with the settings

5204
03:58:28,260 --> 03:58:32,220
it'll show up here

5205
03:58:31,080 --> 03:58:33,540
so

5206
03:58:32,220 --> 03:58:35,220
Scrappy list

5207
03:58:33,540 --> 03:58:37,500
and we have an error

5208
03:58:35,220 --> 03:58:40,920
escape the error and scrape is saying

5209
03:58:37,500 --> 03:58:42,359
that there is an indentation error in

5210
03:58:40,920 --> 03:58:44,760
the downloader middleware so that's

5211
03:58:42,359 --> 03:58:47,040
probably what's causing the issue

5212
03:58:44,760 --> 03:58:48,840
so we can just

5213
03:58:47,040 --> 03:58:50,819
uh

5214
03:58:48,840 --> 03:58:53,939
edit that

5215
03:58:50,819 --> 03:58:57,180
so open up our settings again

5216
03:58:53,939 --> 03:58:59,939
and check

5217
03:58:57,180 --> 03:59:01,439
it's a download middlewares so I think

5218
03:58:59,939 --> 03:59:06,420
it's just

5219
03:59:01,439 --> 03:59:11,760
that there is a space here

5220
03:59:06,420 --> 03:59:14,520
and in case it's also this guy I'll just

5221
03:59:11,760 --> 03:59:16,500
remove the indentations there and do the

5222
03:59:14,520 --> 03:59:18,720
same for the extensions and then

5223
03:59:16,500 --> 03:59:21,600
hopefully

5224
03:59:18,720 --> 03:59:24,359
there's no more issues

5225
03:59:21,600 --> 03:59:25,739
so save that and try run scripture list

5226
03:59:24,359 --> 03:59:28,140
again

5227
03:59:25,739 --> 03:59:33,420
at this time it worked and book spider

5228
03:59:28,140 --> 03:59:35,580
is returned perfect so if we go back to

5229
03:59:33,420 --> 03:59:39,300
our scrapups dashboard

5230
03:59:35,580 --> 03:59:41,580
and try and run that again

5231
03:59:39,300 --> 03:59:42,840
so it's got the server and splatter we

5232
03:59:41,580 --> 03:59:46,100
need selected

5233
03:59:42,840 --> 03:59:46,100
and we can submit job

5234
03:59:46,739 --> 03:59:51,120
so this time

5235
03:59:48,420 --> 03:59:53,279
within hopefully a couple of seconds

5236
03:59:51,120 --> 03:59:56,220
it's just

5237
03:59:53,279 --> 03:59:58,859
go back to our servers page and go back

5238
03:59:56,220 --> 04:00:01,560
to our jobs page we can see it's now

5239
03:59:58,859 --> 04:00:04,560
running so it's after kicking off

5240
04:00:01,560 --> 04:00:05,760
at 6 23.

5241
04:00:04,560 --> 04:00:07,560
I'm going to give that a couple of

5242
04:00:05,760 --> 04:00:10,620
seconds to run and then we should also

5243
04:00:07,560 --> 04:00:14,000
have the stats available for

5244
04:00:10,620 --> 04:00:14,000
this job

5245
04:00:14,880 --> 04:00:20,399
so we can see on Monday

5246
04:00:17,760 --> 04:00:23,640
one job run

5247
04:00:20,399 --> 04:00:25,620
and it's in the middle of running now

5248
04:00:23,640 --> 04:00:27,899
so we'll give it a couple of seconds to

5249
04:00:25,620 --> 04:00:31,020
run and then we should see

5250
04:00:27,899 --> 04:00:33,120
things like the runtime the pages missed

5251
04:00:31,020 --> 04:00:37,520
Pages items

5252
04:00:33,120 --> 04:00:37,520
and all the other stats coming through

5253
04:00:38,880 --> 04:00:45,660
okay so on our jobs page now we can see

5254
04:00:42,779 --> 04:00:49,439
that the status is changed to finished

5255
04:00:45,660 --> 04:00:51,779
it took 25 seconds and

5256
04:00:49,439 --> 04:00:53,699
we can see the number of pages items

5257
04:00:51,779 --> 04:00:56,340
coverage

5258
04:00:53,699 --> 04:00:59,399
and everything there so we can actually

5259
04:00:56,340 --> 04:01:01,560
click into that now and we can see all

5260
04:00:59,399 --> 04:01:03,720
the pages scrape there

5261
04:01:01,560 --> 04:01:05,660
the runtime and everything in more

5262
04:01:03,720 --> 04:01:09,060
detail

5263
04:01:05,660 --> 04:01:10,500
so we also have things like number of

5264
04:01:09,060 --> 04:01:13,739
items

5265
04:01:10,500 --> 04:01:16,140
fields that were scraped so this pulls

5266
04:01:13,739 --> 04:01:18,540
in stuff like the we can see if there

5267
04:01:16,140 --> 04:01:21,420
was any stuff that was missed

5268
04:01:18,540 --> 04:01:22,979
so things like the number of stars the

5269
04:01:21,420 --> 04:01:24,540
price everything most things seem to be

5270
04:01:22,979 --> 04:01:26,819
at 100 percent

5271
04:01:24,540 --> 04:01:28,739
and obviously the description there

5272
04:01:26,819 --> 04:01:30,960
there was one or two descriptions that

5273
04:01:28,739 --> 04:01:35,220
were missed for some reason so this is

5274
04:01:30,960 --> 04:01:37,260
can be useful to see which Fields were

5275
04:01:35,220 --> 04:01:39,840
missed are which Fields came through

5276
04:01:37,260 --> 04:01:41,580
correctly you can also see the amount of

5277
04:01:39,840 --> 04:01:43,560
bandwidth that was used

5278
04:01:41,580 --> 04:01:45,300
and we can see that there was one

5279
04:01:43,560 --> 04:01:47,819
warning as well

5280
04:01:45,300 --> 04:01:51,300
so obviously when you have

5281
04:01:47,819 --> 04:01:53,040
multiple runs of the same spider then

5282
04:01:51,300 --> 04:01:54,720
you can actually compare

5283
04:01:53,040 --> 04:01:58,020
if you're running this daily you could

5284
04:01:54,720 --> 04:02:00,979
compare the stats every day and then if

5285
04:01:58,020 --> 04:02:03,600
on one day suddenly you see a major

5286
04:02:00,979 --> 04:02:05,399
Divergence in the stats you can say okay

5287
04:02:03,600 --> 04:02:07,319
there must be an issue with my spider I

5288
04:02:05,399 --> 04:02:09,660
need to investigate and go in and

5289
04:02:07,319 --> 04:02:10,739
investigate further

5290
04:02:09,660 --> 04:02:12,779
so

5291
04:02:10,739 --> 04:02:15,120
that's very useful we also have the

5292
04:02:12,779 --> 04:02:19,260
status codes if there's 500s coming back

5293
04:02:15,120 --> 04:02:22,859
are 404s the page isn't found so maybe

5294
04:02:19,260 --> 04:02:26,100
they the links are broken so you can use

5295
04:02:22,859 --> 04:02:28,680
these status codes as well to diagnose

5296
04:02:26,100 --> 04:02:31,560
any other issues

5297
04:02:28,680 --> 04:02:34,500
so that's how you would use the scrape

5298
04:02:31,560 --> 04:02:36,899
UPS dashboards to integrate in with

5299
04:02:34,500 --> 04:02:39,000
Scrapy D which would also be running on

5300
04:02:36,899 --> 04:02:42,540
your server

5301
04:02:39,000 --> 04:02:45,060
so you've two different dashboards that

5302
04:02:42,540 --> 04:02:48,239
you can use with scrapeyd scriptyweb are

5303
04:02:45,060 --> 04:02:49,680
the script UPS dashboard and integration

5304
04:02:48,239 --> 04:02:52,080
if you guys have any other questions

5305
04:02:49,680 --> 04:02:54,120
with that you can stick it in the

5306
04:02:52,080 --> 04:02:57,060
comments as well

5307
04:02:54,120 --> 04:02:58,800
so in the next section we'll be looking

5308
04:02:57,060 --> 04:03:01,680
at how we can

5309
04:02:58,800 --> 04:03:03,239
instead of using scrapeyd

5310
04:03:01,680 --> 04:03:05,699
to

5311
04:03:03,239 --> 04:03:09,239
degrade with scrape UPS we'll be looking

5312
04:03:05,699 --> 04:03:10,739
at using the complete scrape UPS

5313
04:03:09,239 --> 04:03:15,060
integration

5314
04:03:10,739 --> 04:03:18,300
to integrate in directly with your

5315
04:03:15,060 --> 04:03:21,600
scrapey project instead of using scrape

5316
04:03:18,300 --> 04:03:24,300
PD as a kind of a middle layer which

5317
04:03:21,600 --> 04:03:27,239
gets integrated with your project and

5318
04:03:24,300 --> 04:03:30,359
then other things have to integrate in

5319
04:03:27,239 --> 04:03:32,460
and hit the API endpoints this just goes

5320
04:03:30,359 --> 04:03:34,439
directly and integrates with your server

5321
04:03:32,460 --> 04:03:37,319
and your project so that's what we'll be

5322
04:03:34,439 --> 04:03:40,500
looking at in part 11 and then Part 12

5323
04:03:37,319 --> 04:03:41,640
we'll be looking at using the scrapey

5324
04:03:40,500 --> 04:03:44,699
cloud

5325
04:03:41,640 --> 04:03:47,359
so that's it for now and see you in part

5326
04:03:44,699 --> 04:03:47,359
11.

5327
04:03:49,859 --> 04:03:53,520
so in part 11 of the scrappy beginners

5328
04:03:52,199 --> 04:03:56,939
course we're going to be looking at

5329
04:03:53,520 --> 04:04:00,420
using scrape UPS to manage deploy and

5330
04:03:56,939 --> 04:04:02,580
monitor our spiders

5331
04:04:00,420 --> 04:04:05,880
so we'll jump straight into it first

5332
04:04:02,580 --> 04:04:09,000
things first we're going to need to set

5333
04:04:05,880 --> 04:04:11,100
up a virtual machine so you can do it

5334
04:04:09,000 --> 04:04:12,600
with AWS if you already have an account

5335
04:04:11,100 --> 04:04:15,660
with them you can do it with

5336
04:04:12,600 --> 04:04:17,819
digitalocean most of these different

5337
04:04:15,660 --> 04:04:20,160
companies have free credits so you can

5338
04:04:17,819 --> 04:04:22,680
just sign up use their free credits try

5339
04:04:20,160 --> 04:04:24,960
it out and then go from there

5340
04:04:22,680 --> 04:04:26,580
so I've already got a digitalocean

5341
04:04:24,960 --> 04:04:28,680
account and I'm going to be using that

5342
04:04:26,580 --> 04:04:31,620
so you guys can follow along with that

5343
04:04:28,680 --> 04:04:34,319
are you guys if you already have an AWS

5344
04:04:31,620 --> 04:04:37,260
or Azure account you can follow on from

5345
04:04:34,319 --> 04:04:39,000
the step where we log in to the actual

5346
04:04:37,260 --> 04:04:41,040
virtual machine

5347
04:04:39,000 --> 04:04:43,199
so I'm just going to quickly set up a

5348
04:04:41,040 --> 04:04:45,779
server here I'm going to go on to the

5349
04:04:43,199 --> 04:04:47,660
cheapest ones they have which are four

5350
04:04:45,779 --> 04:04:51,180
dollars a month

5351
04:04:47,660 --> 04:04:54,660
and I think that's all I need to select

5352
04:04:51,180 --> 04:04:57,300
so I can just create droplets

5353
04:04:54,660 --> 04:05:00,060
now that the droplets been created I

5354
04:04:57,300 --> 04:05:02,460
have the dashboard available where it

5355
04:05:00,060 --> 04:05:04,140
shows things like the IP address and a

5356
04:05:02,460 --> 04:05:06,420
few different graphs

5357
04:05:04,140 --> 04:05:08,880
I'm just going to

5358
04:05:06,420 --> 04:05:11,220
open the console now

5359
04:05:08,880 --> 04:05:13,380
so that's going to open up a new window

5360
04:05:11,220 --> 04:05:16,080
and it's going to

5361
04:05:13,380 --> 04:05:21,000
SSH onto a machine and then we can then

5362
04:05:16,080 --> 04:05:24,359
run commands on our virtual machine

5363
04:05:21,000 --> 04:05:25,680
so while that's getting set up if you

5364
04:05:24,359 --> 04:05:29,100
guys haven't already

5365
04:05:25,680 --> 04:05:31,080
set up a account with scrape UPS so you

5366
04:05:29,100 --> 04:05:33,479
can get a free account there and we'll

5367
04:05:31,080 --> 04:05:34,979
be using them now to integrate with our

5368
04:05:33,479 --> 04:05:38,160
server

5369
04:05:34,979 --> 04:05:39,540
so go ahead create an account I have one

5370
04:05:38,160 --> 04:05:42,540
already set up

5371
04:05:39,540 --> 04:05:46,199
once your account is set up

5372
04:05:42,540 --> 04:05:46,979
go to servers and deployments on the

5373
04:05:46,199 --> 04:05:49,620
site

5374
04:05:46,979 --> 04:05:53,100
where it says add servers click add

5375
04:05:49,620 --> 04:05:56,060
and then we're going to name our server

5376
04:05:53,100 --> 04:05:56,060
free code camp

5377
04:05:57,300 --> 04:06:01,260
and we're going to put in the IP address

5378
04:05:59,220 --> 04:06:04,020
of the server

5379
04:06:01,260 --> 04:06:06,359
so we get that from here

5380
04:06:04,020 --> 04:06:09,239
copy their P address

5381
04:06:06,359 --> 04:06:12,300
paste it in save the details

5382
04:06:09,239 --> 04:06:15,540
and now it says to

5383
04:06:12,300 --> 04:06:19,260
provision a server we need to run this

5384
04:06:15,540 --> 04:06:23,880
script on our server so we copy the

5385
04:06:19,260 --> 04:06:25,680
details there go in to our terminal our

5386
04:06:23,880 --> 04:06:27,060
console for the server

5387
04:06:25,680 --> 04:06:28,939
and

5388
04:06:27,060 --> 04:06:31,979
paste in

5389
04:06:28,939 --> 04:06:34,739
the script details so that's going to go

5390
04:06:31,979 --> 04:06:36,000
off run the script and provision the

5391
04:06:34,739 --> 04:06:37,620
server

5392
04:06:36,000 --> 04:06:40,080
and

5393
04:06:37,620 --> 04:06:42,260
we can see that it is running through

5394
04:06:40,080 --> 04:06:45,359
installing the dependencies it needs

5395
04:06:42,260 --> 04:06:48,060
installing a new user and the authorized

5396
04:06:45,359 --> 04:06:50,040
keys and installing the required

5397
04:06:48,060 --> 04:06:52,199
libraries so that's just going to run

5398
04:06:50,040 --> 04:06:54,660
through those different steps

5399
04:06:52,199 --> 04:06:58,560
and once it's complete we should be able

5400
04:06:54,660 --> 04:07:01,199
to then go on and in Clone our

5401
04:06:58,560 --> 04:07:03,420
spider onto our server using the

5402
04:07:01,199 --> 04:07:05,760
dashboard so that can take an integer

5403
04:07:03,420 --> 04:07:08,359
two we let it just run through the

5404
04:07:05,760 --> 04:07:08,359
different steps

5405
04:07:09,239 --> 04:07:14,399
so now our provisioning has completed

5406
04:07:11,819 --> 04:07:17,939
and it's brought us into the server

5407
04:07:14,399 --> 04:07:20,580
dashboard and here we have options like

5408
04:07:17,939 --> 04:07:23,220
clone repository add spider delete the

5409
04:07:20,580 --> 04:07:27,120
server edit the details schedule jobs

5410
04:07:23,220 --> 04:07:29,220
and the SSH keys for the server so we're

5411
04:07:27,120 --> 04:07:30,600
going to go ahead and go to the Clone

5412
04:07:29,220 --> 04:07:33,600
Repository

5413
04:07:30,600 --> 04:07:36,239
that's where it's going to be getting

5414
04:07:33,600 --> 04:07:37,920
the details of the repository that we

5415
04:07:36,239 --> 04:07:41,520
paste in and then it's going to clone

5416
04:07:37,920 --> 04:07:42,960
the spider directly onto our server so

5417
04:07:41,520 --> 04:07:45,840
we don't need to actually do it manually

5418
04:07:42,960 --> 04:07:46,979
ourselves we can do it through this UI

5419
04:07:45,840 --> 04:07:49,739
here

5420
04:07:46,979 --> 04:07:53,580
so for that we're going to first

5421
04:07:49,739 --> 04:07:58,020
go to the free code comp part 6 that

5422
04:07:53,580 --> 04:08:00,479
we've been using in the part 10 video so

5423
04:07:58,020 --> 04:08:02,640
we're going to use part six and the next

5424
04:08:00,479 --> 04:08:05,100
thing you want to do is you want to Fork

5425
04:08:02,640 --> 04:08:07,560
your own copy obviously if you have your

5426
04:08:05,100 --> 04:08:09,479
own spider and you're using that that's

5427
04:08:07,560 --> 04:08:11,819
fine if you're following along with me

5428
04:08:09,479 --> 04:08:15,239
now the best thing to do is Fork your

5429
04:08:11,819 --> 04:08:17,220
own copy so click the four button follow

5430
04:08:15,239 --> 04:08:20,720
the steps that's just going to copy over

5431
04:08:17,220 --> 04:08:24,600
the free code cam part 6 onto your own

5432
04:08:20,720 --> 04:08:27,120
repo and then from there like I have

5433
04:08:24,600 --> 04:08:30,000
here I've just done it myself I now have

5434
04:08:27,120 --> 04:08:33,420
it under my own name and in here the

5435
04:08:30,000 --> 04:08:36,720
next step will be to add the

5436
04:08:33,420 --> 04:08:41,000
deploy key so this deploy key will

5437
04:08:36,720 --> 04:08:45,660
enable us to do commands like git clone

5438
04:08:41,000 --> 04:08:47,520
and pull the repo directly from our

5439
04:08:45,660 --> 04:08:50,160
GitHub onto the machine

5440
04:08:47,520 --> 04:08:52,920
so we just need to add that key we go to

5441
04:08:50,160 --> 04:08:54,899
settings deploy keys

5442
04:08:52,920 --> 04:08:56,760
add deploy key

5443
04:08:54,899 --> 04:08:59,939
I'm going to call this

5444
04:08:56,760 --> 04:09:02,880
free code cam VM and I'm going to paste

5445
04:08:59,939 --> 04:09:05,580
in my deploy key so you get this deploy

5446
04:09:02,880 --> 04:09:08,100
key in here you copy that

5447
04:09:05,580 --> 04:09:11,160
you add it in here

5448
04:09:08,100 --> 04:09:12,540
remove any spaces and we don't need to

5449
04:09:11,160 --> 04:09:13,800
allow right access for the moment

5450
04:09:12,540 --> 04:09:16,260
because we're just going to be pulling

5451
04:09:13,800 --> 04:09:20,520
and we can add the key

5452
04:09:16,260 --> 04:09:25,140
so once the key is added

5453
04:09:20,520 --> 04:09:27,660
we can then go back to our main UI here

5454
04:09:25,140 --> 04:09:32,460
the next thing we need to do is

5455
04:09:27,660 --> 04:09:35,160
just go and grab our URL

5456
04:09:32,460 --> 04:09:37,500
from the main page here

5457
04:09:35,160 --> 04:09:39,960
grab our URL

5458
04:09:37,500 --> 04:09:43,380
so this is my repository and this is my

5459
04:09:39,960 --> 04:09:44,939
own copy of the free code cam part 6.

5460
04:09:43,380 --> 04:09:49,439
the branch name

5461
04:09:44,939 --> 04:09:51,479
is main so you can see it here it's Main

5462
04:09:49,439 --> 04:09:54,660
so I need to make sure to put in the

5463
04:09:51,479 --> 04:09:57,180
right Branch name and then the language

5464
04:09:54,660 --> 04:10:00,479
is Python and the framework is scrapey

5465
04:09:57,180 --> 04:10:02,279
so that's all correct

5466
04:10:00,479 --> 04:10:03,479
so this is the install script that's

5467
04:10:02,279 --> 04:10:05,580
going to run

5468
04:10:03,479 --> 04:10:09,840
when we click on repo it's going to go

5469
04:10:05,580 --> 04:10:13,140
in to our virtual machine it's going to

5470
04:10:09,840 --> 04:10:15,180
then get clonus and then once it's get

5471
04:10:13,140 --> 04:10:17,460
cloned the repo it's going to go into

5472
04:10:15,180 --> 04:10:18,779
the repo install a python virtual

5473
04:10:17,460 --> 04:10:22,500
environment activate the virtual

5474
04:10:18,779 --> 04:10:25,739
environment and then install the modules

5475
04:10:22,500 --> 04:10:26,939
that are in the requirements.txt so if

5476
04:10:25,739 --> 04:10:30,420
you

5477
04:10:26,939 --> 04:10:33,660
look in here it's going to install the

5478
04:10:30,420 --> 04:10:34,800
different modules that are listed in

5479
04:10:33,660 --> 04:10:37,680
here

5480
04:10:34,800 --> 04:10:39,359
which is everything that the project

5481
04:10:37,680 --> 04:10:41,399
needs to run

5482
04:10:39,359 --> 04:10:43,560
so

5483
04:10:41,399 --> 04:10:45,840
once it's done that it's just going to

5484
04:10:43,560 --> 04:10:49,279
make sure that scrape is installed and

5485
04:10:45,840 --> 04:10:52,199
while we're at it we're going to add the

5486
04:10:49,279 --> 04:10:55,020
monitoring module first grade pops as

5487
04:10:52,199 --> 04:10:57,960
well so let's just add in

5488
04:10:55,020 --> 04:11:01,040
that here as well so install scrape up

5489
04:10:57,960 --> 04:11:04,680
scrape B which installs the monitoring

5490
04:11:01,040 --> 04:11:08,220
python module for us so once that's all

5491
04:11:04,680 --> 04:11:10,140
in here we can click clone repo and it's

5492
04:11:08,220 --> 04:11:12,359
going to go through the steps here so

5493
04:11:10,140 --> 04:11:14,279
it's cloning the repo great

5494
04:11:12,359 --> 04:11:16,680
and then it's going to run the install

5495
04:11:14,279 --> 04:11:18,060
script which can take two or three

5496
04:11:16,680 --> 04:11:20,460
minutes

5497
04:11:18,060 --> 04:11:23,819
and then the next step is it's going to

5498
04:11:20,460 --> 04:11:25,800
find the Scrapy spiders by running the

5499
04:11:23,819 --> 04:11:27,899
scrapey list command

5500
04:11:25,800 --> 04:11:31,620
so I'm just going to give it a minute or

5501
04:11:27,899 --> 04:11:35,040
two and then we will hopefully see our

5502
04:11:31,620 --> 04:11:38,279
repo in our table here and we'll see the

5503
04:11:35,040 --> 04:11:41,760
spider our book spider in under the

5504
04:11:38,279 --> 04:11:44,939
spiders table on the right so as you can

5505
04:11:41,760 --> 04:11:47,160
see the install script ran correctly and

5506
04:11:44,939 --> 04:11:49,859
it was able to find our spiders as well

5507
04:11:47,160 --> 04:11:52,380
so you can see our spider automatically

5508
04:11:49,859 --> 04:11:54,000
came in here and here is our cloned

5509
04:11:52,380 --> 04:11:57,600
Repository

5510
04:11:54,000 --> 04:11:59,160
so if you click in you can see that

5511
04:11:57,600 --> 04:12:01,319
there is

5512
04:11:59,160 --> 04:12:05,160
a deploy script here as well so if you

5513
04:12:01,319 --> 04:12:06,479
need to deploy updates to your code

5514
04:12:05,160 --> 04:12:09,239
you will

5515
04:12:06,479 --> 04:12:10,920
update your own repository and then for

5516
04:12:09,239 --> 04:12:12,720
the code to actually go onto the server

5517
04:12:10,920 --> 04:12:15,840
you just need to go in here and click

5518
04:12:12,720 --> 04:12:18,720
deploy and it will then pull the latest

5519
04:12:15,840 --> 04:12:20,939
from your Repository

5520
04:12:18,720 --> 04:12:24,319
so that's how you would update the code

5521
04:12:20,939 --> 04:12:24,319
on your VM

5522
04:12:24,479 --> 04:12:30,300
okay so we have a repository we have our

5523
04:12:27,540 --> 04:12:32,040
spider so let's just go ahead and show

5524
04:12:30,300 --> 04:12:34,439
you guys you can quickly run the Spider

5525
04:12:32,040 --> 04:12:37,199
by just clicking the run now button

5526
04:12:34,439 --> 04:12:39,420
it'll go in select the server the

5527
04:12:37,199 --> 04:12:40,979
repository and the spider because you

5528
04:12:39,420 --> 04:12:43,620
could have multiple

5529
04:12:40,979 --> 04:12:46,620
spiders in your repository and we're

5530
04:12:43,620 --> 04:12:48,779
just going to click submit job to run it

5531
04:12:46,620 --> 04:12:51,060
straight away

5532
04:12:48,779 --> 04:12:52,800
so the job is started

5533
04:12:51,060 --> 04:12:55,380
and if you want to check the log

5534
04:12:52,800 --> 04:12:57,420
straight away you just come here and

5535
04:12:55,380 --> 04:13:01,680
click view logs

5536
04:12:57,420 --> 04:13:05,220
so you can see it's just gonna head and

5537
04:13:01,680 --> 04:13:07,739
it's running the spider correctly and

5538
04:13:05,220 --> 04:13:10,560
you can see the title product type price

5539
04:13:07,739 --> 04:13:13,859
everything is coming through so that's

5540
04:13:10,560 --> 04:13:16,800
how simple it is to run the spider

5541
04:13:13,859 --> 04:13:19,680
so the last step we want to do now is to

5542
04:13:16,800 --> 04:13:21,899
activate the monitoring for our spider

5543
04:13:19,680 --> 04:13:25,140
so instead of having to just look at a

5544
04:13:21,899 --> 04:13:26,640
bunch of logs in a log file like that we

5545
04:13:25,140 --> 04:13:29,640
can have everything displaying in the

5546
04:13:26,640 --> 04:13:35,160
dashboard like we had in part 10. so to

5547
04:13:29,640 --> 04:13:37,080
do that let's just open up the docs

5548
04:13:35,160 --> 04:13:39,239
go to monitoring

5549
04:13:37,080 --> 04:13:41,640
python scrapey

5550
04:13:39,239 --> 04:13:44,399
scrapey SDK integration

5551
04:13:41,640 --> 04:13:46,979
so we've already done this pip installed

5552
04:13:44,399 --> 04:13:49,979
scrape up scrapey as part of the install

5553
04:13:46,979 --> 04:13:52,020
script when we cloned our repository so

5554
04:13:49,979 --> 04:13:55,739
we don't need to do that again

5555
04:13:52,020 --> 04:13:58,199
but we do need to add in our API key and

5556
04:13:55,739 --> 04:14:00,720
the extension bits here

5557
04:13:58,199 --> 04:14:03,540
so it's telling us we need to add this

5558
04:14:00,720 --> 04:14:04,319
to our settings.py file in our scrapey

5559
04:14:03,540 --> 04:14:07,260
project

5560
04:14:04,319 --> 04:14:11,100
so let's go ahead and do that now

5561
04:14:07,260 --> 04:14:15,060
so I'm going to open up my Repository

5562
04:14:11,100 --> 04:14:18,239
go to book scraper and go to the

5563
04:14:15,060 --> 04:14:21,180
settings.py file and I can just edit it

5564
04:14:18,239 --> 04:14:22,319
directly in here

5565
04:14:21,180 --> 04:14:25,319
so

5566
04:14:22,319 --> 04:14:27,600
I'll need to add the

5567
04:14:25,319 --> 04:14:30,000
three different sections so first I'll

5568
04:14:27,600 --> 04:14:31,680
add the API key and then I'll go and add

5569
04:14:30,000 --> 04:14:32,760
the extension and the downloader

5570
04:14:31,680 --> 04:14:36,960
middlewares

5571
04:14:32,760 --> 04:14:38,399
so I'll just copy this line and go to

5572
04:14:36,960 --> 04:14:41,420
GitHub

5573
04:14:38,399 --> 04:14:41,420
paste this in

5574
04:14:41,640 --> 04:14:46,739
here I need to get my API key which I

5575
04:14:45,120 --> 04:14:49,500
can get

5576
04:14:46,739 --> 04:14:51,979
from my settings

5577
04:14:49,500 --> 04:14:51,979
here

5578
04:14:53,640 --> 04:14:58,979
paste in my API key

5579
04:14:55,800 --> 04:15:02,660
and then I want to go to my extensions

5580
04:14:58,979 --> 04:15:02,660
and then the general middlewares

5581
04:15:04,199 --> 04:15:08,160
extensions

5582
04:15:06,000 --> 04:15:09,420
are currently comforted out so I'll just

5583
04:15:08,160 --> 04:15:14,540
add it underneath

5584
04:15:09,420 --> 04:15:14,540
and last of all downloader middleware's

5585
04:15:14,880 --> 04:15:19,859
so obviously if you guys are using your

5586
04:15:17,279 --> 04:15:22,680
own spider that has

5587
04:15:19,859 --> 04:15:25,140
the downloader midwares uncommented out

5588
04:15:22,680 --> 04:15:28,500
and you're just adding these two lines

5589
04:15:25,140 --> 04:15:30,359
to your existing list but here because

5590
04:15:28,500 --> 04:15:33,720
it's currently commented out I'm just

5591
04:15:30,359 --> 04:15:35,699
pasting in the whole lot in together

5592
04:15:33,720 --> 04:15:39,660
so we've got the extensions the

5593
04:15:35,699 --> 04:15:41,939
downloader middlewares and the API key

5594
04:15:39,660 --> 04:15:46,979
so we should be able to just commit the

5595
04:15:41,939 --> 04:15:50,040
changes and now we can deploy the code

5596
04:15:46,979 --> 04:15:51,779
via our dashboard so now that's

5597
04:15:50,040 --> 04:15:55,380
completed

5598
04:15:51,779 --> 04:15:57,479
we go back to our server

5599
04:15:55,380 --> 04:15:58,739
go into our free code camp

5600
04:15:57,479 --> 04:16:01,080
server

5601
04:15:58,739 --> 04:16:05,160
go into our clone Repository

5602
04:16:01,080 --> 04:16:08,760
and click deploy here

5603
04:16:05,160 --> 04:16:12,180
so the latest has been deployed we can

5604
04:16:08,760 --> 04:16:15,660
check the log as well to see

5605
04:16:12,180 --> 04:16:18,600
did deployment work so we can see it

5606
04:16:15,660 --> 04:16:20,699
updated the book scraper settings.py

5607
04:16:18,600 --> 04:16:23,220
file one file changed with nine

5608
04:16:20,699 --> 04:16:26,220
insertions so that's perfect

5609
04:16:23,220 --> 04:16:27,720
great so now that that's in we should be

5610
04:16:26,220 --> 04:16:31,560
able to

5611
04:16:27,720 --> 04:16:34,640
run our book Spider again

5612
04:16:31,560 --> 04:16:34,640
submit the job

5613
04:16:34,680 --> 04:16:40,140
and

5614
04:16:36,779 --> 04:16:42,540
if we check the logs again

5615
04:16:40,140 --> 04:16:44,160
we can see it's

5616
04:16:42,540 --> 04:16:47,340
kicked off

5617
04:16:44,160 --> 04:16:51,420
and if we go to our

5618
04:16:47,340 --> 04:16:54,380
jobs list we can see there's one running

5619
04:16:51,420 --> 04:16:59,760
so this is the one we ran for part 10

5620
04:16:54,380 --> 04:17:03,120
and this is the one that is running now

5621
04:16:59,760 --> 04:17:05,819
so once it's completed running in about

5622
04:17:03,120 --> 04:17:08,760
20 more seconds we should see the pages

5623
04:17:05,819 --> 04:17:10,680
items coverage and everything else fill

5624
04:17:08,760 --> 04:17:12,420
in as well

5625
04:17:10,680 --> 04:17:14,880
and we can also see that in our

5626
04:17:12,420 --> 04:17:18,779
dashboard we can see

5627
04:17:14,880 --> 04:17:21,319
under Tuesday we have this job that is

5628
04:17:18,779 --> 04:17:21,319
running now

5629
04:17:23,340 --> 04:17:30,479
so we'll just quickly show you how you

5630
04:17:26,819 --> 04:17:32,160
can also schedule it

5631
04:17:30,479 --> 04:17:34,920
so

5632
04:17:32,160 --> 04:17:38,699
in case you're just joining us for part

5633
04:17:34,920 --> 04:17:42,359
11 if you want to schedule a job to run

5634
04:17:38,699 --> 04:17:43,920
on This Server recurring you just go and

5635
04:17:42,359 --> 04:17:47,399
click recurring

5636
04:17:43,920 --> 04:17:49,560
and then you can select we want okay

5637
04:17:47,399 --> 04:17:53,000
every day in March

5638
04:17:49,560 --> 04:17:53,000
we want every

5639
04:17:53,040 --> 04:17:58,739
every time at midnight we want this job

5640
04:17:57,300 --> 04:18:01,260
to run

5641
04:17:58,739 --> 04:18:04,979
so we'll submit the job

5642
04:18:01,260 --> 04:18:05,819
and then we can check in our scheduled

5643
04:18:04,979 --> 04:18:08,279
jobs

5644
04:18:05,819 --> 04:18:11,699
we have book Spider

5645
04:18:08,279 --> 04:18:14,399
which will run as 12 every day only in

5646
04:18:11,699 --> 04:18:16,680
March and there it is

5647
04:18:14,399 --> 04:18:18,660
so then if you need to edit that or you

5648
04:18:16,680 --> 04:18:22,319
want to just disable it you can go to

5649
04:18:18,660 --> 04:18:25,380
the scheduler Tab and you have the

5650
04:18:22,319 --> 04:18:28,140
ability just to disable it there or

5651
04:18:25,380 --> 04:18:30,359
delete it cloness view the logs whatever

5652
04:18:28,140 --> 04:18:32,359
you need to do

5653
04:18:30,359 --> 04:18:35,819
so if we just go back to our dashboard

5654
04:18:32,359 --> 04:18:39,660
we can now see that the job is completed

5655
04:18:35,819 --> 04:18:42,120
the page is scraper there we can see the

5656
04:18:39,660 --> 04:18:44,520
items are there and everything looks

5657
04:18:42,120 --> 04:18:47,460
like it ran correctly so we can compare

5658
04:18:44,520 --> 04:18:50,160
the the two days so yesterday this many

5659
04:18:47,460 --> 04:18:52,020
pages were scraped are this many status

5660
04:18:50,160 --> 04:18:54,660
codes came back today this many status

5661
04:18:52,020 --> 04:18:57,479
codes came back so it is useful if you

5662
04:18:54,660 --> 04:18:59,939
need to compare the same job that was

5663
04:18:57,479 --> 04:19:03,540
run over multiple days you can quickly

5664
04:18:59,939 --> 04:19:06,180
see okay if the runtime varied or if the

5665
04:19:03,540 --> 04:19:07,979
number of page varied varies are the

5666
04:19:06,180 --> 04:19:10,260
items are the coverage

5667
04:19:07,979 --> 04:19:13,140
you can see that very quickly in one

5668
04:19:10,260 --> 04:19:14,460
page one glance so that makes it very

5669
04:19:13,140 --> 04:19:16,500
useful

5670
04:19:14,460 --> 04:19:21,199
and if we need to drill down into the

5671
04:19:16,500 --> 04:19:24,600
individual job we can just click in and

5672
04:19:21,199 --> 04:19:27,899
we can delete the job data or do

5673
04:19:24,600 --> 04:19:29,160
anything else we need to do here

5674
04:19:27,899 --> 04:19:32,399
so

5675
04:19:29,160 --> 04:19:34,020
that brings us to the end of this

5676
04:19:32,399 --> 04:19:36,600
section

5677
04:19:34,020 --> 04:19:39,600
any questions you guys might have

5678
04:19:36,600 --> 04:19:42,300
let us know and I hope you have an idea

5679
04:19:39,600 --> 04:19:45,120
now of how to quickly get set up with a

5680
04:19:42,300 --> 04:19:48,000
virtual machine and hook it up to use

5681
04:19:45,120 --> 04:19:50,399
scrape UPS so a reminder that everything

5682
04:19:48,000 --> 04:19:54,300
that we've used with scrape UPS here is

5683
04:19:50,399 --> 04:19:56,760
free to use so there's no limitations on

5684
04:19:54,300 --> 04:20:00,020
the image of servers you can hook up are

5685
04:19:56,760 --> 04:20:00,020
the amount of jobs you can run

5686
04:20:00,120 --> 04:20:04,939
so that's the end of part 11 guys

5687
04:20:08,340 --> 04:20:12,600
so in part 12 we're going to look at

5688
04:20:10,380 --> 04:20:15,479
everything to do with Scrappy cloud

5689
04:20:12,600 --> 04:20:18,300
so Scrappy Cloud was made by the

5690
04:20:15,479 --> 04:20:21,420
developers of scrapey and it's a way

5691
04:20:18,300 --> 04:20:24,899
which you can deploy and run and

5692
04:20:21,420 --> 04:20:26,880
schedule your spiders on the cloud using

5693
04:20:24,899 --> 04:20:29,520
scrapey Cloud the great thing is you

5694
04:20:26,880 --> 04:20:31,500
don't need to have your own third-party

5695
04:20:29,520 --> 04:20:33,600
server so you don't need to have a

5696
04:20:31,500 --> 04:20:36,660
server with digitalocean or vulture or

5697
04:20:33,600 --> 04:20:39,779
AWS you can just deploy it directly onto

5698
04:20:36,660 --> 04:20:42,359
Scrappy cloud and just run it the only

5699
04:20:39,779 --> 04:20:44,880
downside is that if you want to schedule

5700
04:20:42,359 --> 04:20:47,340
your jobs it's paid so you can run your

5701
04:20:44,880 --> 04:20:49,620
jobs and it's free but to schedule your

5702
04:20:47,340 --> 04:20:53,100
jobs on Scrappy Cloud you have to sign

5703
04:20:49,620 --> 04:20:55,439
up for a monthly subscription

5704
04:20:53,100 --> 04:20:58,620
so we'll show you how everything works

5705
04:20:55,439 --> 04:21:02,279
and then between scrapey Cloud scrape

5706
04:20:58,620 --> 04:21:04,500
ups and Scrapy D you guys will have had

5707
04:21:02,279 --> 04:21:07,140
a full overview of all the different

5708
04:21:04,500 --> 04:21:10,020
ways you can deploy schedule and run

5709
04:21:07,140 --> 04:21:12,899
your spiders on the cloud and you can

5710
04:21:10,020 --> 04:21:15,180
decide then which kind of way you want

5711
04:21:12,899 --> 04:21:17,580
to go do you want to go with a

5712
04:21:15,180 --> 04:21:21,720
completely open source way with using

5713
04:21:17,580 --> 04:21:25,680
just scrapeyd and scripted web or do you

5714
04:21:21,720 --> 04:21:28,520
want to go with a free way using scrape

5715
04:21:25,680 --> 04:21:31,680
UPS or do you want to go with a paid

5716
04:21:28,520 --> 04:21:34,620
solution with scrapey cloud

5717
04:21:31,680 --> 04:21:36,540
so you'll have the full up array of

5718
04:21:34,620 --> 04:21:39,000
options covered by the time we finish

5719
04:21:36,540 --> 04:21:42,500
part 12. okay

5720
04:21:39,000 --> 04:21:46,739
so let's quickly look at scrapey Cloud

5721
04:21:42,500 --> 04:21:49,380
so scribby cloud is obviously made by

5722
04:21:46,739 --> 04:21:51,239
size the Creator is the scrappy scalable

5723
04:21:49,380 --> 04:21:53,279
Cloud hosting for your scrapey spiders

5724
04:21:51,239 --> 04:21:54,960
and that's pretty much what it is host

5725
04:21:53,279 --> 04:21:58,260
and monitor your Scrappy spiders in the

5726
04:21:54,960 --> 04:22:01,080
cloud as we said and it's very reliable

5727
04:21:58,260 --> 04:22:03,120
easy to scale as they say on demand

5728
04:22:01,080 --> 04:22:06,239
scaling they have lots of other

5729
04:22:03,120 --> 04:22:08,279
Integrations as well so

5730
04:22:06,239 --> 04:22:10,560
what we need to do is you need to go

5731
04:22:08,279 --> 04:22:14,040
ahead and create an account with them

5732
04:22:10,560 --> 04:22:17,040
once you have an account you can then go

5733
04:22:14,040 --> 04:22:20,060
into the dashboard and access scrapey

5734
04:22:17,040 --> 04:22:20,060
Cloud here on the side

5735
04:22:20,399 --> 04:22:25,439
so I'm just going to start a new project

5736
04:22:23,699 --> 04:22:27,739
and I'm going to call it just free code

5737
04:22:25,439 --> 04:22:27,739
camp

5738
04:22:28,979 --> 04:22:32,699
and

5739
04:22:30,239 --> 04:22:34,020
click Start

5740
04:22:32,699 --> 04:22:38,220
so

5741
04:22:34,020 --> 04:22:40,680
then it's got the instructions here of

5742
04:22:38,220 --> 04:22:42,479
what we need to do to install the

5743
04:22:40,680 --> 04:22:45,120
command line

5744
04:22:42,479 --> 04:22:49,920
tool so we can easily deploy our spider

5745
04:22:45,120 --> 04:22:52,439
into the scrapey cloud so first things

5746
04:22:49,920 --> 04:22:55,500
first we're going to go ahead and we're

5747
04:22:52,439 --> 04:22:57,720
going to be using the part 6 code

5748
04:22:55,500 --> 04:23:00,840
example again so the code that we used

5749
04:22:57,720 --> 04:23:03,779
for part six of this course we're going

5750
04:23:00,840 --> 04:23:06,779
to just get clone that so I've got an

5751
04:23:03,779 --> 04:23:09,180
empty folder here open in vs code and

5752
04:23:06,779 --> 04:23:12,479
I'm just going to Gig clone the free

5753
04:23:09,180 --> 04:23:14,699
code Camp part 6 and then I'm going to

5754
04:23:12,479 --> 04:23:17,340
quickly install

5755
04:23:14,699 --> 04:23:19,859
a virtual environment obviously if you

5756
04:23:17,340 --> 04:23:22,020
guys are on Windows or on Linux you guys

5757
04:23:19,859 --> 04:23:25,620
need to follow the steps they'll be

5758
04:23:22,020 --> 04:23:27,300
covered in part two of this course to

5759
04:23:25,620 --> 04:23:28,979
make sure you're installing the correct

5760
04:23:27,300 --> 04:23:30,779
virtual environment

5761
04:23:28,979 --> 04:23:34,439
for your operating system

5762
04:23:30,779 --> 04:23:36,840
and then once the virtual environment is

5763
04:23:34,439 --> 04:23:39,660
set up we can activate it

5764
04:23:36,840 --> 04:23:41,060
so just do

5765
04:23:39,660 --> 04:23:43,319
Source

5766
04:23:41,060 --> 04:23:45,660
bin activate

5767
04:23:43,319 --> 04:23:48,300
I know it's activated and now we can go

5768
04:23:45,660 --> 04:23:50,160
and follow the instructions here

5769
04:23:48,300 --> 04:23:52,080
so I'm just going to copy and paste them

5770
04:23:50,160 --> 04:23:55,080
in directly

5771
04:23:52,080 --> 04:23:59,580
so install S hub

5772
04:23:55,080 --> 04:24:01,439
and then to the s-hub login

5773
04:23:59,580 --> 04:24:03,779
and then it's just a matter of putting

5774
04:24:01,439 --> 04:24:07,500
in my API key

5775
04:24:03,779 --> 04:24:10,819
okay so that's all installed

5776
04:24:07,500 --> 04:24:10,819
put in s-hub login

5777
04:24:11,340 --> 04:24:16,680
it says I'm already logged in

5778
04:24:13,620 --> 04:24:19,979
I'll just log out to make sure that you

5779
04:24:16,680 --> 04:24:23,239
guys can see the full process

5780
04:24:19,979 --> 04:24:23,239
so a sub login

5781
04:24:23,279 --> 04:24:27,920
put in my API key from here

5782
04:24:29,760 --> 04:24:33,779
paste in

5783
04:24:31,560 --> 04:24:36,540
my API key obviously you guys will be

5784
04:24:33,779 --> 04:24:40,080
putting in your API key so please don't

5785
04:24:36,540 --> 04:24:44,939
use mine and then we can just do

5786
04:24:40,080 --> 04:24:48,779
s-hub deploy and it should deploy

5787
04:24:44,939 --> 04:24:52,080
if I'm in my

5788
04:24:48,779 --> 04:24:55,520
correct project so I'm inside my project

5789
04:24:52,080 --> 04:24:58,920
and I think it wants there to be a

5790
04:24:55,520 --> 04:25:02,220
scrapey.cfg file so you need to make

5791
04:24:58,920 --> 04:25:06,060
sure you're in the correct folder

5792
04:25:02,220 --> 04:25:08,399
and if it's correct it you can see

5793
04:25:06,060 --> 04:25:11,279
the deploying to Scrappy Cloud project

5794
04:25:08,399 --> 04:25:14,100
and then the project ID there

5795
04:25:11,279 --> 04:25:18,300
so in the meantime you can see it's

5796
04:25:14,100 --> 04:25:23,460
bills the project and then it

5797
04:25:18,300 --> 04:25:26,220
uploads it to the site here so here you

5798
04:25:23,460 --> 04:25:28,620
can see it's been deployed

5799
04:25:26,220 --> 04:25:31,020
and it's successful so if it works

5800
04:25:28,620 --> 04:25:33,960
correctly this should all

5801
04:25:31,020 --> 04:25:36,779
be very similar to what you see

5802
04:25:33,960 --> 04:25:41,819
so now that it's deployed into the cloud

5803
04:25:36,779 --> 04:25:45,120
we can go to our jobs dashboard

5804
04:25:41,819 --> 04:25:47,399
and we should be able to run

5805
04:25:45,120 --> 04:25:50,279
our spider

5806
04:25:47,399 --> 04:25:51,960
so we have our book Spider available now

5807
04:25:50,279 --> 04:25:54,899
in the drop down

5808
04:25:51,960 --> 04:25:56,939
and we can leave everything else the

5809
04:25:54,899 --> 04:25:59,340
same you've got priorities there if you

5810
04:25:56,939 --> 04:26:02,040
want to have certain jobs running ahead

5811
04:25:59,340 --> 04:26:05,520
of others so we can just click run

5812
04:26:02,040 --> 04:26:08,159
and that should kick off the book spider

5813
04:26:05,520 --> 04:26:09,720
and it should start scraping

5814
04:26:08,159 --> 04:26:12,600
our

5815
04:26:09,720 --> 04:26:14,520
books to scrape site so you can see it's

5816
04:26:12,600 --> 04:26:16,260
running away there

5817
04:26:14,520 --> 04:26:18,300
and

5818
04:26:16,260 --> 04:26:22,020
we should see that when it's completed

5819
04:26:18,300 --> 04:26:25,319
it'll be in the completed table

5820
04:26:22,020 --> 04:26:27,239
and it'll also have the items and the

5821
04:26:25,319 --> 04:26:30,180
requests and the errors all available

5822
04:26:27,239 --> 04:26:33,359
here as well so they'll start populating

5823
04:26:30,180 --> 04:26:34,739
in a second

5824
04:26:33,359 --> 04:26:37,680
so

5825
04:26:34,739 --> 04:26:42,080
as you can see three requests the logs

5826
04:26:37,680 --> 04:26:45,300
are there if we wanted to run every day

5827
04:26:42,080 --> 04:26:48,540
once a week periodically whenever we

5828
04:26:45,300 --> 04:26:50,819
want we just go to periodic jobs click

5829
04:26:48,540 --> 04:26:53,939
add periodic job

5830
04:26:50,819 --> 04:26:58,560
select the spider and then we can select

5831
04:26:53,939 --> 04:27:00,000
okay we want to run every Monday at

5832
04:26:58,560 --> 04:27:02,279
6am

5833
04:27:00,000 --> 04:27:04,439
or 7am

5834
04:27:02,279 --> 04:27:08,040
and

5835
04:27:04,439 --> 04:27:10,439
we can just select every day

5836
04:27:08,040 --> 04:27:13,800
or every Monday 7 A.M

5837
04:27:10,439 --> 04:27:16,319
and we can save that

5838
04:27:13,800 --> 04:27:18,359
so as you can see here it says

5839
04:27:16,319 --> 04:27:22,080
the periodic jobs below will not run

5840
04:27:18,359 --> 04:27:24,120
because I have not created a paid

5841
04:27:22,080 --> 04:27:25,680
subscription so you just need to click

5842
04:27:24,120 --> 04:27:28,739
subscribe now

5843
04:27:25,680 --> 04:27:31,979
and then sign up for the paid version

5844
04:27:28,739 --> 04:27:34,439
and then this job will run every Monday

5845
04:27:31,979 --> 04:27:36,180
at 7.

5846
04:27:34,439 --> 04:27:38,580
but if you guys just want to try it out

5847
04:27:36,180 --> 04:27:41,040
then you could just schedule the jobs

5848
04:27:38,580 --> 04:27:42,359
normally by clicking run here manually

5849
04:27:41,040 --> 04:27:43,739
yourselves

5850
04:27:42,359 --> 04:27:47,100
so

5851
04:27:43,739 --> 04:27:49,199
my spider is still running away

5852
04:27:47,100 --> 04:27:51,359
we'll just leave that complete and then

5853
04:27:49,199 --> 04:27:53,220
we'll have a look at the items and

5854
04:27:51,359 --> 04:27:53,939
requests and some of the stats that are

5855
04:27:53,220 --> 04:27:57,840
available

5856
04:27:53,939 --> 04:28:00,720
so as you can see our job is completed

5857
04:27:57,840 --> 04:28:03,479
with the Thousand items

5858
04:28:00,720 --> 04:28:07,319
the requests so the amount of pages that

5859
04:28:03,479 --> 04:28:09,600
were scraped and the logs so we can

5860
04:28:07,319 --> 04:28:13,500
click into the requests

5861
04:28:09,600 --> 04:28:15,239
and you can see T thousand requests if

5862
04:28:13,500 --> 04:28:18,479
you want so you can see all the specific

5863
04:28:15,239 --> 04:28:19,979
URLs that were scraped the statuses

5864
04:28:18,479 --> 04:28:21,659
etc etc

5865
04:28:19,979 --> 04:28:23,279
the items then are a bit more

5866
04:28:21,659 --> 04:28:26,520
interesting obviously because that's the

5867
04:28:23,279 --> 04:28:28,859
actual data that we scraped so we've got

5868
04:28:26,520 --> 04:28:32,640
everything set up nicely there our

5869
04:28:28,859 --> 04:28:33,779
prices taxes titles URLs description

5870
04:28:32,640 --> 04:28:36,180
etc etc

5871
04:28:33,779 --> 04:28:39,060
so you can check quickly if the

5872
04:28:36,180 --> 04:28:41,520
information that was scraped correctly

5873
04:28:39,060 --> 04:28:43,380
came through or not

5874
04:28:41,520 --> 04:28:46,680
if you need to look at the logs or the

5875
04:28:43,380 --> 04:28:49,920
stats that's also available there so

5876
04:28:46,680 --> 04:28:54,479
as you can see it's very polished very

5877
04:28:49,920 --> 04:28:57,000
nice simple to use and also works and

5878
04:28:54,479 --> 04:28:58,859
auto scales when you need to scale

5879
04:28:57,000 --> 04:29:01,380
things as well

5880
04:28:58,859 --> 04:29:03,600
so I think that's the ins and outs of

5881
04:29:01,380 --> 04:29:05,640
scrapey cloud I think now you should

5882
04:29:03,600 --> 04:29:07,760
have a very good idea of the different

5883
04:29:05,640 --> 04:29:11,399
options you have when it comes to

5884
04:29:07,760 --> 04:29:14,399
deploying your spiders and then

5885
04:29:11,399 --> 04:29:17,340
scheduling and running your spiders and

5886
04:29:14,399 --> 04:29:19,680
seeing all the stats in various

5887
04:29:17,340 --> 04:29:22,920
dashboards so just to go through the

5888
04:29:19,680 --> 04:29:27,239
three options you have first option

5889
04:29:22,920 --> 04:29:29,040
Scrapy D which can run just via an API

5890
04:29:27,239 --> 04:29:31,439
endpoint and you can hit the API

5891
04:29:29,040 --> 04:29:32,520
endpoint to schedule things and deploy

5892
04:29:31,439 --> 04:29:36,000
things

5893
04:29:32,520 --> 04:29:38,060
with Scrapy D you have two uis you can

5894
04:29:36,000 --> 04:29:40,500
use with it the

5895
04:29:38,060 --> 04:29:42,540
scrapeydweb are the scrape Ops

5896
04:29:40,500 --> 04:29:44,880
dashboards which

5897
04:29:42,540 --> 04:29:47,100
we've shown you how to install so that's

5898
04:29:44,880 --> 04:29:49,739
the free open source part the second

5899
04:29:47,100 --> 04:29:52,020
option was using scrape ups for

5900
04:29:49,739 --> 04:29:54,899
everything and using the scrape UPS

5901
04:29:52,020 --> 04:29:59,399
integration directly with a server such

5902
04:29:54,899 --> 04:30:01,439
as digitalocean are AWS our vulture so

5903
04:29:59,399 --> 04:30:02,479
you need to set up a VM quickly there

5904
04:30:01,439 --> 04:30:05,939
first

5905
04:30:02,479 --> 04:30:08,159
and then the final option was just using

5906
04:30:05,939 --> 04:30:10,439
the scrapey cloud for everything and

5907
04:30:08,159 --> 04:30:12,960
deploying it directly to Scrappy Cloud

5908
04:30:10,439 --> 04:30:15,960
but it was paid if you want to do any

5909
04:30:12,960 --> 04:30:19,020
sort of periodic jobs to schedule them

5910
04:30:15,960 --> 04:30:20,880
to run daily or weekly or whatever

5911
04:30:19,020 --> 04:30:22,439
so there are the three main options and

5912
04:30:20,880 --> 04:30:25,380
I'll leave it up to you guys to decide

5913
04:30:22,439 --> 04:30:28,439
what works best for you so that brings

5914
04:30:25,380 --> 04:30:30,720
us to the end of Part 12 and in part 13

5915
04:30:28,439 --> 04:30:33,659
we'll just go through everything and do

5916
04:30:30,720 --> 04:30:37,460
quick recap of the entire course

5917
04:30:33,659 --> 04:30:37,460
so see you in part 13 guys

5918
04:30:39,600 --> 04:30:44,279
so guys we've come to the end of our

5919
04:30:41,880 --> 04:30:47,460
Scrappy beginners course this is the

5920
04:30:44,279 --> 04:30:50,399
last part so we'll just do a quick wrap

5921
04:30:47,460 --> 04:30:52,680
up and then I'll give you a small bit of

5922
04:30:50,399 --> 04:30:54,960
an overview of some extra skills you

5923
04:30:52,680 --> 04:30:56,880
might find useful if you want to

5924
04:30:54,960 --> 04:30:58,439
continue on and get better at scraping

5925
04:30:56,880 --> 04:31:00,720
using scraping

5926
04:30:58,439 --> 04:31:02,939
so we've built an end-to-end scrapey

5927
04:31:00,720 --> 04:31:05,640
project that scrapes all the books from

5928
04:31:02,939 --> 04:31:08,460
books to scrape and then cleans the data

5929
04:31:05,640 --> 04:31:11,040
and stores the extracted data in

5930
04:31:08,460 --> 04:31:13,500
different file formats and different

5931
04:31:11,040 --> 04:31:15,600
places such as a database

5932
04:31:13,500 --> 04:31:18,600
we then looked at

5933
04:31:15,600 --> 04:31:22,319
optimizing our headers and user agents

5934
04:31:18,600 --> 04:31:25,620
so that websites would let us get

5935
04:31:22,319 --> 04:31:27,600
through any antibiot software that they

5936
04:31:25,620 --> 04:31:29,760
have on their site and we also looked at

5937
04:31:27,600 --> 04:31:31,439
how we can use proxies in the different

5938
04:31:29,760 --> 04:31:33,239
proxy provider options that are out

5939
04:31:31,439 --> 04:31:36,659
there if we want to have something

5940
04:31:33,239 --> 04:31:39,239
that's a little less Hands-On

5941
04:31:36,659 --> 04:31:41,880
and then finally we looked at how you

5942
04:31:39,239 --> 04:31:44,760
can deploy your spider

5943
04:31:41,880 --> 04:31:46,159
to the cloud onto a server and then how

5944
04:31:44,760 --> 04:31:48,899
you can schedule this to run

5945
04:31:46,159 --> 04:31:53,819
periodically and then how you can view

5946
04:31:48,899 --> 04:31:55,500
the results of your running spiders

5947
04:31:53,819 --> 04:31:57,899
but obviously that isn't everything

5948
04:31:55,500 --> 04:32:00,560
we've only gone through the kind of

5949
04:31:57,899 --> 04:32:02,640
Basics there's still a lot more

5950
04:32:00,560 --> 04:32:05,699
different edge cases that we haven't

5951
04:32:02,640 --> 04:32:08,460
covered and the number one thing is

5952
04:32:05,699 --> 04:32:10,319
probably scraping Dynamic websites so

5953
04:32:08,460 --> 04:32:13,920
there's a lot of websites out there that

5954
04:32:10,319 --> 04:32:15,840
are rendered in the browser so that

5955
04:32:13,920 --> 04:32:18,600
means that they're using a front-end

5956
04:32:15,840 --> 04:32:21,120
framework which will actually only

5957
04:32:18,600 --> 04:32:23,880
display the page once all the data is

5958
04:32:21,120 --> 04:32:28,020
received by the browser so in that case

5959
04:32:23,880 --> 04:32:30,180
if you were to ask for a URL what you

5960
04:32:28,020 --> 04:32:31,800
get back might not contain the data

5961
04:32:30,180 --> 04:32:34,859
you're looking for because it hasn't had

5962
04:32:31,800 --> 04:32:36,659
a chance to render inside in a browser

5963
04:32:34,859 --> 04:32:38,399
so

5964
04:32:36,659 --> 04:32:41,159
things I would recommend that you look

5965
04:32:38,399 --> 04:32:44,220
at in in those cases would be looking at

5966
04:32:41,159 --> 04:32:48,000
things like Scrappy Puppeteer or Scrappy

5967
04:32:44,220 --> 04:32:50,279
selenium which then use scrapey with a

5968
04:32:48,000 --> 04:32:54,420
headless browser integration to actually

5969
04:32:50,279 --> 04:32:58,140
render the website in a headless browser

5970
04:32:54,420 --> 04:33:00,080
so then that way using scrapey Puppeteer

5971
04:32:58,140 --> 04:33:03,000
or scrapey selenium you can actually

5972
04:33:00,080 --> 04:33:04,141
render the page and get the data you

5973
04:33:03,000 --> 04:33:07,260
need

5974
04:33:04,141 --> 04:33:09,359
the other option would be to find the

5975
04:33:07,260 --> 04:33:11,279
API endpoint because most of these sites

5976
04:33:09,359 --> 04:33:13,080
that are front and rendered have API

5977
04:33:11,279 --> 04:33:13,980
endpoints and you can find the data

5978
04:33:13,080 --> 04:33:16,320
there

5979
04:33:13,980 --> 04:33:19,320
so I'll just give you one example of

5980
04:33:16,320 --> 04:33:22,859
what it would look like to see the data

5981
04:33:19,320 --> 04:33:25,141
coming back from an API endpoint so I

5982
04:33:22,859 --> 04:33:26,539
would recommend if you guys want to work

5983
04:33:25,141 --> 04:33:28,980
through a couple of different challenges

5984
04:33:26,539 --> 04:33:31,680
site have put together this great site

5985
04:33:28,980 --> 04:33:34,379
which we've been using for our books to

5986
04:33:31,680 --> 04:33:37,379
scrape.com but they also have a bunch of

5987
04:33:34,379 --> 04:33:40,799
different other examples using this

5988
04:33:37,379 --> 04:33:43,561
quotes dot to scrape.com where you can

5989
04:33:40,799 --> 04:33:44,879
for example have infinite scrolling so

5990
04:33:43,561 --> 04:33:46,141
how would you get around a page where

5991
04:33:44,879 --> 04:33:48,600
you have infinite scrolling like

5992
04:33:46,141 --> 04:33:51,240
something like Instagram or Facebook and

5993
04:33:48,600 --> 04:33:53,641
you can practice having a JavaScript

5994
04:33:51,240 --> 04:33:55,740
rendered site so there's all these kind

5995
04:33:53,641 --> 04:33:57,660
of different options which you can

5996
04:33:55,740 --> 04:33:59,520
practice with and they have all these

5997
04:33:57,660 --> 04:34:02,219
different pages available here for you

5998
04:33:59,520 --> 04:34:04,799
to practice on so

5999
04:34:02,219 --> 04:34:08,039
one example would be

6000
04:34:04,799 --> 04:34:10,320
this one see we're scrolling down and if

6001
04:34:08,039 --> 04:34:13,500
you go to your network tab you can see

6002
04:34:10,320 --> 04:34:16,439
there is data being asked for every time

6003
04:34:13,500 --> 04:34:19,260
we scroll down so more pages of data are

6004
04:34:16,439 --> 04:34:22,199
being requested and they come back but

6005
04:34:19,260 --> 04:34:25,379
instead of it being HTML it comes back

6006
04:34:22,199 --> 04:34:27,719
as Json and then this Json data we've

6007
04:34:25,379 --> 04:34:30,959
got the quotes which then the front end

6008
04:34:27,719 --> 04:34:32,459
framework then goes off and populates

6009
04:34:30,959 --> 04:34:33,600
the page with

6010
04:34:32,459 --> 04:34:35,400
so

6011
04:34:33,600 --> 04:34:36,600
this is an example of where you can

6012
04:34:35,400 --> 04:34:40,561
actually

6013
04:34:36,600 --> 04:34:43,439
directly query an API endpoint instead

6014
04:34:40,561 --> 04:34:47,760
of actually scraping the HTML you can

6015
04:34:43,439 --> 04:34:49,859
ask for the API endpoint to give you

6016
04:34:47,760 --> 04:34:51,359
back the data directly so that even

6017
04:34:49,859 --> 04:34:53,760
makes your life easier

6018
04:34:51,359 --> 04:34:55,379
so this is one example of what I was

6019
04:34:53,760 --> 04:34:58,619
talking about if you come into contact

6020
04:34:55,379 --> 04:35:01,141
with a front-end rendered

6021
04:34:58,619 --> 04:35:03,420
page that gets rendered in the browser

6022
04:35:01,141 --> 04:35:05,699
so I really would recommend you guys

6023
04:35:03,420 --> 04:35:07,859
checking out and working your way

6024
04:35:05,699 --> 04:35:11,699
through these different challenges that

6025
04:35:07,859 --> 04:35:14,160
are available on the twoscribe.com site

6026
04:35:11,699 --> 04:35:16,439
now another very important thing is

6027
04:35:14,160 --> 04:35:18,539
obviously getting through a login

6028
04:35:16,439 --> 04:35:20,699
endpoint which we didn't do in this

6029
04:35:18,539 --> 04:35:23,580
course but which is something which a

6030
04:35:20,699 --> 04:35:24,779
lot of websites would have so that is

6031
04:35:23,580 --> 04:35:27,000
something as well I would really

6032
04:35:24,779 --> 04:35:29,039
recommend that you guys go off and

6033
04:35:27,000 --> 04:35:32,760
explore how to do

6034
04:35:29,039 --> 04:35:35,219
so the last major thing I think would be

6035
04:35:32,760 --> 04:35:37,740
looking at how you can scrape at scale

6036
04:35:35,219 --> 04:35:40,740
if you really want to imagine you're

6037
04:35:37,740 --> 04:35:42,719
scraping millions of pages a day you

6038
04:35:40,740 --> 04:35:46,141
know there's different ways which you

6039
04:35:42,719 --> 04:35:49,199
can use things like scrapey redis to use

6040
04:35:46,141 --> 04:35:51,359
redis to store all the URLs you want to

6041
04:35:49,199 --> 04:35:53,279
scrape in one central place and then you

6042
04:35:51,359 --> 04:35:57,119
could have multiple different servers

6043
04:35:53,279 --> 04:36:00,240
pulling URLs off this queue and then the

6044
04:35:57,119 --> 04:36:02,459
old URLs can be scraped by multiple

6045
04:36:00,240 --> 04:36:05,641
different worker machines at the same

6046
04:36:02,459 --> 04:36:06,840
time again this is all using scraping so

6047
04:36:05,641 --> 04:36:09,180
that is something I would highly

6048
04:36:06,840 --> 04:36:11,400
recommend you guys to look at as well if

6049
04:36:09,180 --> 04:36:15,480
you're interested in scraping at scale

6050
04:36:11,400 --> 04:36:18,660
now all this stuff we also have as

6051
04:36:15,480 --> 04:36:21,240
articles and videos so if you guys want

6052
04:36:18,660 --> 04:36:23,279
to check out some more videos and

6053
04:36:21,240 --> 04:36:25,020
in-depth articles I would recommend you

6054
04:36:23,279 --> 04:36:28,439
guys checking that out like we have

6055
04:36:25,020 --> 04:36:31,320
scraping behind logins and we have

6056
04:36:28,439 --> 04:36:33,719
separate articles on using Scrappy

6057
04:36:31,320 --> 04:36:34,740
Puppeteer Scrapy selenium

6058
04:36:33,719 --> 04:36:37,619
Etc

6059
04:36:34,740 --> 04:36:40,439
and also using scrapey redis if you want

6060
04:36:37,619 --> 04:36:42,719
to do a distributed worker architecture

6061
04:36:40,439 --> 04:36:45,359
so all that is up there for you guys if

6062
04:36:42,719 --> 04:36:48,420
you want to continue on your journey and

6063
04:36:45,359 --> 04:36:51,480
learn more about scrapey and the kind of

6064
04:36:48,420 --> 04:36:53,459
more challenging parts of using scrapey

6065
04:36:51,480 --> 04:36:56,939
to scrape the web

6066
04:36:53,459 --> 04:36:59,279
so I think that comes to the end of our

6067
04:36:56,939 --> 04:37:01,619
course I'd like to thank you for

6068
04:36:59,279 --> 04:37:04,260
following along and if you have any

6069
04:37:01,619 --> 04:37:06,420
questions just reach out put a comment

6070
04:37:04,260 --> 04:37:10,039
on the video and we'll do our best to

6071
04:37:06,420 --> 04:37:10,039
get back to you thanks guys

